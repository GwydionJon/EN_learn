{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen: cannot load any more object with static TLS\n___________________________________________________________________________\nContents of /tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/sklearn/__check_build:\n__init__.py               _check_build.cpython-38-x86_64-linux-gnu.so__pycache__\nsetup.py\n___________________________________________________________________________\nIt seems that scikit-learn has not been built correctly.\n\nIf you have installed scikit-learn from source, please do not forget\nto build the package before using it: run `python setup.py install` or\n`make` in the source directory.\n\nIf you have used an installer, please check that it is suited for your\nPython version, your operating system and your platform.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/sklearn/__check_build/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_check_build\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_build\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen: cannot load any more object with static TLS",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d3e3c2926575>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#import kerastuner as kt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/sklearn/__check_build/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_check_build\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_build\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mraise_build_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/sklearn/__check_build/__init__.py\u001b[0m in \u001b[0;36mraise_build_error\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mdir_content\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     raise ImportError(\"\"\"%s\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0m___________________________________________________________________________\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mContents\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen: cannot load any more object with static TLS\n___________________________________________________________________________\nContents of /tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/sklearn/__check_build:\n__init__.py               _check_build.cpython-38-x86_64-linux-gnu.so__pycache__\nsetup.py\n___________________________________________________________________________\nIt seems that scikit-learn has not been built correctly.\n\nIf you have installed scikit-learn from source, please do not forget\nto build the package before using it: run `python setup.py install` or\n`make` in the source directory.\n\nIf you have used an installer, please check that it is suited for your\nPython version, your operating system and your platform."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#import kerastuner as kt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only the number of maxima\n",
    "df_spectra_all=pd.read_csv(\"spectrum_energy_input_numberOfPeaks.csv\",index_col=[0])\n",
    "all_data = df_spectra_all[[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\",\"no_of_max\"]]\n",
    "# drop all rows containing 0.5 to make it binary\n",
    "print(len(all_data))\n",
    "all_data = all_data.query('k6a1 != 0.5 & k6a2 != 0.5 & k11 != 0.5 & k12 != 0.5 & k9a1 != 0.5 & k9a2 != 0.5')\n",
    "print(len(all_data))\n",
    "#set the values to zero and one\n",
    "#labels = ['k6a1','k6a2','k11','k12','k9a1','k9a2']\n",
    "#[all_data[i].mask(all_data[i] == 0.25, 0, inplace=True) for i in labels]\n",
    "#[all_data[i].mask(all_data[i] == 0.75, 1, inplace=True) for i in labels]\n",
    "#print(all_data.head(10))\n",
    "data_train, data_test = train_test_split(all_data, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns_A = []\n",
    "\n",
    "no_of_max = tf.feature_column.numeric_column(\"no_of_max\")\n",
    "#my_feature_layer_A = tf.keras.layers.DenseFeatures(no_of_max_bucket)\n",
    "feature_columns_A.append(no_of_max)\n",
    "k6a1 = tf.feature_column.numeric_column(\"k6a1\")\n",
    "feature_columns_A.append(k6a1)\n",
    "k6a2 = tf.feature_column.numeric_column(\"k6a2\")\n",
    "feature_columns_A.append(k6a2)\n",
    "k11 = tf.feature_column.numeric_column(\"k11\")\n",
    "feature_columns_A.append(k11)\n",
    "k12 = tf.feature_column.numeric_column(\"k12\")\n",
    "feature_columns_A.append(k12)\n",
    "k9a1 = tf.feature_column.numeric_column(\"k9a1\")\n",
    "feature_columns_A.append(k9a1)\n",
    "k9a2 = tf.feature_column.numeric_column(\"k9a2\")\n",
    "feature_columns_A.append(k9a2)\n",
    "\n",
    "my_feature_layer_A = tf.keras.layers.DenseFeatures(feature_columns_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "    \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "    for m in list_of_metrics:\n",
    "        x = hist[m]\n",
    "        plt.plot(epochs[1:], x[1:], label=m)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for activation functions check https://keras.io/api/layers/activations/\n",
    "def create_model2(my_learning_rate, my_feature_layer,my_metrics,my_act_function = \"softmax\"):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(my_feature_layer)\n",
    "    layers=[20,12]\n",
    "    for layer in layers:\n",
    "        model.add(tf.keras.layers.Dense(units = layer, activation = my_act_function))\n",
    "    model.add(tf.keras.layers.Dense(units=6,name='Output', activation = 'softmax'))                             \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),                                       \n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics=my_metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, dataset, epochs, label_name,\n",
    "                batch_size=None,shuffle=True):\n",
    "    features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    label=dataset[label_name].to_numpy()\n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle)\n",
    "  \n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    return epochs, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "epochs = 50\n",
    "batch_size = 12\n",
    "\n",
    "#specify the classification threshold\n",
    "classification_threshold = 0.15\n",
    "\n",
    "# Establish the metrics the model will measure.\n",
    "metric = [tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),\n",
    "      tf.keras.metrics.Precision(thresholds=classification_threshold,name='precision'),\n",
    "      tf.keras.metrics.Recall(thresholds=classification_threshold,name='recall'),]\n",
    "\n",
    "label_name = [\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"]\n",
    "#label_name = \"k6a1\"\n",
    "my_model = create_model2(learning_rate, my_feature_layer_A,metric,my_act_function=\"sigmoid\")\n",
    "epochs, hist = train_model(my_model, data_train, epochs, \n",
    "                          label_name, batch_size)\n",
    "# Plot a graph of the metric(s) vs. epochs.\n",
    "#list_of_metrics_to_plot = ['accuracy'] \n",
    "list_of_metrics_to_plot = ['accuracy', 'precision', 'recall'] \n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {name:np.array(value) for name, value in data_test.items()}\n",
    "label=data_test[label_name].to_numpy()\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "evaluation=my_model.evaluate(x = features, y = label, batch_size=batch_size)\n",
    "predicted = my_model.predict(features)\n",
    "print(predicted)\n",
    "df_test=pd.DataFrame(label,columns=[\"k6a1_test\",\"k6a2_test\",\"k11_test\",\"k12_test\",\"k9a1_test\",\"k9a2_test\"])\n",
    "#df_test=pd.DataFrame(label,columns=[\"k6a1_test\"])\n",
    "df_predict=pd.DataFrame(predicted,columns=[\"k6a1_hat\",\"k6a2_hat\",\"k11_hat\",\"k12_hat\",\"k9a1_hat\",\"k9a2_hat\"])\n",
    "#df_predict=pd.DataFrame(predicted,columns=[\"k6a1_hat\"])\n",
    "#df_test = df_test.round(0)\n",
    "#df_predict = df_predict.round(0)\n",
    "pd.concat([df_test,df_predict], axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umbauen in ein echtes Klassifizierungsproblem\n",
    "\n",
    "see https://sebastianraschka.com/faq/docs/softmax_regression.html\n",
    "\n",
    "Wir haben für jede Konstante drei mögliche Werte: 0.25, 0.5 und 0.75 - das ist somit ein \"ternary\" (dreifaltiges?) Klassifikationsproblem. Die softmax Funktion am Ende des Modells gibt eine Wahrscheinlichkeit an, inwiefern das \"feature\" zu welcher Klasse gehört (prozentual). \n",
    "\n",
    "Ich würde mal anfangen und für k6a1 ein Modell bauen mit one-hot encoding für die drei möglichen Werte und als feature die Anzahl der Peaks. Das gleiche für ausschliesslich k6a2, k11, k12, k9a1, k9a2. Vermutlich wird das nicht so gut funktionieren weil die Information einfach nicht ausreichend ist. Da würde ich mal versuchen zu verstehen, welche Vorhersagen dir das Modell gibt, und dass du die Wahrscheinlichkeiten für die drei möglichen Klassen bekommst, für jedes Beispiel.\n",
    "\n",
    "Im zweiten Schritt würde ich dann andere Klassen-Kombinationen probieren:\n",
    "Klasse 1 - k6a1, k11, k9a1 \n",
    "Klasse 2 - k6a2, k12, k9a2 \n",
    "\n",
    "Sagen wir, die jeweiligen Werte liegen bei -1 (momentan 0.25), 0 (momentan 0.5), 1 (momentan 0.75). Die Summe aller Werte liegt damit zwischen -3 und +3.\n",
    "\n",
    "Dann wäre Fall A: sum(Klasse 1) < -1; Fall B: -1 < sum(Klasse 1) < 1; Fall C: sum(Klasse 1) > 1. Mit den Grenzen kann man etwas herumspielen; und das gleiche jeweils für Klasse 2.\n",
    "\n",
    "Oder vielleicht hast du noch andere Ideen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#import kerastuner as kt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spectra_all=pd.read_csv(\"spectrum_energy_input_numberOfPeaks.csv\",index_col=[0])\n",
    "all_data = df_spectra_all[[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\",\"no_of_max\"]]\n",
    "\n",
    "all_data=pd.get_dummies(all_data,columns=[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"])\n",
    "\n",
    "\n",
    "#all_data[\"k6a1\"]=all_data[[\"k6a1_0.25\",\"k6a1_0.5\",\"k6a1_0.75\"]].values.tolist()\n",
    "#all_data[\"k6a2\"]=all_data[[\"k6a1_0.25\",\"k6a1_0.5\",\"k6a1_0.75\"]].values.tolist()\n",
    "#all_data[\"k11\"]=all_data[[\"k11_0.25\",\"k11_0.5\",\"k11_0.75\"]].values.tolist()\n",
    "#all_data[\"k12\"]=all_data[[\"k12_0.25\",\"k12_0.5\",\"k12_0.75\"]].values.tolist()\n",
    "#all_data[\"k9a1\"]=all_data[[\"k9a1_0.25\",\"k9a1_0.5\",\"k9a1_0.75\"]].values.tolist()\n",
    "#all_data[\"k9a2\"]=all_data[[\"k9a2_0.25\",\"k9a2_0.5\",\"k9a2_0.75\"]].values.tolist()\n",
    "#all_data=all_data.drop(columns=[\"k6a1_0.25\",\"k6a1_0.5\",\"k6a1_0.75\",\"k6a2_0.25\",\"k6a2_0.5\",\"k6a2_0.75\",\"k11_0.25\",\n",
    "#                                \"k11_0.5\",\"k11_0.75\",\"k12_0.25\",\"k12_0.5\",\"k12_0.75\",\"k9a1_0.25\",\"k9a1_0.5\",\n",
    "#                                \"k9a1_0.75\",\"k9a2_0.25\",\"k9a2_0.5\",\"k9a2_0.75\"])\n",
    "\n",
    "data_train, data_test = train_test_split(all_data, test_size=0.20, random_state=42)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     49
    ]
   },
   "outputs": [],
   "source": [
    "feature_columns_A = []\n",
    "\n",
    "no_of_max = tf.feature_column.numeric_column(\"no_of_max\")\n",
    "#my_feature_layer_A = tf.keras.layers.DenseFeatures(no_of_max_bucket)\n",
    "feature_columns_A.append(no_of_max)\n",
    "#k6a1 = tf.feature_column.numeric_column(\"k6a1\")\n",
    "#feature_columns_A.append(k6a1)\n",
    "#k6a2 = tf.feature_column.numeric_column(\"k6a2\")\n",
    "#feature_columns_A.append(k6a2)\n",
    "#k11 = tf.feature_column.numeric_column(\"k11\")\n",
    "#feature_columns_A.append(k11)\n",
    "#k12 = tf.feature_column.numeric_column(\"k12\")\n",
    "#feature_columns_A.append(k12)\n",
    "#k9a1 = tf.feature_column.numeric_column(\"k9a1\")\n",
    "#feature_columns_A.append(k9a1)\n",
    "#k9a2 = tf.feature_column.numeric_column(\"k9a2\")\n",
    "#feature_columns_A.append(k9a2)\n",
    "\n",
    "my_feature_layer_A = tf.keras.layers.DenseFeatures(feature_columns_A)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "    \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "    for m in list_of_metrics:\n",
    "        x = hist[m]\n",
    "        plt.plot(epochs[1:], x[1:], label=m)\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "# for activation functions check https://keras.io/api/layers/activations/\n",
    "def create_model2(my_learning_rate, my_feature_layer,my_metrics,my_act_function = \"softmax\"):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(my_feature_layer)\n",
    "    layers=[20,12]\n",
    "    for layer in layers:\n",
    "        model.add(tf.keras.layers.Dense(units = layer, activation = my_act_function))\n",
    "    model.add(tf.keras.layers.Dense(units=18,name='Output', activation = 'softmax'))                             \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),                                       \n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics=my_metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, dataset, epochs, label_name,\n",
    "                batch_size=None,shuffle=True):\n",
    "    #features = {name:np.array(np.asarray(value)) for name, value in dataset.items()}\n",
    "\n",
    "    features={\"no_of_max\":dataset[\"no_of_max\"].to_numpy()}\n",
    "   # print(features)\n",
    "\n",
    "    #for multiple inputs\n",
    "    print(type(label_name))\n",
    "    if(type(label_name)==list):\n",
    "        label=dataset[label_name].to_numpy()\n",
    "        label_array=np.zeros((label.shape[0],label.shape[1],3))\n",
    "        for i in range(label.shape[0]):\n",
    "            for j in range(label.shape[1]):\n",
    "                for k in range(len(label[i,j])):\n",
    "                    label_array[i,j,k]=label[i,j][k]\n",
    "            #print(label_array[i,j])\n",
    "           # print(type(label_array[i,j]))\n",
    "        print(label_array.shape) \n",
    "        label=label_array\n",
    "    else:\n",
    "        label=dataset[label_name].to_numpy()\n",
    "        label_array=np.zeros((label.shape[0],3))\n",
    "        for i in range(label.shape[0]):\n",
    "            for k in range(len(label[i])):\n",
    "                label_array[i,k]=label[i][k]\n",
    "          \n",
    "        print(label_array.shape) \n",
    "        label=label_array\n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle)\n",
    "  \n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    return epochs, hist\n",
    "\n",
    "\n",
    "def train_model2(model, dataset, epochs, label_name,\n",
    "                batch_size=None,shuffle=True):\n",
    "    #features = {name:np.array(np.asarray(value)) for name, value in dataset.items()}\n",
    "\n",
    "    features={\"no_of_max\":dataset[\"no_of_max\"].to_numpy()}\n",
    "   # print(features)\n",
    "    label=dataset[label_name].to_numpy()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle)\n",
    "  \n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    return epochs, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "epochs = 500\n",
    "batch_size = 12\n",
    "\n",
    "#specify the classification threshold\n",
    "classification_threshold = 0.15\n",
    "\n",
    "# Establish the metrics the model will measure.\n",
    "metric = [tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),\n",
    "      tf.keras.metrics.Precision(thresholds=classification_threshold,name='precision'),\n",
    "      tf.keras.metrics.Recall(thresholds=classification_threshold,name='recall'),]\n",
    "\n",
    "#label_name = [\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"]\n",
    "#label_name = \"k6a1\"\n",
    "label_name=[\"k6a1_0.25\",\"k6a1_0.5\",\"k6a1_0.75\",\"k6a2_0.25\",\"k6a2_0.5\",\"k6a2_0.75\",\"k11_0.25\",\"k11_0.5\",\"k11_0.75\",\"k12_0.25\",\"k12_0.5\",\"k12_0.75\",\"k9a1_0.25\",\"k9a1_0.5\",\"k9a1_0.75\",\"k9a2_0.25\",\"k9a2_0.5\",\"k9a2_0.75\"]\n",
    "my_model = create_model2(learning_rate, my_feature_layer_A,metric,my_act_function=\"softmax\")\n",
    "\n",
    "# Plot a graph of the metric(s) vs. epochs.\n",
    "#list_of_metrics_to_plot = ['accuracy'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs, hist = train_model2(my_model, data_train, epochs, \n",
    "                          label_name, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_metrics_to_plot = ['accuracy', 'precision', 'recall'] \n",
    "\n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if(type(label_name)==list):\n",
    "    label=data_test[label_name].to_numpy()\n",
    "    label_array=np.zeros((label.shape[0],label.shape[1],3))\n",
    "    for i in range(label.shape[0]):\n",
    "        for j in range(label.shape[1]):\n",
    "            for k in range(len(label[i,j])):\n",
    "                label_array[i,j,k]=label[i,j][k]\n",
    "        #print(label_array[i,j])\n",
    "       # print(type(label_array[i,j]))\n",
    "    #print(label_array.shape) \n",
    "    label=label_array\n",
    "else:\n",
    "    label=data_test[label_name].to_numpy()\n",
    "    label_array=np.zeros((label.shape[0],3))\n",
    "    for i in range(label.shape[0]):\n",
    "        for k in range(len(label[i])):\n",
    "            label_array[i,k]=label[i][k]\n",
    "\n",
    "    #print(label_array.shape) \n",
    "    label=label_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     12
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features={\"no_of_max\":data_test[\"no_of_max\"].to_numpy()}\n",
    "label=data_test[label_name].to_numpy()\n",
    "#print(label.reshape((len(label),6,3)))\n",
    "\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "evaluation=my_model.evaluate(x = features, y = label, batch_size=batch_size)\n",
    "\n",
    "predicted = my_model.predict(features)\n",
    "\n",
    "label=label.reshape((len(label),6,3))\n",
    "\n",
    "predicted=predicted.reshape((len(predicted),6,3))\n",
    "print(predicted.shape)\n",
    "\n",
    "label_list=[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"]\n",
    "for i in range(predicted.shape[0]):\n",
    "    print(\"Test No:\", i)\n",
    "    for j in range(predicted.shape[1]):\n",
    "        #print(1/sum(predicted[i,j]))\n",
    "        print(label_list[j],\"Label: \",label[i,j],\"predicted: \", predicted[i,j]*1/sum(predicted[i,j]) )\n",
    "print()\n",
    "\n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "df_test=pd.DataFrame(label,columns=[\"k6a1_test\",\"k6a2_test\",\"k11_test\",\"k12_test\",\"k9a1_test\",\"k9a2_test\"])\n",
    "#df_test=pd.DataFrame(label,columns=[\"k6a1_0.25_label\",\"k6a1_0.5_label\",\"k6a1_0.75_label\"])\n",
    "df_predict=pd.DataFrame(predicted,columns=[\"k6a1_hat\",\"k6a2_hat\",\"k11_hat\",\"k12_hat\",\"k9a1_hat\",\"k9a2_hat\"])\n",
    "#df_predict=pd.DataFrame(predicted,columns=[\"k6a1_0.25\",\"k6a1_0.5\",\"k6a1_0.75\"])\n",
    "#df_test = df_test.round(0)\n",
    "#df_predict = df_predict.round(0)\n",
    "pd.concat([df_test,df_predict], axis=1).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen: cannot load any more object with static TLS\n___________________________________________________________________________\nContents of /tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/sklearn/__check_build:\n__init__.py               _check_build.cpython-38-x86_64-linux-gnu.so__pycache__\nsetup.py\n___________________________________________________________________________\nIt seems that scikit-learn has not been built correctly.\n\nIf you have installed scikit-learn from source, please do not forget\nto build the package before using it: run `python setup.py install` or\n`make` in the source directory.\n\nIf you have used an installer, please check that it is suited for your\nPython version, your operating system and your platform.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/sklearn/__check_build/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_check_build\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_build\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen: cannot load any more object with static TLS",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ccf9a84b22fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#import kerastuner as kt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/sklearn/__check_build/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_check_build\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_build\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mraise_build_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/sklearn/__check_build/__init__.py\u001b[0m in \u001b[0;36mraise_build_error\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mdir_content\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     raise ImportError(\"\"\"%s\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0m___________________________________________________________________________\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mContents\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen: cannot load any more object with static TLS\n___________________________________________________________________________\nContents of /tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/sklearn/__check_build:\n__init__.py               _check_build.cpython-38-x86_64-linux-gnu.so__pycache__\nsetup.py\n___________________________________________________________________________\nIt seems that scikit-learn has not been built correctly.\n\nIf you have installed scikit-learn from source, please do not forget\nto build the package before using it: run `python setup.py install` or\n`make` in the source directory.\n\nIf you have used an installer, please check that it is suited for your\nPython version, your operating system and your platform."
     ]
    }
   ],
   "source": [
    "#class predictions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#import kerastuner as kt\n",
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   k6a1  k6a2   k11   k12  k9a1  k9a2  no_of_max\n",
      "0  0.25  0.25  0.25  0.25  0.25  0.25         18\n",
      "1  0.25  0.25  0.25  0.25  0.25  0.50          5\n",
      "2  0.25  0.25  0.25  0.25  0.25  0.75         19\n",
      "3  0.25  0.25  0.25  0.25  0.50  0.25         19\n",
      "4  0.25  0.25  0.25  0.25  0.50  0.50          5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "/tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "/tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/tmpa/tcstud25/miniconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k6a1</th>\n",
       "      <th>k6a2</th>\n",
       "      <th>k11</th>\n",
       "      <th>k12</th>\n",
       "      <th>k9a1</th>\n",
       "      <th>k9a2</th>\n",
       "      <th>no_of_max</th>\n",
       "      <th>Class0</th>\n",
       "      <th>Class1</th>\n",
       "      <th>Class2</th>\n",
       "      <th>Class3</th>\n",
       "      <th>Class4</th>\n",
       "      <th>Class5</th>\n",
       "      <th>Class6</th>\n",
       "      <th>Class7</th>\n",
       "      <th>Class8</th>\n",
       "      <th>Class9</th>\n",
       "      <th>Class10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k6a1  k6a2   k11   k12  k9a1  k9a2  no_of_max  Class0  Class1  Class2  \\\n",
       "0  0.25  0.25  0.25  0.25  0.25  0.25         18     1.0     0.0     0.0   \n",
       "1  0.25  0.25  0.25  0.25  0.25  0.50          5     0.0     0.0     0.0   \n",
       "2  0.25  0.25  0.25  0.25  0.25  0.75         19     0.0     0.0     0.0   \n",
       "3  0.25  0.25  0.25  0.25  0.50  0.25         19     0.0     0.0     0.0   \n",
       "4  0.25  0.25  0.25  0.25  0.50  0.50          5     0.0     0.0     0.0   \n",
       "\n",
       "   Class3  Class4  Class5  Class6  Class7  Class8  Class9  Class10  \n",
       "0     0.0     1.0     1.0     1.0     1.0     0.0     1.0      0.0  \n",
       "1     0.0     1.0     1.0     0.0     1.0     0.0     1.0      0.0  \n",
       "2     0.0     1.0     1.0     0.0     1.0     0.0     0.0      0.0  \n",
       "3     0.0     1.0     1.0     0.0     1.0     0.0     1.0      0.0  \n",
       "4     0.0     1.0     1.0     1.0     1.0     0.0     1.0      0.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spectra_all=pd.read_csv(\"spectrum_energy_input_numberOfPeaks.csv\",index_col=[0])\n",
    "all_data = df_spectra_all[[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\",\"no_of_max\"]]\n",
    "no_data_points=(len(all_data[\"k6a1\"].values))\n",
    "#all_data=pd.get_dummies(all_data,columns=[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"])\n",
    "\n",
    "#values between 0.75 and 2.25\n",
    "#different classes:\n",
    "#0 all < 0.5\n",
    "#1 all > 0.5\n",
    "#2 k6a1,k11,k9a1 >0.5, k6a2,k11,k9a2 <0.5\n",
    "#3 k6a1,k11,k9a1 <0.5, k6a2,k11,k9a2 >0.5\n",
    "#4 k6a1 == k6a2 \n",
    "#5 k11 == k12 \n",
    "#6 k9a1 == k9a2 \n",
    "#7 sum(k6a1,k11,k9a1) < 1.25\n",
    "#8 sum(k6a1,k11,k9a1) > 2\n",
    "#9 sum(k6a2,k12,k9a2) < 1.25\n",
    "#10 sum(k6a2,k12,k9a2) > 2\n",
    "\n",
    "print(all_data.head())\n",
    "#class_array=np.zeros((no_data_points,11))\n",
    "all_data.loc[(all_data['k6a1'] <0.5) & (all_data['k6a2'] <0.5)& (all_data['k11'] <0.5)\n",
    "                        & (all_data['k12'] <0.5)& (all_data['k9a1'] <0.5)&( all_data['k9a2'] <0.5), 'Class0'] = 1  \n",
    "all_data.loc[(all_data['k6a1'] >0.5) & (all_data['k6a2'] >0.5)& (all_data['k11'] >0.5)\n",
    "                        & (all_data['k12'] >0.5)& (all_data['k9a1'] >0.5)&( all_data['k9a2'] >0.5), 'Class1'] = 1  \n",
    "\n",
    "all_data.loc[(all_data['k6a1'] >0.5) & (all_data['k6a2'] <0.5)& (all_data['k11'] >0.5)\n",
    "                        & (all_data['k12'] <0.5)& (all_data['k9a1'] >0.5)&( all_data['k9a2'] <0.5), 'Class2'] = 1  \n",
    "all_data.loc[(all_data['k6a1'] <0.5) & (all_data['k6a2'] >0.5)& (all_data['k11'] <0.5)\n",
    "                        & (all_data['k12'] >0.5)& (all_data['k9a1'] <0.5)&( all_data['k9a2'] >0.5), 'Class3'] = 1  \n",
    "all_data.loc[(all_data['k6a1'] == all_data['k6a2']), 'Class4'] = 1  \n",
    "all_data.loc[(all_data['k11'] == all_data['k12']), 'Class5'] = 1  \n",
    "all_data.loc[(all_data['k9a1'] == all_data['k9a2']), 'Class6'] = 1  \n",
    "all_data.loc[((all_data['k6a1']+all_data['k11']+all_data['k9a1'])<1.25 ), 'Class7'] = 1  \n",
    "all_data.loc[((all_data['k6a1']+all_data['k11']+all_data['k9a1'])>2 ), 'Class8'] = 1  \n",
    "all_data.loc[((all_data['k6a2']+all_data['k12']+all_data['k9a2'])<1.25 ), 'Class9'] = 1  \n",
    "all_data.loc[((all_data['k6a2']+all_data['k12']+all_data['k9a2'])>2 ), 'Class10'] = 1  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_data=all_data.fillna(0)\n",
    "data_train=all_data\n",
    "\n",
    "#data_train, data_test = train_test_split(all_data, test_size=0.20, random_state=42)\n",
    "#data_train.corr()\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns_A = []\n",
    "\n",
    "no_of_max = tf.feature_column.numeric_column(\"no_of_max\")\n",
    "feature_columns_A.append(no_of_max)\n",
    "\n",
    "my_feature_layer_A = tf.keras.layers.DenseFeatures(feature_columns_A)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "    \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "    for m in list_of_metrics:\n",
    "        x = hist[m]\n",
    "        plt.plot(epochs[1:], x[1:], label=m)\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "# for activation functions check https://keras.io/api/layers/activations/\n",
    "def create_model2(my_learning_rate, my_feature_layer,my_metrics,my_act_function = \"softmax\"):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(my_feature_layer)\n",
    "    layers=[20,25,15,12]\n",
    "    for layer in layers:\n",
    "        model.add(tf.keras.layers.Dense(units = layer, activation = my_act_function))\n",
    "    model.add(tf.keras.layers.Dense(units=11,name='Output', activation = 'softmax'))                             \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),                                       \n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics=my_metrics)\n",
    "    return model\n",
    "\n",
    "def train_model2(model, dataset, epochs, label_name,\n",
    "                batch_size=None,shuffle=True):\n",
    "    #features = {name:np.array(np.asarray(value)) for name, value in dataset.items()}\n",
    "\n",
    "    features={\"no_of_max\":dataset[\"no_of_max\"].to_numpy()}\n",
    "   # print(features)\n",
    "    label=dataset[label_name].to_numpy()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle)\n",
    "  \n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    return epochs, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "epochs = 500\n",
    "batch_size = 300\n",
    "print(len(data_train))\n",
    "#specify the classification threshold\n",
    "classification_threshold = 0.125\n",
    "\n",
    "# Establish the metrics the model will measure.\n",
    "metric = [tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),\n",
    "      tf.keras.metrics.Precision(thresholds=classification_threshold,name='precision'),\n",
    "      tf.keras.metrics.Recall(thresholds=classification_threshold,name='recall'),]\n",
    "\n",
    "label_name=[\"Class0\",\"Class1\",\"Class2\",\"Class3\",\"Class4\",\"Class5\",\"Class6\",\"Class7\",\"Class8\",\"Class9\",\"Class10\"]\n",
    "my_model = create_model2(learning_rate, my_feature_layer_A,metric,my_act_function=\"sigmoid\")\n",
    "\n",
    "# Plot a graph of the metric(s) vs. epochs.\n",
    "#list_of_metrics_to_plot = ['accuracy'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4277 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 2/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.4265 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 3/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4256 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 4/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.4249 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 5/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.4242 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 6/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.4236 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 7/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.4230 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 8/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.4224 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 9/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4218 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 10/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4213 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 11/500\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4340 - accuracy: 0.7800 - precision: 0.0000e+00 - recall: 0.0000e+0 - 0s 13ms/step - loss: 0.4207 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 12/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4202 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 13/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4197 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 14/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4191 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 15/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4186 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 16/500\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4108 - accuracy: 0.7861 - precision: 0.0000e+00 - recall: 0.0000e+0 - 0s 13ms/step - loss: 0.4180 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 17/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4175 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 18/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4170 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 19/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4165 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 20/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4159 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 21/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4154 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 22/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4149 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 23/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4144 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 24/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4139 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 25/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4134 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 26/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4129 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 27/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4123 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 28/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4118 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 29/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4113 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 30/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4108 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 31/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4103 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 32/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4098 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 33/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4093 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 34/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4088 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 35/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4083 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 36/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4079 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 37/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4074 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 38/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4069 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 39/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4064 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 40/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4059 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 41/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4054 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 42/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4050 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 43/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4045 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 44/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4040 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 45/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4035 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 46/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4031 - accuracy: 0.7843 - precision: 0.0014 - recall: 9.9701e-04\n",
      "Epoch 47/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4026 - accuracy: 0.7664 - precision: 0.1244 - recall: 0.1436   \n",
      "Epoch 48/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4021 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 49/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4016 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 50/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4012 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 51/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4007 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 52/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.4002 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 53/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3998 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 54/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3993 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 55/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3989 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 56/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3984 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 57/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3980 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 58/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3975 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 59/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3971 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 60/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3967 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 61/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3962 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 62/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3958 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 63/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3953 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 64/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3949 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 65/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3944 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 66/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3940 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 67/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3936 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 68/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3931 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 69/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3927 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 70/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3923 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 71/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3919 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 72/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3914 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 73/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3910 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 74/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3906 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 75/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3902 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 76/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3897 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 77/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3893 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 78/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3889 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 79/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3885 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 80/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3881 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 81/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3876 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 82/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3872 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 83/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3868 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 84/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3864 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 85/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3860 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 86/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3856 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 87/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3852 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 88/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3848 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 89/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3844 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 90/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3840 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 91/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3836 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 92/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3832 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 93/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3828 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 94/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3824 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 95/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3821 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 96/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3817 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 97/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3813 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 98/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3809 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 99/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3805 - accuracy: 0.7540 - precision: 0.1674 - recall: 0.2433\n",
      "Epoch 100/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3801 - accuracy: 0.7372 - precision: 0.2070 - recall: 0.3888\n",
      "Epoch 101/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3798 - accuracy: 0.7311 - precision: 0.2167 - recall: 0.4397\n",
      "Epoch 102/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3794 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 103/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3790 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 104/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3786 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 105/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3783 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 106/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3779 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 107/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3775 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 108/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3772 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 109/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3768 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 110/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3764 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 111/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3761 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 112/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3757 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 113/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3753 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 114/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3750 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 115/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3746 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 116/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3743 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 117/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3739 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 118/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3736 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 119/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3732 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 120/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3729 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 121/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3725 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 122/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3722 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 123/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3718 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 124/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3715 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 125/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3711 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 126/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3708 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 127/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3705 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 128/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3701 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 129/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3698 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 130/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3695 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 131/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3691 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 132/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3688 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 133/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3685 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 134/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3681 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 135/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3678 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 136/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3675 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 137/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3671 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 138/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3668 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 139/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3665 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 140/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3662 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 141/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3659 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 142/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3655 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 143/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3652 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 144/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3649 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 145/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3646 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 146/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3643 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 147/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3640 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 148/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3636 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 149/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3633 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 150/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3630 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 151/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3627 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 152/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3624 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 153/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3621 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 154/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3618 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 155/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3615 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 156/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3612 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 157/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3609 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 158/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3606 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 159/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3603 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 160/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3600 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 161/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3597 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 162/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3594 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 163/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3591 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 164/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3588 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 165/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3586 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 166/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3583 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 167/500\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3580 - accuracy: 0.7248 - precision: 0.2256 - recall: 0.490 - 0s 8ms/step - loss: 0.3580 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 168/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3577 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 169/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3574 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 170/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3571 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 171/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3568 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 172/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3565 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 173/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3563 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 174/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3560 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 175/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3557 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 176/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3554 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 177/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 12ms/step - loss: 0.3552 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 178/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3549 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 179/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3546 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 180/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3543 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 181/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3541 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 182/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3538 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 183/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3535 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 184/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3532 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 185/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3530 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 186/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3527 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 187/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3524 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 188/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3522 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 189/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3519 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 190/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3516 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 191/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3514 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 192/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3511 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 193/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3509 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 194/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3506 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 195/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3503 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 196/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3501 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 197/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3498 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 198/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3496 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 199/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3493 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 200/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3490 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 201/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3488 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 202/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3485 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 203/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3483 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 204/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3480 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 205/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3478 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 206/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3475 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 207/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3473 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 208/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3471 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 209/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3468 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 210/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3466 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 211/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3463 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 212/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3461 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 213/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3458 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 214/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3456 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 215/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3453 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 216/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3451 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 217/500\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3509 - accuracy: 0.7230 - precision: 0.2267 - recall: 0.483 - 0s 8ms/step - loss: 0.3448 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 218/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3446 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 219/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3444 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 220/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3441 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 221/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3439 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 222/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3437 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 223/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3434 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 224/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3432 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 225/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3430 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 226/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3427 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 227/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3425 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 228/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3423 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 229/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3420 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 230/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3418 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 231/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3416 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 232/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3414 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 233/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3411 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 234/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3409 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 235/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3407 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 236/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3404 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 237/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3402 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 238/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3400 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 239/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3398 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 240/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3396 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 241/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3393 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 242/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3391 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 243/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3389 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 244/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3387 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 245/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3385 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 246/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3383 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 247/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3380 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 248/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3378 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 249/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3376 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 250/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3374 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 251/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3372 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 252/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3370 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 253/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3368 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 254/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3366 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 255/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3364 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 256/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3362 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 257/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3359 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 258/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3357 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 259/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3355 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 260/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3353 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 261/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3351 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 262/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3349 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 263/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3347 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 264/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.3345 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 265/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3343 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 266/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3341 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 267/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3339 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 268/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3337 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 269/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3335 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 270/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3333 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 271/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3331 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 272/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3329 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 273/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3327 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 274/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3325 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 275/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3323 - accuracy: 0.7237 - precision: 0.2227 - recall: 0.4855\n",
      "Epoch 276/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3321 - accuracy: 0.7279 - precision: 0.2262 - recall: 0.4855\n",
      "Epoch 277/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3319 - accuracy: 0.8000 - precision: 0.3090 - recall: 0.4845\n",
      "Epoch 278/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3317 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 279/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3315 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 280/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3313 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 281/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3312 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 282/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3310 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 283/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3308 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 284/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3306 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 285/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3304 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 286/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3302 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 287/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3300 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 288/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3298 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 289/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3297 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 290/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3295 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 291/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3293 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 292/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3291 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 293/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.3289 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 294/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3287 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 295/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3286 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 296/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3284 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 297/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3282 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 298/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3280 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 299/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.3278 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 300/500\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3345 - accuracy: 0.8085 - precision: 0.3250 - recall: 0.462 - 0s 11ms/step - loss: 0.3277 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 301/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.3275 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 302/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.3273 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 303/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.3271 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 304/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3270 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 305/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3268 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 306/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3266 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 307/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3264 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 308/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3263 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 309/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3261 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 310/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3259 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 311/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3258 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 312/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3256 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 313/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3254 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 314/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3252 - accuracy: 0.8143 - precision: 0.3333 - recall: 0.4845\n",
      "Epoch 315/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3251 - accuracy: 0.7941 - precision: 0.3278 - recall: 0.6152\n",
      "Epoch 316/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3249 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 317/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3247 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 318/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3246 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 319/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3244 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 320/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3242 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 321/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3241 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 322/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3239 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 323/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3238 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 324/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3236 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 325/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3234 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 326/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3233 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 327/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3231 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 328/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3229 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 329/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3228 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 330/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3226 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 331/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3225 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 332/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3223 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 333/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3222 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 334/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3220 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 335/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3218 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 336/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3217 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 337/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3215 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 338/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3214 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 339/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3212 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 340/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3211 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 341/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3209 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 342/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3208 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 343/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3206 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 344/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3205 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 345/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3203 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 346/500\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3282 - accuracy: 0.7861 - precision: 0.3456 - recall: 0.726 - 0s 8ms/step - loss: 0.3202 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 347/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3200 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 348/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3199 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 349/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3197 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 350/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3196 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 351/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3195 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 352/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3193 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 353/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3192 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 354/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3190 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 355/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3189 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 356/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3188 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 357/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3186 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 358/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3185 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 359/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3183 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 360/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3182 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 361/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3181 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 362/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3179 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 363/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3178 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 364/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3177 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 365/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3175 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 366/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3174 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 367/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3172 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 368/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3171 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 369/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3170 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 370/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3169 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 371/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3167 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 372/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3166 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 373/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3165 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 374/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3163 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 375/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3162 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 376/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3161 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 377/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3160 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 378/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3158 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 379/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3157 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 380/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3156 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 381/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3155 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 382/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3153 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 383/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3152 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 384/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3151 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 385/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3150 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 386/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3148 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 387/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3147 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 388/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3146 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 389/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3145 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 390/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3143 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 391/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3142 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 392/500\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.3141 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 393/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3140 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 394/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.3139 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 395/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3138 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 396/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.3136 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 397/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3135 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 398/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3134 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 399/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3133 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 400/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3132 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 401/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3131 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 402/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3129 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 403/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3128 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 404/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3127 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 405/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3126 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 406/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3125 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 407/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3124 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 408/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3123 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 409/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3122 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 410/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3120 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 411/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3119 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 412/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3118 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 413/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3117 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 414/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3116 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 415/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3115 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 416/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3114 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 417/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3113 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 418/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3112 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 419/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3111 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 420/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3110 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 421/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3109 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 422/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3108 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 423/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3107 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 424/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3106 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 425/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3105 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 426/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3104 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 427/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3103 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 428/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3102 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 429/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3101 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 430/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3100 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 431/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3099 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 432/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3098 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 433/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3097 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 434/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3096 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 435/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3095 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 436/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3094 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 437/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3093 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 438/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3092 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 439/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3091 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 440/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3090 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 441/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3089 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 442/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3088 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 443/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3087 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 444/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3086 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 445/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3085 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 446/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3085 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 447/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3084 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 448/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3083 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 449/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3082 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 450/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3081 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 451/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3080 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 452/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3079 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 453/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3078 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 454/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3078 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 455/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3077 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 456/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3076 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 457/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3075 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 458/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3074 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 459/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3073 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 460/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3073 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 461/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3072 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 462/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3071 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 463/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3070 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 464/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3069 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 465/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3068 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 466/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3068 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 467/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3067 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 468/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3066 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 469/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3065 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 470/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3064 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 471/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3064 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 472/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3063 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 473/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3062 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 474/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3061 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 475/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3060 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 476/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3060 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 477/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3059 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 478/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3058 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 479/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3057 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 480/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3057 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 481/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3056 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 482/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3055 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 483/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3054 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 484/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3054 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 485/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3053 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 486/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3052 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 487/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3051 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 488/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3051 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 489/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3050 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 490/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3049 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 491/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3048 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 492/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3048 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 493/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3047 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 494/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3046 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 495/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3045 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 496/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3045 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 497/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3044 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 498/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3043 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 499/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3043 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "Epoch 500/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.3042 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n"
     ]
    }
   ],
   "source": [
    "epochs_list, hist = train_model2(my_model, data_train, epochs, \n",
    "                          label_name, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluate the new model against the test set:\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3042 - accuracy: 0.7840 - precision: 0.3333 - recall: 0.7268\n",
      "(729, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_test</th>\n",
       "      <th>1_test</th>\n",
       "      <th>2_test</th>\n",
       "      <th>3_test</th>\n",
       "      <th>4_test</th>\n",
       "      <th>5_test</th>\n",
       "      <th>6_test</th>\n",
       "      <th>7_test</th>\n",
       "      <th>8_test</th>\n",
       "      <th>9_test</th>\n",
       "      <th>...</th>\n",
       "      <th>Class1</th>\n",
       "      <th>Class2</th>\n",
       "      <th>Class3</th>\n",
       "      <th>Class4</th>\n",
       "      <th>Class5</th>\n",
       "      <th>Class6</th>\n",
       "      <th>Class7</th>\n",
       "      <th>Class8</th>\n",
       "      <th>Class9</th>\n",
       "      <th>Class10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021288</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.062970</td>\n",
       "      <td>0.233646</td>\n",
       "      <td>0.191179</td>\n",
       "      <td>0.220710</td>\n",
       "      <td>0.092044</td>\n",
       "      <td>0.023819</td>\n",
       "      <td>0.100393</td>\n",
       "      <td>0.028525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021287</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.062970</td>\n",
       "      <td>0.233640</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.220713</td>\n",
       "      <td>0.092043</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.100394</td>\n",
       "      <td>0.028526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021287</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.062970</td>\n",
       "      <td>0.233640</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.220713</td>\n",
       "      <td>0.092043</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.100394</td>\n",
       "      <td>0.028526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021287</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.062970</td>\n",
       "      <td>0.233640</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.220713</td>\n",
       "      <td>0.092043</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.100394</td>\n",
       "      <td>0.028526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021287</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.062970</td>\n",
       "      <td>0.233640</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.220713</td>\n",
       "      <td>0.092043</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.100394</td>\n",
       "      <td>0.028526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021288</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.062970</td>\n",
       "      <td>0.233646</td>\n",
       "      <td>0.191179</td>\n",
       "      <td>0.220710</td>\n",
       "      <td>0.092044</td>\n",
       "      <td>0.023819</td>\n",
       "      <td>0.100393</td>\n",
       "      <td>0.028525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021288</td>\n",
       "      <td>0.010639</td>\n",
       "      <td>0.062970</td>\n",
       "      <td>0.233653</td>\n",
       "      <td>0.191175</td>\n",
       "      <td>0.220707</td>\n",
       "      <td>0.092045</td>\n",
       "      <td>0.023819</td>\n",
       "      <td>0.100392</td>\n",
       "      <td>0.028524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021322</td>\n",
       "      <td>0.010637</td>\n",
       "      <td>0.063017</td>\n",
       "      <td>0.233377</td>\n",
       "      <td>0.191037</td>\n",
       "      <td>0.220966</td>\n",
       "      <td>0.092145</td>\n",
       "      <td>0.023786</td>\n",
       "      <td>0.100391</td>\n",
       "      <td>0.028516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021322</td>\n",
       "      <td>0.010637</td>\n",
       "      <td>0.063017</td>\n",
       "      <td>0.233377</td>\n",
       "      <td>0.191037</td>\n",
       "      <td>0.220966</td>\n",
       "      <td>0.092145</td>\n",
       "      <td>0.023786</td>\n",
       "      <td>0.100391</td>\n",
       "      <td>0.028516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021288</td>\n",
       "      <td>0.010639</td>\n",
       "      <td>0.062970</td>\n",
       "      <td>0.233653</td>\n",
       "      <td>0.191175</td>\n",
       "      <td>0.220707</td>\n",
       "      <td>0.092045</td>\n",
       "      <td>0.023819</td>\n",
       "      <td>0.100392</td>\n",
       "      <td>0.028524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021288</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.062970</td>\n",
       "      <td>0.233646</td>\n",
       "      <td>0.191179</td>\n",
       "      <td>0.220710</td>\n",
       "      <td>0.092044</td>\n",
       "      <td>0.023819</td>\n",
       "      <td>0.100393</td>\n",
       "      <td>0.028525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021287</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.062970</td>\n",
       "      <td>0.233640</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.220713</td>\n",
       "      <td>0.092043</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.100394</td>\n",
       "      <td>0.028526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021286</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.062971</td>\n",
       "      <td>0.233633</td>\n",
       "      <td>0.191186</td>\n",
       "      <td>0.220717</td>\n",
       "      <td>0.092043</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.100395</td>\n",
       "      <td>0.028527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021286</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.062971</td>\n",
       "      <td>0.233633</td>\n",
       "      <td>0.191186</td>\n",
       "      <td>0.220717</td>\n",
       "      <td>0.092043</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.100395</td>\n",
       "      <td>0.028527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021287</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.062970</td>\n",
       "      <td>0.233640</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.220713</td>\n",
       "      <td>0.092043</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.100394</td>\n",
       "      <td>0.028526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021288</td>\n",
       "      <td>0.010639</td>\n",
       "      <td>0.062970</td>\n",
       "      <td>0.233653</td>\n",
       "      <td>0.191175</td>\n",
       "      <td>0.220707</td>\n",
       "      <td>0.092045</td>\n",
       "      <td>0.023819</td>\n",
       "      <td>0.100392</td>\n",
       "      <td>0.028524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021287</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.062970</td>\n",
       "      <td>0.233640</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.220713</td>\n",
       "      <td>0.092043</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.100394</td>\n",
       "      <td>0.028526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021286</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.062971</td>\n",
       "      <td>0.233633</td>\n",
       "      <td>0.191186</td>\n",
       "      <td>0.220717</td>\n",
       "      <td>0.092043</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.100395</td>\n",
       "      <td>0.028527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021285</td>\n",
       "      <td>0.010637</td>\n",
       "      <td>0.062971</td>\n",
       "      <td>0.233627</td>\n",
       "      <td>0.191189</td>\n",
       "      <td>0.220720</td>\n",
       "      <td>0.092042</td>\n",
       "      <td>0.023817</td>\n",
       "      <td>0.100396</td>\n",
       "      <td>0.028528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021285</td>\n",
       "      <td>0.010637</td>\n",
       "      <td>0.062971</td>\n",
       "      <td>0.233627</td>\n",
       "      <td>0.191189</td>\n",
       "      <td>0.220720</td>\n",
       "      <td>0.092042</td>\n",
       "      <td>0.023817</td>\n",
       "      <td>0.100396</td>\n",
       "      <td>0.028528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021286</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.062971</td>\n",
       "      <td>0.233633</td>\n",
       "      <td>0.191186</td>\n",
       "      <td>0.220717</td>\n",
       "      <td>0.092043</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.100395</td>\n",
       "      <td>0.028527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021287</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.062970</td>\n",
       "      <td>0.233640</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.220713</td>\n",
       "      <td>0.092043</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.100394</td>\n",
       "      <td>0.028526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021322</td>\n",
       "      <td>0.010637</td>\n",
       "      <td>0.063017</td>\n",
       "      <td>0.233377</td>\n",
       "      <td>0.191037</td>\n",
       "      <td>0.220966</td>\n",
       "      <td>0.092145</td>\n",
       "      <td>0.023786</td>\n",
       "      <td>0.100391</td>\n",
       "      <td>0.028516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021322</td>\n",
       "      <td>0.010637</td>\n",
       "      <td>0.063017</td>\n",
       "      <td>0.233377</td>\n",
       "      <td>0.191037</td>\n",
       "      <td>0.220966</td>\n",
       "      <td>0.092145</td>\n",
       "      <td>0.023786</td>\n",
       "      <td>0.100391</td>\n",
       "      <td>0.028516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021322</td>\n",
       "      <td>0.010637</td>\n",
       "      <td>0.063017</td>\n",
       "      <td>0.233377</td>\n",
       "      <td>0.191037</td>\n",
       "      <td>0.220966</td>\n",
       "      <td>0.092145</td>\n",
       "      <td>0.023786</td>\n",
       "      <td>0.100391</td>\n",
       "      <td>0.028516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021287</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.062970</td>\n",
       "      <td>0.233640</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.220713</td>\n",
       "      <td>0.092043</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.100394</td>\n",
       "      <td>0.028526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021287</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>0.062970</td>\n",
       "      <td>0.233640</td>\n",
       "      <td>0.191183</td>\n",
       "      <td>0.220713</td>\n",
       "      <td>0.092043</td>\n",
       "      <td>0.023818</td>\n",
       "      <td>0.100394</td>\n",
       "      <td>0.028526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021285</td>\n",
       "      <td>0.010637</td>\n",
       "      <td>0.062971</td>\n",
       "      <td>0.233627</td>\n",
       "      <td>0.191189</td>\n",
       "      <td>0.220720</td>\n",
       "      <td>0.092042</td>\n",
       "      <td>0.023817</td>\n",
       "      <td>0.100396</td>\n",
       "      <td>0.028528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021310</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.062986</td>\n",
       "      <td>0.233591</td>\n",
       "      <td>0.191080</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.092092</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>0.100381</td>\n",
       "      <td>0.028513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0_test  1_test  2_test  3_test  4_test  5_test  6_test  7_test  8_test  \\\n",
       "0      1.0     0.0     0.0     0.0     1.0     1.0     1.0     1.0     0.0   \n",
       "1      0.0     0.0     0.0     0.0     1.0     1.0     0.0     1.0     0.0   \n",
       "2      0.0     0.0     0.0     0.0     1.0     1.0     0.0     1.0     0.0   \n",
       "3      0.0     0.0     0.0     0.0     1.0     1.0     0.0     1.0     0.0   \n",
       "4      0.0     0.0     0.0     0.0     1.0     1.0     1.0     1.0     0.0   \n",
       "5      0.0     0.0     0.0     0.0     1.0     1.0     0.0     1.0     0.0   \n",
       "6      0.0     0.0     0.0     0.0     1.0     1.0     0.0     0.0     0.0   \n",
       "7      0.0     0.0     0.0     0.0     1.0     1.0     0.0     0.0     0.0   \n",
       "8      0.0     0.0     0.0     0.0     1.0     1.0     1.0     0.0     0.0   \n",
       "9      0.0     0.0     0.0     0.0     1.0     0.0     1.0     1.0     0.0   \n",
       "10     0.0     0.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0   \n",
       "11     0.0     0.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0   \n",
       "12     0.0     0.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0   \n",
       "13     0.0     0.0     0.0     0.0     1.0     0.0     1.0     1.0     0.0   \n",
       "14     0.0     0.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0   \n",
       "15     0.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "16     0.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "17     0.0     0.0     0.0     0.0     1.0     0.0     1.0     0.0     0.0   \n",
       "18     0.0     0.0     0.0     0.0     1.0     0.0     1.0     1.0     0.0   \n",
       "19     0.0     0.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0   \n",
       "20     0.0     0.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0   \n",
       "21     0.0     0.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0   \n",
       "22     0.0     0.0     0.0     0.0     1.0     0.0     1.0     1.0     0.0   \n",
       "23     0.0     0.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0   \n",
       "24     0.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "25     0.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "26     0.0     0.0     0.0     0.0     1.0     0.0     1.0     0.0     0.0   \n",
       "27     0.0     0.0     0.0     0.0     1.0     0.0     1.0     1.0     0.0   \n",
       "28     0.0     0.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0   \n",
       "29     0.0     0.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0   \n",
       "30     0.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "31     0.0     0.0     0.0     0.0     1.0     0.0     1.0     0.0     0.0   \n",
       "32     0.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "33     0.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "34     0.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "35     0.0     0.0     0.0     0.0     1.0     0.0     1.0     0.0     0.0   \n",
       "36     0.0     0.0     0.0     0.0     1.0     1.0     1.0     1.0     0.0   \n",
       "37     0.0     0.0     0.0     0.0     1.0     1.0     0.0     1.0     0.0   \n",
       "38     0.0     0.0     0.0     0.0     1.0     1.0     0.0     1.0     0.0   \n",
       "39     0.0     0.0     0.0     0.0     1.0     1.0     0.0     0.0     0.0   \n",
       "40     0.0     0.0     0.0     0.0     1.0     1.0     1.0     0.0     0.0   \n",
       "41     0.0     0.0     0.0     0.0     1.0     1.0     0.0     0.0     0.0   \n",
       "42     0.0     0.0     0.0     0.0     1.0     1.0     0.0     0.0     0.0   \n",
       "43     0.0     0.0     0.0     0.0     1.0     1.0     0.0     0.0     0.0   \n",
       "44     0.0     0.0     0.0     0.0     1.0     1.0     1.0     0.0     0.0   \n",
       "45     0.0     0.0     0.0     0.0     1.0     0.0     1.0     1.0     0.0   \n",
       "46     0.0     0.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0   \n",
       "47     0.0     0.0     0.0     0.0     1.0     0.0     0.0     1.0     0.0   \n",
       "48     0.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "49     0.0     0.0     0.0     0.0     1.0     0.0     1.0     0.0     0.0   \n",
       "\n",
       "    9_test  ...    Class1    Class2    Class3    Class4    Class5    Class6  \\\n",
       "0      1.0  ...  0.021288  0.010638  0.062970  0.233646  0.191179  0.220710   \n",
       "1      1.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "2      0.0  ...  0.021287  0.010638  0.062970  0.233640  0.191183  0.220713   \n",
       "3      1.0  ...  0.021287  0.010638  0.062970  0.233640  0.191183  0.220713   \n",
       "4      1.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "5      0.0  ...  0.021287  0.010638  0.062970  0.233640  0.191183  0.220713   \n",
       "6      1.0  ...  0.021287  0.010638  0.062970  0.233640  0.191183  0.220713   \n",
       "7      1.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "8      0.0  ...  0.021288  0.010638  0.062970  0.233646  0.191179  0.220710   \n",
       "9      1.0  ...  0.021288  0.010639  0.062970  0.233653  0.191175  0.220707   \n",
       "10     0.0  ...  0.021322  0.010637  0.063017  0.233377  0.191037  0.220966   \n",
       "11     0.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "12     1.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "13     0.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "14     0.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "15     1.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "16     0.0  ...  0.021322  0.010637  0.063017  0.233377  0.191037  0.220966   \n",
       "17     0.0  ...  0.021288  0.010639  0.062970  0.233653  0.191175  0.220707   \n",
       "18     0.0  ...  0.021288  0.010638  0.062970  0.233646  0.191179  0.220710   \n",
       "19     0.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "20     0.0  ...  0.021287  0.010638  0.062970  0.233640  0.191183  0.220713   \n",
       "21     0.0  ...  0.021286  0.010638  0.062971  0.233633  0.191186  0.220717   \n",
       "22     0.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "23     0.0  ...  0.021286  0.010638  0.062971  0.233633  0.191186  0.220717   \n",
       "24     0.0  ...  0.021287  0.010638  0.062970  0.233640  0.191183  0.220713   \n",
       "25     0.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "26     0.0  ...  0.021288  0.010639  0.062970  0.233653  0.191175  0.220707   \n",
       "27     1.0  ...  0.021287  0.010638  0.062970  0.233640  0.191183  0.220713   \n",
       "28     1.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "29     0.0  ...  0.021286  0.010638  0.062971  0.233633  0.191186  0.220717   \n",
       "30     1.0  ...  0.021285  0.010637  0.062971  0.233627  0.191189  0.220720   \n",
       "31     1.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "32     0.0  ...  0.021285  0.010637  0.062971  0.233627  0.191189  0.220720   \n",
       "33     1.0  ...  0.021286  0.010638  0.062971  0.233633  0.191186  0.220717   \n",
       "34     1.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "35     0.0  ...  0.021287  0.010638  0.062970  0.233640  0.191183  0.220713   \n",
       "36     1.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "37     0.0  ...  0.021322  0.010637  0.063017  0.233377  0.191037  0.220966   \n",
       "38     0.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "39     1.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "40     0.0  ...  0.021322  0.010637  0.063017  0.233377  0.191037  0.220966   \n",
       "41     0.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "42     1.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "43     0.0  ...  0.021322  0.010637  0.063017  0.233377  0.191037  0.220966   \n",
       "44     0.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "45     0.0  ...  0.021287  0.010638  0.062970  0.233640  0.191183  0.220713   \n",
       "46     0.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "47     0.0  ...  0.021287  0.010638  0.062970  0.233640  0.191183  0.220713   \n",
       "48     0.0  ...  0.021285  0.010637  0.062971  0.233627  0.191189  0.220720   \n",
       "49     0.0  ...  0.021310  0.010642  0.062986  0.233591  0.191080  0.220800   \n",
       "\n",
       "      Class7    Class8    Class9   Class10  \n",
       "0   0.092044  0.023819  0.100393  0.028525  \n",
       "1   0.092092  0.023809  0.100381  0.028513  \n",
       "2   0.092043  0.023818  0.100394  0.028526  \n",
       "3   0.092043  0.023818  0.100394  0.028526  \n",
       "4   0.092092  0.023809  0.100381  0.028513  \n",
       "5   0.092043  0.023818  0.100394  0.028526  \n",
       "6   0.092043  0.023818  0.100394  0.028526  \n",
       "7   0.092092  0.023809  0.100381  0.028513  \n",
       "8   0.092044  0.023819  0.100393  0.028525  \n",
       "9   0.092045  0.023819  0.100392  0.028524  \n",
       "10  0.092145  0.023786  0.100391  0.028516  \n",
       "11  0.092092  0.023809  0.100381  0.028513  \n",
       "12  0.092092  0.023809  0.100381  0.028513  \n",
       "13  0.092092  0.023809  0.100381  0.028513  \n",
       "14  0.092092  0.023809  0.100381  0.028513  \n",
       "15  0.092092  0.023809  0.100381  0.028513  \n",
       "16  0.092145  0.023786  0.100391  0.028516  \n",
       "17  0.092045  0.023819  0.100392  0.028524  \n",
       "18  0.092044  0.023819  0.100393  0.028525  \n",
       "19  0.092092  0.023809  0.100381  0.028513  \n",
       "20  0.092043  0.023818  0.100394  0.028526  \n",
       "21  0.092043  0.023818  0.100395  0.028527  \n",
       "22  0.092092  0.023809  0.100381  0.028513  \n",
       "23  0.092043  0.023818  0.100395  0.028527  \n",
       "24  0.092043  0.023818  0.100394  0.028526  \n",
       "25  0.092092  0.023809  0.100381  0.028513  \n",
       "26  0.092045  0.023819  0.100392  0.028524  \n",
       "27  0.092043  0.023818  0.100394  0.028526  \n",
       "28  0.092092  0.023809  0.100381  0.028513  \n",
       "29  0.092043  0.023818  0.100395  0.028527  \n",
       "30  0.092042  0.023817  0.100396  0.028528  \n",
       "31  0.092092  0.023809  0.100381  0.028513  \n",
       "32  0.092042  0.023817  0.100396  0.028528  \n",
       "33  0.092043  0.023818  0.100395  0.028527  \n",
       "34  0.092092  0.023809  0.100381  0.028513  \n",
       "35  0.092043  0.023818  0.100394  0.028526  \n",
       "36  0.092092  0.023809  0.100381  0.028513  \n",
       "37  0.092145  0.023786  0.100391  0.028516  \n",
       "38  0.092092  0.023809  0.100381  0.028513  \n",
       "39  0.092092  0.023809  0.100381  0.028513  \n",
       "40  0.092145  0.023786  0.100391  0.028516  \n",
       "41  0.092092  0.023809  0.100381  0.028513  \n",
       "42  0.092092  0.023809  0.100381  0.028513  \n",
       "43  0.092145  0.023786  0.100391  0.028516  \n",
       "44  0.092092  0.023809  0.100381  0.028513  \n",
       "45  0.092043  0.023818  0.100394  0.028526  \n",
       "46  0.092092  0.023809  0.100381  0.028513  \n",
       "47  0.092043  0.023818  0.100394  0.028526  \n",
       "48  0.092042  0.023817  0.100396  0.028528  \n",
       "49  0.092092  0.023809  0.100381  0.028513  \n",
       "\n",
       "[50 rows x 22 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features={\"no_of_max\":data_train[\"no_of_max\"].to_numpy()}\n",
    "label=data_train[label_name].to_numpy()\n",
    "#print(label.reshape((len(label),6,3)))\n",
    "\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "evaluation=my_model.evaluate(x = features, y = label, batch_size=batch_size)\n",
    "\n",
    "predicted = my_model.predict(features)\n",
    "\n",
    "#label=label.reshape((len(label),6,3))\n",
    "\n",
    "#predicted=predicted.reshape((len(predicted),6,3))\n",
    "print(predicted.shape)\n",
    "\n",
    "\n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "df_test=pd.DataFrame(label,columns=[\"0_test\",\"1_test\",\"2_test\",\"3_test\",\"4_test\",\"5_test\",\"6_test\",\"7_test\",\"8_test\",\"9_test\",\"10_test\"])\n",
    "#df_test=pd.DataFrame(label,columns=[\"k6a1_0.25_label\",\"k6a1_0.5_label\",\"k6a1_0.75_label\"])\n",
    "df_predict=pd.DataFrame(predicted,columns=[\"Class0\",\"Class1\",\"Class2\",\"Class3\",\"Class4\",\"Class5\",\"Class6\",\"Class7\",\"Class8\",\"Class9\",\"Class10\"])\n",
    "#df_predict=pd.DataFrame(predicted,columns=[\"k6a1_0.25\",\"k6a1_0.5\",\"k6a1_0.75\"])\n",
    "#df_test = df_test.round(0)\n",
    "#df_predict = df_predict.round(0)\n",
    "pd.concat([df_test,df_predict], axis=1).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_metrics_to_plot = ['accuracy', 'precision', 'recall'] \n",
    "\n",
    "plot_curve(epochs_list, hist, list_of_metrics_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=pd.concat([df_test,df_predict], axis=1).corr()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "sns.heatmap(corr, mask=mask, cmap=\"viridis\", vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train for each segment indepently\n",
    "\n",
    "#values between 0.75 and 2.25\n",
    "#different classes:\n",
    "#category 1\n",
    "#0 all < 0.5\n",
    "#1 all > 0.5\n",
    "\n",
    "\n",
    "#category 2\n",
    "#0 all < 0.5\n",
    "#1 all > 0.5\n",
    "#2 k6a1,k11,k9a1 >0.5, k6a2,k12,k9a2 <0.5\n",
    "#3 k6a1,k11,k9a1 <0.5, k6a2,k12,k9a2 >0.5\n",
    "\n",
    "\n",
    "\n",
    "#category 3\n",
    "#sum(k6a1,k11,k9a1) < 1.25\n",
    "#sum(k6a1,k11,k9a1) > 2\n",
    "\n",
    "#category 4\n",
    "#sum(k6a2,k12,k9a2) < 1.25\n",
    "#sum(k6a2,k12,k9a2) > 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
