{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#import kerastuner as kt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only the number of maxima\n",
    "df_spectra_all=pd.read_csv(\"spectrum_energy_input_numberOfPeaks.csv\",index_col=[0])\n",
    "all_data = df_spectra_all[[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\",\"no_of_max\"]]\n",
    "# drop all rows containing 0.5 to make it binary\n",
    "print(len(all_data))\n",
    "all_data = all_data.query('k6a1 != 0.5 & k6a2 != 0.5 & k11 != 0.5 & k12 != 0.5 & k9a1 != 0.5 & k9a2 != 0.5')\n",
    "print(len(all_data))\n",
    "#set the values to zero and one\n",
    "#labels = ['k6a1','k6a2','k11','k12','k9a1','k9a2']\n",
    "#[all_data[i].mask(all_data[i] == 0.25, 0, inplace=True) for i in labels]\n",
    "#[all_data[i].mask(all_data[i] == 0.75, 1, inplace=True) for i in labels]\n",
    "#print(all_data.head(10))\n",
    "data_train, data_test = train_test_split(all_data, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns_A = []\n",
    "\n",
    "no_of_max = tf.feature_column.numeric_column(\"no_of_max\")\n",
    "#my_feature_layer_A = tf.keras.layers.DenseFeatures(no_of_max_bucket)\n",
    "feature_columns_A.append(no_of_max)\n",
    "k6a1 = tf.feature_column.numeric_column(\"k6a1\")\n",
    "feature_columns_A.append(k6a1)\n",
    "k6a2 = tf.feature_column.numeric_column(\"k6a2\")\n",
    "feature_columns_A.append(k6a2)\n",
    "k11 = tf.feature_column.numeric_column(\"k11\")\n",
    "feature_columns_A.append(k11)\n",
    "k12 = tf.feature_column.numeric_column(\"k12\")\n",
    "feature_columns_A.append(k12)\n",
    "k9a1 = tf.feature_column.numeric_column(\"k9a1\")\n",
    "feature_columns_A.append(k9a1)\n",
    "k9a2 = tf.feature_column.numeric_column(\"k9a2\")\n",
    "feature_columns_A.append(k9a2)\n",
    "\n",
    "my_feature_layer_A = tf.keras.layers.DenseFeatures(feature_columns_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "    \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "    for m in list_of_metrics:\n",
    "        x = hist[m]\n",
    "        plt.plot(epochs[1:], x[1:], label=m)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for activation functions check https://keras.io/api/layers/activations/\n",
    "def create_model2(my_learning_rate, my_feature_layer,my_metrics,my_act_function = \"softmax\"):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(my_feature_layer)\n",
    "    layers=[20,12]\n",
    "    for layer in layers:\n",
    "        model.add(tf.keras.layers.Dense(units = layer, activation = my_act_function))\n",
    "    model.add(tf.keras.layers.Dense(units=6,name='Output', activation = 'softmax'))                             \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),                                       \n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics=my_metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, dataset, epochs, label_name,\n",
    "                batch_size=None,shuffle=True):\n",
    "    features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    label=dataset[label_name].to_numpy()\n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle)\n",
    "  \n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    return epochs, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "epochs = 50\n",
    "batch_size = 12\n",
    "\n",
    "#specify the classification threshold\n",
    "classification_threshold = 0.15\n",
    "\n",
    "# Establish the metrics the model will measure.\n",
    "metric = [tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),\n",
    "      tf.keras.metrics.Precision(thresholds=classification_threshold,name='precision'),\n",
    "      tf.keras.metrics.Recall(thresholds=classification_threshold,name='recall'),]\n",
    "\n",
    "label_name = [\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"]\n",
    "#label_name = \"k6a1\"\n",
    "my_model = create_model2(learning_rate, my_feature_layer_A,metric,my_act_function=\"sigmoid\")\n",
    "epochs, hist = train_model(my_model, data_train, epochs, \n",
    "                          label_name, batch_size)\n",
    "# Plot a graph of the metric(s) vs. epochs.\n",
    "#list_of_metrics_to_plot = ['accuracy'] \n",
    "list_of_metrics_to_plot = ['accuracy', 'precision', 'recall'] \n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {name:np.array(value) for name, value in data_test.items()}\n",
    "label=data_test[label_name].to_numpy()\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "evaluation=my_model.evaluate(x = features, y = label, batch_size=batch_size)\n",
    "predicted = my_model.predict(features)\n",
    "print(predicted)\n",
    "df_test=pd.DataFrame(label,columns=[\"k6a1_test\",\"k6a2_test\",\"k11_test\",\"k12_test\",\"k9a1_test\",\"k9a2_test\"])\n",
    "#df_test=pd.DataFrame(label,columns=[\"k6a1_test\"])\n",
    "df_predict=pd.DataFrame(predicted,columns=[\"k6a1_hat\",\"k6a2_hat\",\"k11_hat\",\"k12_hat\",\"k9a1_hat\",\"k9a2_hat\"])\n",
    "#df_predict=pd.DataFrame(predicted,columns=[\"k6a1_hat\"])\n",
    "#df_test = df_test.round(0)\n",
    "#df_predict = df_predict.round(0)\n",
    "pd.concat([df_test,df_predict], axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umbauen in ein echtes Klassifizierungsproblem\n",
    "\n",
    "see https://sebastianraschka.com/faq/docs/softmax_regression.html\n",
    "\n",
    "Wir haben für jede Konstante drei mögliche Werte: 0.25, 0.5 und 0.75 - das ist somit ein \"ternary\" (dreifaltiges?) Klassifikationsproblem. Die softmax Funktion am Ende des Modells gibt eine Wahrscheinlichkeit an, inwiefern das \"feature\" zu welcher Klasse gehört (prozentual). \n",
    "\n",
    "Ich würde mal anfangen und für k6a1 ein Modell bauen mit one-hot encoding für die drei möglichen Werte und als feature die Anzahl der Peaks. Das gleiche für ausschliesslich k6a2, k11, k12, k9a1, k9a2. Vermutlich wird das nicht so gut funktionieren weil die Information einfach nicht ausreichend ist. Da würde ich mal versuchen zu verstehen, welche Vorhersagen dir das Modell gibt, und dass du die Wahrscheinlichkeiten für die drei möglichen Klassen bekommst, für jedes Beispiel.\n",
    "\n",
    "Im zweiten Schritt würde ich dann andere Klassen-Kombinationen probieren:\n",
    "Klasse 1 - k6a1, k11, k9a1 \n",
    "Klasse 2 - k6a2, k12, k9a2 \n",
    "\n",
    "Sagen wir, die jeweiligen Werte liegen bei -1 (momentan 0.25), 0 (momentan 0.5), 1 (momentan 0.75). Die Summe aller Werte liegt damit zwischen -3 und +3.\n",
    "\n",
    "Dann wäre Fall A: sum(Klasse 1) < -1; Fall B: -1 < sum(Klasse 1) < 1; Fall C: sum(Klasse 1) > 1. Mit den Grenzen kann man etwas herumspielen; und das gleiche jeweils für Klasse 2.\n",
    "\n",
    "Oder vielleicht hast du noch andere Ideen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#import kerastuner as kt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spectra_all=pd.read_csv(\"spectrum_energy_input_numberOfPeaks.csv\",index_col=[0])\n",
    "all_data = df_spectra_all[[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\",\"no_of_max\"]]\n",
    "\n",
    "all_data=pd.get_dummies(all_data,columns=[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"])\n",
    "\n",
    "\n",
    "#all_data[\"k6a1\"]=all_data[[\"k6a1_0.25\",\"k6a1_0.5\",\"k6a1_0.75\"]].values.tolist()\n",
    "#all_data[\"k6a2\"]=all_data[[\"k6a1_0.25\",\"k6a1_0.5\",\"k6a1_0.75\"]].values.tolist()\n",
    "#all_data[\"k11\"]=all_data[[\"k11_0.25\",\"k11_0.5\",\"k11_0.75\"]].values.tolist()\n",
    "#all_data[\"k12\"]=all_data[[\"k12_0.25\",\"k12_0.5\",\"k12_0.75\"]].values.tolist()\n",
    "#all_data[\"k9a1\"]=all_data[[\"k9a1_0.25\",\"k9a1_0.5\",\"k9a1_0.75\"]].values.tolist()\n",
    "#all_data[\"k9a2\"]=all_data[[\"k9a2_0.25\",\"k9a2_0.5\",\"k9a2_0.75\"]].values.tolist()\n",
    "#all_data=all_data.drop(columns=[\"k6a1_0.25\",\"k6a1_0.5\",\"k6a1_0.75\",\"k6a2_0.25\",\"k6a2_0.5\",\"k6a2_0.75\",\"k11_0.25\",\n",
    "#                                \"k11_0.5\",\"k11_0.75\",\"k12_0.25\",\"k12_0.5\",\"k12_0.75\",\"k9a1_0.25\",\"k9a1_0.5\",\n",
    "#                                \"k9a1_0.75\",\"k9a2_0.25\",\"k9a2_0.5\",\"k9a2_0.75\"])\n",
    "\n",
    "data_train, data_test = train_test_split(all_data, test_size=0.20, random_state=42)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     49
    ]
   },
   "outputs": [],
   "source": [
    "feature_columns_A = []\n",
    "\n",
    "no_of_max = tf.feature_column.numeric_column(\"no_of_max\")\n",
    "#my_feature_layer_A = tf.keras.layers.DenseFeatures(no_of_max_bucket)\n",
    "feature_columns_A.append(no_of_max)\n",
    "#k6a1 = tf.feature_column.numeric_column(\"k6a1\")\n",
    "#feature_columns_A.append(k6a1)\n",
    "#k6a2 = tf.feature_column.numeric_column(\"k6a2\")\n",
    "#feature_columns_A.append(k6a2)\n",
    "#k11 = tf.feature_column.numeric_column(\"k11\")\n",
    "#feature_columns_A.append(k11)\n",
    "#k12 = tf.feature_column.numeric_column(\"k12\")\n",
    "#feature_columns_A.append(k12)\n",
    "#k9a1 = tf.feature_column.numeric_column(\"k9a1\")\n",
    "#feature_columns_A.append(k9a1)\n",
    "#k9a2 = tf.feature_column.numeric_column(\"k9a2\")\n",
    "#feature_columns_A.append(k9a2)\n",
    "\n",
    "my_feature_layer_A = tf.keras.layers.DenseFeatures(feature_columns_A)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "    \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "    for m in list_of_metrics:\n",
    "        x = hist[m]\n",
    "        plt.plot(epochs[1:], x[1:], label=m)\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "# for activation functions check https://keras.io/api/layers/activations/\n",
    "def create_model2(my_learning_rate, my_feature_layer,my_metrics,my_act_function = \"softmax\"):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(my_feature_layer)\n",
    "    layers=[20,12]\n",
    "    for layer in layers:\n",
    "        model.add(tf.keras.layers.Dense(units = layer, activation = my_act_function))\n",
    "    model.add(tf.keras.layers.Dense(units=18,name='Output', activation = 'softmax'))                             \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),                                       \n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics=my_metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, dataset, epochs, label_name,\n",
    "                batch_size=None,shuffle=True):\n",
    "    #features = {name:np.array(np.asarray(value)) for name, value in dataset.items()}\n",
    "\n",
    "    features={\"no_of_max\":dataset[\"no_of_max\"].to_numpy()}\n",
    "   # print(features)\n",
    "\n",
    "    #for multiple inputs\n",
    "    print(type(label_name))\n",
    "    if(type(label_name)==list):\n",
    "        label=dataset[label_name].to_numpy()\n",
    "        label_array=np.zeros((label.shape[0],label.shape[1],3))\n",
    "        for i in range(label.shape[0]):\n",
    "            for j in range(label.shape[1]):\n",
    "                for k in range(len(label[i,j])):\n",
    "                    label_array[i,j,k]=label[i,j][k]\n",
    "            #print(label_array[i,j])\n",
    "           # print(type(label_array[i,j]))\n",
    "        print(label_array.shape) \n",
    "        label=label_array\n",
    "    else:\n",
    "        label=dataset[label_name].to_numpy()\n",
    "        label_array=np.zeros((label.shape[0],3))\n",
    "        for i in range(label.shape[0]):\n",
    "            for k in range(len(label[i])):\n",
    "                label_array[i,k]=label[i][k]\n",
    "          \n",
    "        print(label_array.shape) \n",
    "        label=label_array\n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle)\n",
    "  \n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    return epochs, hist\n",
    "\n",
    "\n",
    "def train_model2(model, dataset, epochs, label_name,\n",
    "                batch_size=None,shuffle=True):\n",
    "    #features = {name:np.array(np.asarray(value)) for name, value in dataset.items()}\n",
    "\n",
    "    features={\"no_of_max\":dataset[\"no_of_max\"].to_numpy()}\n",
    "   # print(features)\n",
    "    label=dataset[label_name].to_numpy()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle)\n",
    "  \n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    return epochs, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "epochs = 500\n",
    "batch_size = 12\n",
    "\n",
    "#specify the classification threshold\n",
    "classification_threshold = 0.15\n",
    "\n",
    "# Establish the metrics the model will measure.\n",
    "metric = [tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),\n",
    "      tf.keras.metrics.Precision(thresholds=classification_threshold,name='precision'),\n",
    "      tf.keras.metrics.Recall(thresholds=classification_threshold,name='recall'),]\n",
    "\n",
    "#label_name = [\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"]\n",
    "#label_name = \"k6a1\"\n",
    "label_name=[\"k6a1_0.25\",\"k6a1_0.5\",\"k6a1_0.75\",\"k6a2_0.25\",\"k6a2_0.5\",\"k6a2_0.75\",\"k11_0.25\",\"k11_0.5\",\"k11_0.75\",\"k12_0.25\",\"k12_0.5\",\"k12_0.75\",\"k9a1_0.25\",\"k9a1_0.5\",\"k9a1_0.75\",\"k9a2_0.25\",\"k9a2_0.5\",\"k9a2_0.75\"]\n",
    "my_model = create_model2(learning_rate, my_feature_layer_A,metric,my_act_function=\"softmax\")\n",
    "\n",
    "# Plot a graph of the metric(s) vs. epochs.\n",
    "#list_of_metrics_to_plot = ['accuracy'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs, hist = train_model2(my_model, data_train, epochs, \n",
    "                          label_name, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_metrics_to_plot = ['accuracy', 'precision', 'recall'] \n",
    "\n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if(type(label_name)==list):\n",
    "    label=data_test[label_name].to_numpy()\n",
    "    label_array=np.zeros((label.shape[0],label.shape[1],3))\n",
    "    for i in range(label.shape[0]):\n",
    "        for j in range(label.shape[1]):\n",
    "            for k in range(len(label[i,j])):\n",
    "                label_array[i,j,k]=label[i,j][k]\n",
    "        #print(label_array[i,j])\n",
    "       # print(type(label_array[i,j]))\n",
    "    #print(label_array.shape) \n",
    "    label=label_array\n",
    "else:\n",
    "    label=data_test[label_name].to_numpy()\n",
    "    label_array=np.zeros((label.shape[0],3))\n",
    "    for i in range(label.shape[0]):\n",
    "        for k in range(len(label[i])):\n",
    "            label_array[i,k]=label[i][k]\n",
    "\n",
    "    #print(label_array.shape) \n",
    "    label=label_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     12
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features={\"no_of_max\":data_test[\"no_of_max\"].to_numpy()}\n",
    "label=data_test[label_name].to_numpy()\n",
    "#print(label.reshape((len(label),6,3)))\n",
    "\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "evaluation=my_model.evaluate(x = features, y = label, batch_size=batch_size)\n",
    "\n",
    "predicted = my_model.predict(features)\n",
    "\n",
    "label=label.reshape((len(label),6,3))\n",
    "\n",
    "predicted=predicted.reshape((len(predicted),6,3))\n",
    "print(predicted.shape)\n",
    "\n",
    "label_list=[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"]\n",
    "for i in range(predicted.shape[0]):\n",
    "    print(\"Test No:\", i)\n",
    "    for j in range(predicted.shape[1]):\n",
    "        #print(1/sum(predicted[i,j]))\n",
    "        print(label_list[j],\"Label: \",label[i,j],\"predicted: \", predicted[i,j]*1/sum(predicted[i,j]) )\n",
    "print()\n",
    "\n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "df_test=pd.DataFrame(label,columns=[\"k6a1_test\",\"k6a2_test\",\"k11_test\",\"k12_test\",\"k9a1_test\",\"k9a2_test\"])\n",
    "#df_test=pd.DataFrame(label,columns=[\"k6a1_0.25_label\",\"k6a1_0.5_label\",\"k6a1_0.75_label\"])\n",
    "df_predict=pd.DataFrame(predicted,columns=[\"k6a1_hat\",\"k6a2_hat\",\"k11_hat\",\"k12_hat\",\"k9a1_hat\",\"k9a2_hat\"])\n",
    "#df_predict=pd.DataFrame(predicted,columns=[\"k6a1_0.25\",\"k6a1_0.5\",\"k6a1_0.75\"])\n",
    "#df_test = df_test.round(0)\n",
    "#df_predict = df_predict.round(0)\n",
    "pd.concat([df_test,df_predict], axis=1).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#class predictions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#import kerastuner as kt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   k6a1  k6a2   k11   k12  k9a1  k9a2  no_of_max\n",
      "0  0.25  0.25  0.25  0.25  0.25  0.25         18\n",
      "1  0.25  0.25  0.25  0.25  0.25  0.50          5\n",
      "2  0.25  0.25  0.25  0.25  0.25  0.75         19\n",
      "3  0.25  0.25  0.25  0.25  0.50  0.25         19\n",
      "4  0.25  0.25  0.25  0.25  0.50  0.50          5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:845: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k6a1</th>\n",
       "      <th>k6a2</th>\n",
       "      <th>k11</th>\n",
       "      <th>k12</th>\n",
       "      <th>k9a1</th>\n",
       "      <th>k9a2</th>\n",
       "      <th>no_of_max</th>\n",
       "      <th>Class0</th>\n",
       "      <th>Class1</th>\n",
       "      <th>Class2</th>\n",
       "      <th>Class3</th>\n",
       "      <th>Class4</th>\n",
       "      <th>Class5</th>\n",
       "      <th>Class6</th>\n",
       "      <th>Class7</th>\n",
       "      <th>Class8</th>\n",
       "      <th>Class9</th>\n",
       "      <th>Class10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k6a1  k6a2   k11   k12  k9a1  k9a2  no_of_max  Class0  Class1  Class2  \\\n",
       "247  0.50  0.25  0.25  0.25  0.50  0.50          5     0.0     0.0     0.0   \n",
       "335  0.50  0.50  0.25  0.50  0.25  0.75          1     0.0     0.0     0.0   \n",
       "701  0.75  0.75  0.50  0.75  0.75  0.75         19     0.0     0.0     0.0   \n",
       "440  0.50  0.75  0.50  0.25  0.75  0.75         19     0.0     0.0     0.0   \n",
       "136  0.25  0.50  0.75  0.25  0.25  0.50          1     0.0     0.0     0.0   \n",
       "\n",
       "     Class3  Class4  Class5  Class6  Class7  Class8  Class9  Class10  \n",
       "247     0.0     0.0     1.0     1.0     0.0     0.0     1.0      0.0  \n",
       "335     0.0     1.0     0.0     0.0     1.0     0.0     0.0      0.0  \n",
       "701     0.0     1.0     0.0     1.0     0.0     0.0     0.0      1.0  \n",
       "440     0.0     0.0     0.0     1.0     0.0     0.0     0.0      0.0  \n",
       "136     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spectra_all=pd.read_csv(\"spectrum_energy_input_numberOfPeaks.csv\",index_col=[0])\n",
    "all_data = df_spectra_all[[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\",\"no_of_max\"]]\n",
    "no_data_points=(len(all_data[\"k6a1\"].values))\n",
    "#all_data=pd.get_dummies(all_data,columns=[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"])\n",
    "\n",
    "#values between 0.75 and 2.25\n",
    "#different classes:\n",
    "#0 all < 0.5\n",
    "#1 all > 0.5\n",
    "#2 k6a1,k11,k9a1 >0.5, k6a2,k11,k9a2 <0.5\n",
    "#3 k6a1,k11,k9a1 <0.5, k6a2,k11,k9a2 >0.5\n",
    "#4 k6a1 == k6a2 \n",
    "#5 k11 == k12 \n",
    "#6 k9a1 == k9a2 \n",
    "#7 sum(k6a1,k11,k9a1) < 1.25\n",
    "#8 sum(k6a1,k11,k9a1) > 2\n",
    "#9 sum(k6a2,k12,k9a2) < 1.25\n",
    "#10 sum(k6a2,k12,k9a2) > 2\n",
    "\n",
    "print(all_data.head())\n",
    "#class_array=np.zeros((no_data_points,11))\n",
    "all_data.loc[(all_data['k6a1'] <0.5) & (all_data['k6a2'] <0.5)& (all_data['k11'] <0.5)\n",
    "                        & (all_data['k12'] <0.5)& (all_data['k9a1'] <0.5)&( all_data['k9a2'] <0.5), 'Class0'] = 1  \n",
    "all_data.loc[(all_data['k6a1'] >0.5) & (all_data['k6a2'] >0.5)& (all_data['k11'] >0.5)\n",
    "                        & (all_data['k12'] >0.5)& (all_data['k9a1'] >0.5)&( all_data['k9a2'] >0.5), 'Class1'] = 1  \n",
    "\n",
    "all_data.loc[(all_data['k6a1'] >0.5) & (all_data['k6a2'] <0.5)& (all_data['k11'] >0.5)\n",
    "                        & (all_data['k12'] <0.5)& (all_data['k9a1'] >0.5)&( all_data['k9a2'] <0.5), 'Class2'] = 1  \n",
    "all_data.loc[(all_data['k6a1'] <0.5) & (all_data['k6a2'] >0.5)& (all_data['k11'] <0.5)\n",
    "                        & (all_data['k12'] >0.5)& (all_data['k9a1'] <0.5)&( all_data['k9a2'] >0.5), 'Class3'] = 1  \n",
    "all_data.loc[(all_data['k6a1'] == all_data['k6a2']), 'Class4'] = 1  \n",
    "all_data.loc[(all_data['k11'] == all_data['k12']), 'Class5'] = 1  \n",
    "all_data.loc[(all_data['k9a1'] == all_data['k9a2']), 'Class6'] = 1  \n",
    "all_data.loc[((all_data['k6a1']+all_data['k11']+all_data['k9a1'])<1.25 ), 'Class7'] = 1  \n",
    "all_data.loc[((all_data['k6a1']+all_data['k11']+all_data['k9a1'])>2 ), 'Class8'] = 1  \n",
    "all_data.loc[((all_data['k6a2']+all_data['k12']+all_data['k9a2'])<1.25 ), 'Class9'] = 1  \n",
    "all_data.loc[((all_data['k6a2']+all_data['k12']+all_data['k9a2'])>2 ), 'Class10'] = 1  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_data=all_data.fillna(0)\n",
    "\n",
    "\n",
    "data_train, data_test = train_test_split(all_data, test_size=0.20, random_state=42)\n",
    "data_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns_A = []\n",
    "\n",
    "no_of_max = tf.feature_column.numeric_column(\"no_of_max\")\n",
    "feature_columns_A.append(no_of_max)\n",
    "\n",
    "my_feature_layer_A = tf.keras.layers.DenseFeatures(feature_columns_A)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "    \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "    for m in list_of_metrics:\n",
    "        x = hist[m]\n",
    "        plt.plot(epochs[1:], x[1:], label=m)\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "# for activation functions check https://keras.io/api/layers/activations/\n",
    "def create_model2(my_learning_rate, my_feature_layer,my_metrics,my_act_function = \"softmax\"):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(my_feature_layer)\n",
    "    layers=[20,12]\n",
    "    for layer in layers:\n",
    "        model.add(tf.keras.layers.Dense(units = layer, activation = my_act_function))\n",
    "    model.add(tf.keras.layers.Dense(units=11,name='Output', activation = 'softmax'))                             \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),                                       \n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics=my_metrics)\n",
    "    return model\n",
    "\n",
    "def train_model2(model, dataset, epochs, label_name,\n",
    "                batch_size=None,shuffle=True):\n",
    "    #features = {name:np.array(np.asarray(value)) for name, value in dataset.items()}\n",
    "\n",
    "    features={\"no_of_max\":dataset[\"no_of_max\"].to_numpy()}\n",
    "   # print(features)\n",
    "    label=dataset[label_name].to_numpy()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle)\n",
    "  \n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    return epochs, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "epochs = 500\n",
    "batch_size = 12\n",
    "\n",
    "#specify the classification threshold\n",
    "classification_threshold = 0.15\n",
    "\n",
    "# Establish the metrics the model will measure.\n",
    "metric = [tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classification_threshold),\n",
    "      tf.keras.metrics.Precision(thresholds=classification_threshold,name='precision'),\n",
    "      tf.keras.metrics.Recall(thresholds=classification_threshold,name='recall'),]\n",
    "\n",
    "label_name=[\"Class0\",\"Class1\",\"Class2\",\"Class3\",\"Class4\",\"Class5\",\"Class6\",\"Class7\",\"Class8\",\"Class9\",\"Class10\"]\n",
    "my_model = create_model2(learning_rate, my_feature_layer_A,metric,my_act_function=\"softmax\")\n",
    "\n",
    "# Plot a graph of the metric(s) vs. epochs.\n",
    "#list_of_metrics_to_plot = ['accuracy'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "49/49 [==============================] - 0s 684us/step - loss: 0.2988 - accuracy: 0.7719 - precision: 0.3170 - recall: 0.7218  \n",
      "Epoch 2/500\n",
      "49/49 [==============================] - 0s 655us/step - loss: 0.2940 - accuracy: 0.7736 - precision: 0.3213 - recall: 0.7368\n",
      "Epoch 3/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2942 - accuracy: 0.7670 - precision: 0.3163 - recall: 0.7506\n",
      "Epoch 4/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2932 - accuracy: 0.7631 - precision: 0.3143 - recall: 0.7644\n",
      "Epoch 5/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2927 - accuracy: 0.7630 - precision: 0.3092 - recall: 0.7331\n",
      "Epoch 6/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2944 - accuracy: 0.7695 - precision: 0.3162 - recall: 0.7331\n",
      "Epoch 7/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2966 - accuracy: 0.7625 - precision: 0.3109 - recall: 0.7469\n",
      "Epoch 8/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2925 - accuracy: 0.7663 - precision: 0.3160 - recall: 0.7544\n",
      "Epoch 9/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2921 - accuracy: 0.7670 - precision: 0.3145 - recall: 0.7393\n",
      "Epoch 10/500\n",
      "49/49 [==============================] - 0s 655us/step - loss: 0.2931 - accuracy: 0.7622 - precision: 0.3098 - recall: 0.7419\n",
      "Epoch 11/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2937 - accuracy: 0.7656 - precision: 0.3132 - recall: 0.7406\n",
      "Epoch 12/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2910 - accuracy: 0.7684 - precision: 0.3132 - recall: 0.7218\n",
      "Epoch 13/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2929 - accuracy: 0.7653 - precision: 0.3162 - recall: 0.7619\n",
      "Epoch 14/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2912 - accuracy: 0.7655 - precision: 0.3142 - recall: 0.7481\n",
      "Epoch 15/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2940 - accuracy: 0.7616 - precision: 0.3119 - recall: 0.7594\n",
      "Epoch 16/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2921 - accuracy: 0.7622 - precision: 0.3131 - recall: 0.7632\n",
      "Epoch 17/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2905 - accuracy: 0.7666 - precision: 0.3171 - recall: 0.7594\n",
      "Epoch 18/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2903 - accuracy: 0.7552 - precision: 0.3097 - recall: 0.7870\n",
      "Epoch 19/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2938 - accuracy: 0.7653 - precision: 0.3125 - recall: 0.7381\n",
      "Epoch 20/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2900 - accuracy: 0.7659 - precision: 0.3168 - recall: 0.7619\n",
      "Epoch 21/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2903 - accuracy: 0.7694 - precision: 0.3184 - recall: 0.7481\n",
      "Epoch 22/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2887 - accuracy: 0.7683 - precision: 0.3193 - recall: 0.7619\n",
      "Epoch 23/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2901 - accuracy: 0.7667 - precision: 0.3163 - recall: 0.7531\n",
      "Epoch 24/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2896 - accuracy: 0.7666 - precision: 0.3167 - recall: 0.7569\n",
      "Epoch 25/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2916 - accuracy: 0.7695 - precision: 0.3209 - recall: 0.7632\n",
      "Epoch 26/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2926 - accuracy: 0.7581 - precision: 0.3109 - recall: 0.7757\n",
      "Epoch 27/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2908 - accuracy: 0.7677 - precision: 0.3162 - recall: 0.7456\n",
      "Epoch 28/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2913 - accuracy: 0.7706 - precision: 0.3194 - recall: 0.7456\n",
      "Epoch 29/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2946 - accuracy: 0.7672 - precision: 0.3172 - recall: 0.7556\n",
      "Epoch 30/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2928 - accuracy: 0.7611 - precision: 0.3135 - recall: 0.7732\n",
      "Epoch 31/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2930 - accuracy: 0.7709 - precision: 0.3178 - recall: 0.7331\n",
      "Epoch 32/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2936 - accuracy: 0.7664 - precision: 0.3179 - recall: 0.7657\n",
      "Epoch 33/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2928 - accuracy: 0.7649 - precision: 0.3163 - recall: 0.7657\n",
      "Epoch 34/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2931 - accuracy: 0.7641 - precision: 0.3141 - recall: 0.7569\n",
      "Epoch 35/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2905 - accuracy: 0.7675 - precision: 0.3164 - recall: 0.7481\n",
      "Epoch 36/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2908 - accuracy: 0.7649 - precision: 0.3147 - recall: 0.7556\n",
      "Epoch 37/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2887 - accuracy: 0.7673 - precision: 0.3183 - recall: 0.7619\n",
      "Epoch 38/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2878 - accuracy: 0.7645 - precision: 0.3130 - recall: 0.7469\n",
      "Epoch 39/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2978 - accuracy: 0.7686 - precision: 0.3177 - recall: 0.7494\n",
      "Epoch 40/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2986 - accuracy: 0.7689 - precision: 0.3171 - recall: 0.7431\n",
      "Epoch 41/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2917 - accuracy: 0.7653 - precision: 0.3135 - recall: 0.7444\n",
      "Epoch 42/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2894 - accuracy: 0.7670 - precision: 0.3153 - recall: 0.7444\n",
      "Epoch 43/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2920 - accuracy: 0.7758 - precision: 0.3230 - recall: 0.7318\n",
      "Epoch 44/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2893 - accuracy: 0.7672 - precision: 0.3172 - recall: 0.7556\n",
      "Epoch 45/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2878 - accuracy: 0.7658 - precision: 0.3172 - recall: 0.7657\n",
      "Epoch 46/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2898 - accuracy: 0.7667 - precision: 0.3173 - recall: 0.7594\n",
      "Epoch 47/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2905 - accuracy: 0.7675 - precision: 0.3175 - recall: 0.7556\n",
      "Epoch 48/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2912 - accuracy: 0.7692 - precision: 0.3192 - recall: 0.7544\n",
      "Epoch 49/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2903 - accuracy: 0.7755 - precision: 0.3234 - recall: 0.7368\n",
      "Epoch 50/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2891 - accuracy: 0.7656 - precision: 0.3188 - recall: 0.7769\n",
      "Epoch 51/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2895 - accuracy: 0.7678 - precision: 0.3167 - recall: 0.7481\n",
      "Epoch 52/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2897 - accuracy: 0.7658 - precision: 0.3161 - recall: 0.7581\n",
      "Epoch 53/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2895 - accuracy: 0.7673 - precision: 0.3174 - recall: 0.7556\n",
      "Epoch 54/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2963 - accuracy: 0.7636 - precision: 0.3084 - recall: 0.7243\n",
      "Epoch 55/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2907 - accuracy: 0.7624 - precision: 0.3107 - recall: 0.7469\n",
      "Epoch 56/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2938 - accuracy: 0.7642 - precision: 0.3139 - recall: 0.7544\n",
      "Epoch 57/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2918 - accuracy: 0.7653 - precision: 0.3133 - recall: 0.7431\n",
      "Epoch 58/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2887 - accuracy: 0.7599 - precision: 0.3107 - recall: 0.7632\n",
      "Epoch 59/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2888 - accuracy: 0.7670 - precision: 0.3172 - recall: 0.7569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2917 - accuracy: 0.7658 - precision: 0.3126 - recall: 0.7356\n",
      "Epoch 61/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2883 - accuracy: 0.7636 - precision: 0.3163 - recall: 0.7744\n",
      "Epoch 62/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2909 - accuracy: 0.7666 - precision: 0.3136 - recall: 0.7368\n",
      "Epoch 63/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2916 - accuracy: 0.7691 - precision: 0.3211 - recall: 0.7682\n",
      "Epoch 64/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2887 - accuracy: 0.7681 - precision: 0.3161 - recall: 0.7419\n",
      "Epoch 65/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2890 - accuracy: 0.7642 - precision: 0.3141 - recall: 0.7556\n",
      "Epoch 66/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2885 - accuracy: 0.7649 - precision: 0.3161 - recall: 0.7644\n",
      "Epoch 67/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2929 - accuracy: 0.7652 - precision: 0.3137 - recall: 0.7469\n",
      "Epoch 68/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2938 - accuracy: 0.7617 - precision: 0.3120 - recall: 0.7594\n",
      "Epoch 69/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2906 - accuracy: 0.7672 - precision: 0.3176 - recall: 0.7581\n",
      "Epoch 70/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2884 - accuracy: 0.7638 - precision: 0.3124 - recall: 0.7481\n",
      "Epoch 71/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2874 - accuracy: 0.7652 - precision: 0.3160 - recall: 0.7619\n",
      "Epoch 72/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2880 - accuracy: 0.7656 - precision: 0.3159 - recall: 0.7581\n",
      "Epoch 73/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2877 - accuracy: 0.7633 - precision: 0.3156 - recall: 0.7719\n",
      "Epoch 74/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2934 - accuracy: 0.7656 - precision: 0.3157 - recall: 0.7569\n",
      "Epoch 75/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2894 - accuracy: 0.7700 - precision: 0.3181 - recall: 0.7419\n",
      "Epoch 76/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2891 - accuracy: 0.7717 - precision: 0.3190 - recall: 0.7356\n",
      "Epoch 77/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2889 - accuracy: 0.7649 - precision: 0.3145 - recall: 0.7544\n",
      "Epoch 78/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2894 - accuracy: 0.7625 - precision: 0.3146 - recall: 0.7707\n",
      "Epoch 79/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2892 - accuracy: 0.7720 - precision: 0.3217 - recall: 0.7506\n",
      "Epoch 80/500\n",
      "49/49 [==============================] - 0s 655us/step - loss: 0.2899 - accuracy: 0.7700 - precision: 0.3197 - recall: 0.7519\n",
      "Epoch 81/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2904 - accuracy: 0.7650 - precision: 0.3155 - recall: 0.7594\n",
      "Epoch 82/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2868 - accuracy: 0.7705 - precision: 0.3200 - recall: 0.7506\n",
      "Epoch 83/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2935 - accuracy: 0.7630 - precision: 0.3112 - recall: 0.7456\n",
      "Epoch 84/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2939 - accuracy: 0.7664 - precision: 0.3164 - recall: 0.7556\n",
      "Epoch 85/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2892 - accuracy: 0.7641 - precision: 0.3154 - recall: 0.7657\n",
      "Epoch 86/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2880 - accuracy: 0.7692 - precision: 0.3198 - recall: 0.7581\n",
      "Epoch 87/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2879 - accuracy: 0.7585 - precision: 0.3120 - recall: 0.7807\n",
      "Epoch 88/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2873 - accuracy: 0.7661 - precision: 0.3191 - recall: 0.7757\n",
      "Epoch 89/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2890 - accuracy: 0.7647 - precision: 0.3168 - recall: 0.7707\n",
      "Epoch 90/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2888 - accuracy: 0.7636 - precision: 0.3161 - recall: 0.7732\n",
      "Epoch 91/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2889 - accuracy: 0.7675 - precision: 0.3187 - recall: 0.7632\n",
      "Epoch 92/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2880 - accuracy: 0.7688 - precision: 0.3196 - recall: 0.7607\n",
      "Epoch 93/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2947 - accuracy: 0.7614 - precision: 0.3111 - recall: 0.7556\n",
      "Epoch 94/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2893 - accuracy: 0.7588 - precision: 0.3113 - recall: 0.7744\n",
      "Epoch 95/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2889 - accuracy: 0.7644 - precision: 0.3154 - recall: 0.7632\n",
      "Epoch 96/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2887 - accuracy: 0.7722 - precision: 0.3217 - recall: 0.7494\n",
      "Epoch 97/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2912 - accuracy: 0.7672 - precision: 0.3170 - recall: 0.7544\n",
      "Epoch 98/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2891 - accuracy: 0.7647 - precision: 0.3170 - recall: 0.7719\n",
      "Epoch 99/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2891 - accuracy: 0.7678 - precision: 0.3188 - recall: 0.7619\n",
      "Epoch 100/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2888 - accuracy: 0.7644 - precision: 0.3169 - recall: 0.7732\n",
      "Epoch 101/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2927 - accuracy: 0.7653 - precision: 0.3162 - recall: 0.7619\n",
      "Epoch 102/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2891 - accuracy: 0.7669 - precision: 0.3173 - recall: 0.7581\n",
      "Epoch 103/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2887 - accuracy: 0.7689 - precision: 0.3198 - recall: 0.7607\n",
      "Epoch 104/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2880 - accuracy: 0.7666 - precision: 0.3163 - recall: 0.7544\n",
      "Epoch 105/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2883 - accuracy: 0.7661 - precision: 0.3178 - recall: 0.7669\n",
      "Epoch 106/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2872 - accuracy: 0.7644 - precision: 0.3163 - recall: 0.7694\n",
      "Epoch 107/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2894 - accuracy: 0.7638 - precision: 0.3159 - recall: 0.7707\n",
      "Epoch 108/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2903 - accuracy: 0.7680 - precision: 0.3186 - recall: 0.7594\n",
      "Epoch 109/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2876 - accuracy: 0.7680 - precision: 0.3216 - recall: 0.7794\n",
      "Epoch 110/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2885 - accuracy: 0.7661 - precision: 0.3176 - recall: 0.7657\n",
      "Epoch 111/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2903 - accuracy: 0.7669 - precision: 0.3190 - recall: 0.7694\n",
      "Epoch 112/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2874 - accuracy: 0.7703 - precision: 0.3217 - recall: 0.7632\n",
      "Epoch 113/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2923 - accuracy: 0.7645 - precision: 0.3169 - recall: 0.7719\n",
      "Epoch 114/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2893 - accuracy: 0.7636 - precision: 0.3138 - recall: 0.7581\n",
      "Epoch 115/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2901 - accuracy: 0.7658 - precision: 0.3138 - recall: 0.7431\n",
      "Epoch 116/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2888 - accuracy: 0.7670 - precision: 0.3172 - recall: 0.7569\n",
      "Epoch 117/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2894 - accuracy: 0.7664 - precision: 0.3150 - recall: 0.7469\n",
      "Epoch 118/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 600us/step - loss: 0.2880 - accuracy: 0.7706 - precision: 0.3205 - recall: 0.7531\n",
      "Epoch 119/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2893 - accuracy: 0.7655 - precision: 0.3169 - recall: 0.7657\n",
      "Epoch 120/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2884 - accuracy: 0.7686 - precision: 0.3176 - recall: 0.7481\n",
      "Epoch 121/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2943 - accuracy: 0.7734 - precision: 0.3233 - recall: 0.7506\n",
      "Epoch 122/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2879 - accuracy: 0.7698 - precision: 0.3203 - recall: 0.7569\n",
      "Epoch 123/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2886 - accuracy: 0.7672 - precision: 0.3182 - recall: 0.7619\n",
      "Epoch 124/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2896 - accuracy: 0.7658 - precision: 0.3165 - recall: 0.7607\n",
      "Epoch 125/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2893 - accuracy: 0.7650 - precision: 0.3137 - recall: 0.7481\n",
      "Epoch 126/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2883 - accuracy: 0.7678 - precision: 0.3179 - recall: 0.7556\n",
      "Epoch 127/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2885 - accuracy: 0.7694 - precision: 0.3194 - recall: 0.7544\n",
      "Epoch 128/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2880 - accuracy: 0.7706 - precision: 0.3234 - recall: 0.7719\n",
      "Epoch 129/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2881 - accuracy: 0.7737 - precision: 0.3251 - recall: 0.7607\n",
      "Epoch 130/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2888 - accuracy: 0.7639 - precision: 0.3133 - recall: 0.7531\n",
      "Epoch 131/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2871 - accuracy: 0.7652 - precision: 0.3156 - recall: 0.7594\n",
      "Epoch 132/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2893 - accuracy: 0.7714 - precision: 0.3195 - recall: 0.7406\n",
      "Epoch 133/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2877 - accuracy: 0.7683 - precision: 0.3195 - recall: 0.7632\n",
      "Epoch 134/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2941 - accuracy: 0.7664 - precision: 0.3164 - recall: 0.7556\n",
      "Epoch 135/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2902 - accuracy: 0.7625 - precision: 0.3138 - recall: 0.7657\n",
      "Epoch 136/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2882 - accuracy: 0.7716 - precision: 0.3223 - recall: 0.7581\n",
      "Epoch 137/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2892 - accuracy: 0.7655 - precision: 0.3154 - recall: 0.7556\n",
      "Epoch 138/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2882 - accuracy: 0.7737 - precision: 0.3221 - recall: 0.7406\n",
      "Epoch 139/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2884 - accuracy: 0.7659 - precision: 0.3135 - recall: 0.7406\n",
      "Epoch 140/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2880 - accuracy: 0.7680 - precision: 0.3175 - recall: 0.7519\n",
      "Epoch 141/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2880 - accuracy: 0.7631 - precision: 0.3135 - recall: 0.7594\n",
      "Epoch 142/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2859 - accuracy: 0.7658 - precision: 0.3172 - recall: 0.7657\n",
      "Epoch 143/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2913 - accuracy: 0.7673 - precision: 0.3179 - recall: 0.7594\n",
      "Epoch 144/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2896 - accuracy: 0.7672 - precision: 0.3168 - recall: 0.7531\n",
      "Epoch 145/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2879 - accuracy: 0.7688 - precision: 0.3189 - recall: 0.7556\n",
      "Epoch 146/500\n",
      "49/49 [==============================] - 0s 581us/step - loss: 0.2886 - accuracy: 0.7664 - precision: 0.3164 - recall: 0.7556\n",
      "Epoch 147/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2885 - accuracy: 0.7649 - precision: 0.3157 - recall: 0.7619\n",
      "Epoch 148/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2893 - accuracy: 0.7678 - precision: 0.3214 - recall: 0.7794\n",
      "Epoch 149/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2887 - accuracy: 0.7624 - precision: 0.3138 - recall: 0.7669\n",
      "Epoch 150/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2909 - accuracy: 0.7680 - precision: 0.3182 - recall: 0.7569\n",
      "Epoch 151/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2895 - accuracy: 0.7677 - precision: 0.3204 - recall: 0.7732\n",
      "Epoch 152/500\n",
      "49/49 [==============================] - 0s 655us/step - loss: 0.2927 - accuracy: 0.7680 - precision: 0.3192 - recall: 0.7632\n",
      "Epoch 153/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2881 - accuracy: 0.7659 - precision: 0.3174 - recall: 0.7657\n",
      "Epoch 154/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2892 - accuracy: 0.7663 - precision: 0.3166 - recall: 0.7581\n",
      "Epoch 155/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2880 - accuracy: 0.7677 - precision: 0.3204 - recall: 0.7732\n",
      "Epoch 156/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2908 - accuracy: 0.7653 - precision: 0.3177 - recall: 0.7719\n",
      "Epoch 157/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2873 - accuracy: 0.7625 - precision: 0.3146 - recall: 0.7707\n",
      "Epoch 158/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2879 - accuracy: 0.7638 - precision: 0.3172 - recall: 0.7794\n",
      "Epoch 159/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2880 - accuracy: 0.7594 - precision: 0.3118 - recall: 0.7732\n",
      "Epoch 160/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2996 - accuracy: 0.7650 - precision: 0.3166 - recall: 0.7669\n",
      "Epoch 161/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2894 - accuracy: 0.7652 - precision: 0.3150 - recall: 0.7556\n",
      "Epoch 162/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2885 - accuracy: 0.7689 - precision: 0.3200 - recall: 0.7619\n",
      "Epoch 163/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2895 - accuracy: 0.7608 - precision: 0.3154 - recall: 0.7882\n",
      "Epoch 164/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2911 - accuracy: 0.7655 - precision: 0.3180 - recall: 0.7732\n",
      "Epoch 165/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2884 - accuracy: 0.7645 - precision: 0.3171 - recall: 0.7732\n",
      "Epoch 166/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2931 - accuracy: 0.7677 - precision: 0.3202 - recall: 0.7719\n",
      "Epoch 167/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2875 - accuracy: 0.7670 - precision: 0.3197 - recall: 0.7732\n",
      "Epoch 168/500\n",
      "49/49 [==============================] - 0s 613us/step - loss: 0.2969 - accuracy: 0.7631 - precision: 0.3139 - recall: 0.7619\n",
      "Epoch 169/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2868 - accuracy: 0.7627 - precision: 0.3138 - recall: 0.7644\n",
      "Epoch 170/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2884 - accuracy: 0.7725 - precision: 0.3241 - recall: 0.7632\n",
      "Epoch 171/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2900 - accuracy: 0.7649 - precision: 0.3176 - recall: 0.7744\n",
      "Epoch 172/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2872 - accuracy: 0.7645 - precision: 0.3148 - recall: 0.7581\n",
      "Epoch 173/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2893 - accuracy: 0.7602 - precision: 0.3099 - recall: 0.7556\n",
      "Epoch 174/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.3137 - accuracy: 0.7691 - precision: 0.3184 - recall: 0.7506\n",
      "Epoch 175/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.3366 - accuracy: 0.7678 - precision: 0.3155 - recall: 0.7406\n",
      "Epoch 176/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 564us/step - loss: 0.3332 - accuracy: 0.7672 - precision: 0.3156 - recall: 0.7456\n",
      "Epoch 177/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.3353 - accuracy: 0.7599 - precision: 0.3115 - recall: 0.7682\n",
      "Epoch 178/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.3331 - accuracy: 0.7686 - precision: 0.3174 - recall: 0.7469\n",
      "Epoch 179/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.3344 - accuracy: 0.7659 - precision: 0.3166 - recall: 0.7607\n",
      "Epoch 180/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.3022 - accuracy: 0.7624 - precision: 0.3157 - recall: 0.7794\n",
      "Epoch 181/500\n",
      "49/49 [==============================] - 0s 612us/step - loss: 0.2912 - accuracy: 0.7661 - precision: 0.3168 - recall: 0.7607\n",
      "Epoch 182/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2881 - accuracy: 0.7661 - precision: 0.3166 - recall: 0.7594\n",
      "Epoch 183/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2888 - accuracy: 0.7666 - precision: 0.3181 - recall: 0.7657\n",
      "Epoch 184/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2899 - accuracy: 0.7681 - precision: 0.3188 - recall: 0.7594\n",
      "Epoch 185/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2915 - accuracy: 0.7649 - precision: 0.3174 - recall: 0.7732\n",
      "Epoch 186/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2877 - accuracy: 0.7672 - precision: 0.3168 - recall: 0.7531\n",
      "Epoch 187/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.3005 - accuracy: 0.7661 - precision: 0.3181 - recall: 0.7694\n",
      "Epoch 188/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2904 - accuracy: 0.7748 - precision: 0.3248 - recall: 0.7506\n",
      "Epoch 189/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.3211 - accuracy: 0.7705 - precision: 0.3186 - recall: 0.7419\n",
      "Epoch 190/500\n",
      "49/49 [==============================] - 0s 575us/step - loss: 0.2937 - accuracy: 0.7694 - precision: 0.3168 - recall: 0.7381\n",
      "Epoch 191/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2931 - accuracy: 0.7656 - precision: 0.3132 - recall: 0.7406\n",
      "Epoch 192/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.3045 - accuracy: 0.7681 - precision: 0.3169 - recall: 0.7469\n",
      "Epoch 193/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2947 - accuracy: 0.7658 - precision: 0.3155 - recall: 0.7544\n",
      "Epoch 194/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2910 - accuracy: 0.7692 - precision: 0.3186 - recall: 0.7506\n",
      "Epoch 195/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2897 - accuracy: 0.7689 - precision: 0.3179 - recall: 0.7481\n",
      "Epoch 196/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2921 - accuracy: 0.7652 - precision: 0.3133 - recall: 0.7444\n",
      "Epoch 197/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2912 - accuracy: 0.7670 - precision: 0.3119 - recall: 0.7231\n",
      "Epoch 198/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2908 - accuracy: 0.7694 - precision: 0.3192 - recall: 0.7531\n",
      "Epoch 199/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2903 - accuracy: 0.7683 - precision: 0.3141 - recall: 0.7281\n",
      "Epoch 200/500\n",
      "49/49 [==============================] - 0s 617us/step - loss: 0.2908 - accuracy: 0.7694 - precision: 0.3166 - recall: 0.7368\n",
      "Epoch 201/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2884 - accuracy: 0.7691 - precision: 0.3171 - recall: 0.7419\n",
      "Epoch 202/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2897 - accuracy: 0.7688 - precision: 0.3185 - recall: 0.7531\n",
      "Epoch 203/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2916 - accuracy: 0.7686 - precision: 0.3174 - recall: 0.7469\n",
      "Epoch 204/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2898 - accuracy: 0.7697 - precision: 0.3184 - recall: 0.7456\n",
      "Epoch 205/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2892 - accuracy: 0.7688 - precision: 0.3142 - recall: 0.7256\n",
      "Epoch 206/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2889 - accuracy: 0.7678 - precision: 0.3186 - recall: 0.7607\n",
      "Epoch 207/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2887 - accuracy: 0.7731 - precision: 0.3244 - recall: 0.7607\n",
      "Epoch 208/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2875 - accuracy: 0.7653 - precision: 0.3167 - recall: 0.7657\n",
      "Epoch 209/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2885 - accuracy: 0.7688 - precision: 0.3183 - recall: 0.7519\n",
      "Epoch 210/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2883 - accuracy: 0.7675 - precision: 0.3200 - recall: 0.7719\n",
      "Epoch 211/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2891 - accuracy: 0.7694 - precision: 0.3188 - recall: 0.7506\n",
      "Epoch 212/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2899 - accuracy: 0.7650 - precision: 0.3141 - recall: 0.7506\n",
      "Epoch 213/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2942 - accuracy: 0.7644 - precision: 0.3156 - recall: 0.7644\n",
      "Epoch 214/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2893 - accuracy: 0.7731 - precision: 0.3225 - recall: 0.7481\n",
      "Epoch 215/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2899 - accuracy: 0.7697 - precision: 0.3195 - recall: 0.7531\n",
      "Epoch 216/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2896 - accuracy: 0.7653 - precision: 0.3152 - recall: 0.7556\n",
      "Epoch 217/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2934 - accuracy: 0.7628 - precision: 0.3132 - recall: 0.7594\n",
      "Epoch 218/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2906 - accuracy: 0.7691 - precision: 0.3190 - recall: 0.7544\n",
      "Epoch 219/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2873 - accuracy: 0.7692 - precision: 0.3203 - recall: 0.7619\n",
      "Epoch 220/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2895 - accuracy: 0.7731 - precision: 0.3196 - recall: 0.7293\n",
      "Epoch 221/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2912 - accuracy: 0.7689 - precision: 0.3192 - recall: 0.7569\n",
      "Epoch 222/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2892 - accuracy: 0.7742 - precision: 0.3241 - recall: 0.7506\n",
      "Epoch 223/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2909 - accuracy: 0.7748 - precision: 0.3258 - recall: 0.7569\n",
      "Epoch 224/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2901 - accuracy: 0.7698 - precision: 0.3185 - recall: 0.7456\n",
      "Epoch 225/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2895 - accuracy: 0.7625 - precision: 0.3130 - recall: 0.7607\n",
      "Epoch 226/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2912 - accuracy: 0.7692 - precision: 0.3176 - recall: 0.7444\n",
      "Epoch 227/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2889 - accuracy: 0.7720 - precision: 0.3225 - recall: 0.7556\n",
      "Epoch 228/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2920 - accuracy: 0.7714 - precision: 0.3195 - recall: 0.7406\n",
      "Epoch 229/500\n",
      "49/49 [==============================] - 0s 593us/step - loss: 0.2904 - accuracy: 0.7641 - precision: 0.3149 - recall: 0.7619\n",
      "Epoch 230/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2890 - accuracy: 0.7678 - precision: 0.3192 - recall: 0.7644\n",
      "Epoch 231/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2933 - accuracy: 0.7702 - precision: 0.3214 - recall: 0.7619\n",
      "Epoch 232/500\n",
      "49/49 [==============================] - 0s 655us/step - loss: 0.2896 - accuracy: 0.7625 - precision: 0.3111 - recall: 0.7481\n",
      "Epoch 233/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2910 - accuracy: 0.7647 - precision: 0.3174 - recall: 0.7744\n",
      "Epoch 234/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 582us/step - loss: 0.2902 - accuracy: 0.7675 - precision: 0.3187 - recall: 0.7632\n",
      "Epoch 235/500\n",
      "49/49 [==============================] - 0s 577us/step - loss: 0.2922 - accuracy: 0.7670 - precision: 0.3155 - recall: 0.7456\n",
      "Epoch 236/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2902 - accuracy: 0.7683 - precision: 0.3151 - recall: 0.7343\n",
      "Epoch 237/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2883 - accuracy: 0.7726 - precision: 0.3214 - recall: 0.7444\n",
      "Epoch 238/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2926 - accuracy: 0.7737 - precision: 0.3217 - recall: 0.7381\n",
      "Epoch 239/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2884 - accuracy: 0.7656 - precision: 0.3161 - recall: 0.7594\n",
      "Epoch 240/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2891 - accuracy: 0.7703 - precision: 0.3210 - recall: 0.7581\n",
      "Epoch 241/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2893 - accuracy: 0.7700 - precision: 0.3208 - recall: 0.7594\n",
      "Epoch 242/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2896 - accuracy: 0.7677 - precision: 0.3175 - recall: 0.7544\n",
      "Epoch 243/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2907 - accuracy: 0.7703 - precision: 0.3186 - recall: 0.7431\n",
      "Epoch 244/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2893 - accuracy: 0.7675 - precision: 0.3191 - recall: 0.7657\n",
      "Epoch 245/500\n",
      "49/49 [==============================] - 0s 596us/step - loss: 0.2904 - accuracy: 0.7739 - precision: 0.3207 - recall: 0.7306\n",
      "Epoch 246/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2895 - accuracy: 0.7647 - precision: 0.3140 - recall: 0.7519\n",
      "Epoch 247/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2907 - accuracy: 0.7700 - precision: 0.3199 - recall: 0.7531\n",
      "Epoch 248/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2884 - accuracy: 0.7695 - precision: 0.3168 - recall: 0.7368\n",
      "Epoch 249/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2903 - accuracy: 0.7733 - precision: 0.3235 - recall: 0.7531\n",
      "Epoch 250/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2927 - accuracy: 0.7706 - precision: 0.3182 - recall: 0.7381\n",
      "Epoch 251/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2886 - accuracy: 0.7748 - precision: 0.3252 - recall: 0.7531\n",
      "Epoch 252/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2909 - accuracy: 0.7681 - precision: 0.3186 - recall: 0.7581\n",
      "Epoch 253/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2897 - accuracy: 0.7722 - precision: 0.3209 - recall: 0.7444\n",
      "Epoch 254/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2891 - accuracy: 0.7677 - precision: 0.3179 - recall: 0.7569\n",
      "Epoch 255/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2902 - accuracy: 0.7652 - precision: 0.3156 - recall: 0.7594\n",
      "Epoch 256/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2931 - accuracy: 0.7638 - precision: 0.3149 - recall: 0.7644\n",
      "Epoch 257/500\n",
      "49/49 [==============================] - 0s 593us/step - loss: 0.2895 - accuracy: 0.7737 - precision: 0.3228 - recall: 0.7456\n",
      "Epoch 258/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2898 - accuracy: 0.7720 - precision: 0.3207 - recall: 0.7444\n",
      "Epoch 259/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2882 - accuracy: 0.7673 - precision: 0.3178 - recall: 0.7581\n",
      "Epoch 260/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2904 - accuracy: 0.7714 - precision: 0.3187 - recall: 0.7356\n",
      "Epoch 261/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2884 - accuracy: 0.7672 - precision: 0.3166 - recall: 0.7519\n",
      "Epoch 262/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2893 - accuracy: 0.7689 - precision: 0.3179 - recall: 0.7481\n",
      "Epoch 263/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2905 - accuracy: 0.7641 - precision: 0.3135 - recall: 0.7531\n",
      "Epoch 264/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2872 - accuracy: 0.7649 - precision: 0.3153 - recall: 0.7594\n",
      "Epoch 265/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2896 - accuracy: 0.7683 - precision: 0.3206 - recall: 0.7707\n",
      "Epoch 266/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2895 - accuracy: 0.7742 - precision: 0.3228 - recall: 0.7419\n",
      "Epoch 267/500\n",
      "49/49 [==============================] - 0s 576us/step - loss: 0.2890 - accuracy: 0.7677 - precision: 0.3177 - recall: 0.7556\n",
      "Epoch 268/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2915 - accuracy: 0.7694 - precision: 0.3186 - recall: 0.7494\n",
      "Epoch 269/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2918 - accuracy: 0.7703 - precision: 0.3190 - recall: 0.7456\n",
      "Epoch 270/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2891 - accuracy: 0.7667 - precision: 0.3167 - recall: 0.7556\n",
      "Epoch 271/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2906 - accuracy: 0.7734 - precision: 0.3240 - recall: 0.7556\n",
      "Epoch 272/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2903 - accuracy: 0.7711 - precision: 0.3212 - recall: 0.7544\n",
      "Epoch 273/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2925 - accuracy: 0.7653 - precision: 0.3138 - recall: 0.7469\n",
      "Epoch 274/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2902 - accuracy: 0.7697 - precision: 0.3193 - recall: 0.7519\n",
      "Epoch 275/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2889 - accuracy: 0.7736 - precision: 0.3231 - recall: 0.7481\n",
      "Epoch 276/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2894 - accuracy: 0.7702 - precision: 0.3202 - recall: 0.7544\n",
      "Epoch 277/500\n",
      "49/49 [==============================] - 0s 583us/step - loss: 0.2887 - accuracy: 0.7706 - precision: 0.3205 - recall: 0.7531\n",
      "Epoch 278/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2907 - accuracy: 0.7725 - precision: 0.3203 - recall: 0.7381\n",
      "Epoch 279/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2880 - accuracy: 0.7705 - precision: 0.3196 - recall: 0.7481\n",
      "Epoch 280/500\n",
      "49/49 [==============================] - 0s 613us/step - loss: 0.2896 - accuracy: 0.7653 - precision: 0.3162 - recall: 0.7619\n",
      "Epoch 281/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2906 - accuracy: 0.7694 - precision: 0.3184 - recall: 0.7481\n",
      "Epoch 282/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2896 - accuracy: 0.7678 - precision: 0.3186 - recall: 0.7607\n",
      "Epoch 283/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2902 - accuracy: 0.7703 - precision: 0.3196 - recall: 0.7494\n",
      "Epoch 284/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2918 - accuracy: 0.7697 - precision: 0.3187 - recall: 0.7481\n",
      "Epoch 285/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2886 - accuracy: 0.7717 - precision: 0.3229 - recall: 0.7607\n",
      "Epoch 286/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2885 - accuracy: 0.7703 - precision: 0.3208 - recall: 0.7569\n",
      "Epoch 287/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2899 - accuracy: 0.7569 - precision: 0.3083 - recall: 0.7669\n",
      "Epoch 288/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2892 - accuracy: 0.7659 - precision: 0.3166 - recall: 0.7607\n",
      "Epoch 289/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2930 - accuracy: 0.7644 - precision: 0.3165 - recall: 0.7707\n",
      "Epoch 290/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2890 - accuracy: 0.7659 - precision: 0.3159 - recall: 0.7556\n",
      "Epoch 291/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2906 - accuracy: 0.7719 - precision: 0.3200 - recall: 0.7406\n",
      "Epoch 292/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 582us/step - loss: 0.2908 - accuracy: 0.7694 - precision: 0.3201 - recall: 0.7594\n",
      "Epoch 293/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2906 - accuracy: 0.7675 - precision: 0.3202 - recall: 0.7732\n",
      "Epoch 294/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2885 - accuracy: 0.7658 - precision: 0.3169 - recall: 0.7632\n",
      "Epoch 295/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2886 - accuracy: 0.7692 - precision: 0.3157 - recall: 0.7318\n",
      "Epoch 296/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2905 - accuracy: 0.7705 - precision: 0.3198 - recall: 0.7494\n",
      "Epoch 297/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2911 - accuracy: 0.7681 - precision: 0.3157 - recall: 0.7393\n",
      "Epoch 298/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2900 - accuracy: 0.7734 - precision: 0.3227 - recall: 0.7469\n",
      "Epoch 299/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2889 - accuracy: 0.7667 - precision: 0.3194 - recall: 0.7732\n",
      "Epoch 300/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2896 - accuracy: 0.7736 - precision: 0.3238 - recall: 0.7531\n",
      "Epoch 301/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2903 - accuracy: 0.7625 - precision: 0.3115 - recall: 0.7506\n",
      "Epoch 302/500\n",
      "49/49 [==============================] - 0s 597us/step - loss: 0.2888 - accuracy: 0.7709 - precision: 0.3197 - recall: 0.7456\n",
      "Epoch 303/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2879 - accuracy: 0.7709 - precision: 0.3230 - recall: 0.7669\n",
      "Epoch 304/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2911 - accuracy: 0.7770 - precision: 0.3254 - recall: 0.7381\n",
      "Epoch 305/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2891 - accuracy: 0.7700 - precision: 0.3200 - recall: 0.7544\n",
      "Epoch 306/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2905 - accuracy: 0.7649 - precision: 0.3139 - recall: 0.7506\n",
      "Epoch 307/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2886 - accuracy: 0.7686 - precision: 0.3174 - recall: 0.7469\n",
      "Epoch 308/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2900 - accuracy: 0.7683 - precision: 0.3176 - recall: 0.7506\n",
      "Epoch 309/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2894 - accuracy: 0.7649 - precision: 0.3134 - recall: 0.7469\n",
      "Epoch 310/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2884 - accuracy: 0.7714 - precision: 0.3218 - recall: 0.7556\n",
      "Epoch 311/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2892 - accuracy: 0.7678 - precision: 0.3186 - recall: 0.7607\n",
      "Epoch 312/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2882 - accuracy: 0.7765 - precision: 0.3270 - recall: 0.7519\n",
      "Epoch 313/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2912 - accuracy: 0.7692 - precision: 0.3201 - recall: 0.7607\n",
      "Epoch 314/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2879 - accuracy: 0.7728 - precision: 0.3218 - recall: 0.7456\n",
      "Epoch 315/500\n",
      "49/49 [==============================] - 0s 583us/step - loss: 0.2933 - accuracy: 0.7677 - precision: 0.3177 - recall: 0.7556\n",
      "Epoch 316/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2884 - accuracy: 0.7666 - precision: 0.3146 - recall: 0.7431\n",
      "Epoch 317/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2913 - accuracy: 0.7712 - precision: 0.3199 - recall: 0.7444\n",
      "Epoch 318/500\n",
      "49/49 [==============================] - 0s 580us/step - loss: 0.2881 - accuracy: 0.7745 - precision: 0.3218 - recall: 0.7331\n",
      "Epoch 319/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2924 - accuracy: 0.7688 - precision: 0.3210 - recall: 0.7694\n",
      "Epoch 320/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2909 - accuracy: 0.7692 - precision: 0.3188 - recall: 0.7519\n",
      "Epoch 321/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2898 - accuracy: 0.7658 - precision: 0.3176 - recall: 0.7682\n",
      "Epoch 322/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2923 - accuracy: 0.7737 - precision: 0.3238 - recall: 0.7519\n",
      "Epoch 323/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2888 - accuracy: 0.7667 - precision: 0.3163 - recall: 0.7531\n",
      "Epoch 324/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2896 - accuracy: 0.7680 - precision: 0.3184 - recall: 0.7581\n",
      "Epoch 325/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2879 - accuracy: 0.7709 - precision: 0.3195 - recall: 0.7444\n",
      "Epoch 326/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2875 - accuracy: 0.7642 - precision: 0.3171 - recall: 0.7757\n",
      "Epoch 327/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2926 - accuracy: 0.7698 - precision: 0.3206 - recall: 0.7594\n",
      "Epoch 328/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2897 - accuracy: 0.7675 - precision: 0.3170 - recall: 0.7519\n",
      "Epoch 329/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2886 - accuracy: 0.7709 - precision: 0.3193 - recall: 0.7431\n",
      "Epoch 330/500\n",
      "49/49 [==============================] - 0s 595us/step - loss: 0.2914 - accuracy: 0.7730 - precision: 0.3198 - recall: 0.7318\n",
      "Epoch 331/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2893 - accuracy: 0.7716 - precision: 0.3242 - recall: 0.7707\n",
      "Epoch 332/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2919 - accuracy: 0.7624 - precision: 0.3135 - recall: 0.7644\n",
      "Epoch 333/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2917 - accuracy: 0.7722 - precision: 0.3197 - recall: 0.7368\n",
      "Epoch 334/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2897 - accuracy: 0.7658 - precision: 0.3172 - recall: 0.7657\n",
      "Epoch 335/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2890 - accuracy: 0.7709 - precision: 0.3189 - recall: 0.7406\n",
      "Epoch 336/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2915 - accuracy: 0.7745 - precision: 0.3228 - recall: 0.7393\n",
      "Epoch 337/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2919 - accuracy: 0.7658 - precision: 0.3163 - recall: 0.7594\n",
      "Epoch 338/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2922 - accuracy: 0.7717 - precision: 0.3200 - recall: 0.7419\n",
      "Epoch 339/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2893 - accuracy: 0.7717 - precision: 0.3217 - recall: 0.7531\n",
      "Epoch 340/500\n",
      "49/49 [==============================] - 0s 598us/step - loss: 0.2896 - accuracy: 0.7695 - precision: 0.3199 - recall: 0.7569\n",
      "Epoch 341/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2914 - accuracy: 0.7670 - precision: 0.3168 - recall: 0.7544\n",
      "Epoch 342/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2892 - accuracy: 0.7700 - precision: 0.3189 - recall: 0.7469\n",
      "Epoch 343/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2916 - accuracy: 0.7656 - precision: 0.3175 - recall: 0.7682\n",
      "Epoch 344/500\n",
      "49/49 [==============================] - 0s 655us/step - loss: 0.2893 - accuracy: 0.7689 - precision: 0.3177 - recall: 0.7469\n",
      "Epoch 345/500\n",
      "49/49 [==============================] - 0s 673us/step - loss: 0.2894 - accuracy: 0.7645 - precision: 0.3157 - recall: 0.7644\n",
      "Epoch 346/500\n",
      "49/49 [==============================] - 0s 655us/step - loss: 0.2905 - accuracy: 0.7684 - precision: 0.3174 - recall: 0.7481\n",
      "Epoch 347/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2887 - accuracy: 0.7667 - precision: 0.3186 - recall: 0.7682\n",
      "Epoch 348/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2881 - accuracy: 0.7675 - precision: 0.3172 - recall: 0.7531\n",
      "Epoch 349/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2905 - accuracy: 0.7716 - precision: 0.3206 - recall: 0.7469\n",
      "Epoch 350/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 600us/step - loss: 0.2878 - accuracy: 0.7726 - precision: 0.3210 - recall: 0.7419\n",
      "Epoch 351/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2895 - accuracy: 0.7663 - precision: 0.3168 - recall: 0.7594\n",
      "Epoch 352/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2886 - accuracy: 0.7659 - precision: 0.3204 - recall: 0.7857\n",
      "Epoch 353/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2889 - accuracy: 0.7673 - precision: 0.3152 - recall: 0.7419\n",
      "Epoch 354/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2883 - accuracy: 0.7700 - precision: 0.3169 - recall: 0.7343\n",
      "Epoch 355/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2885 - accuracy: 0.7663 - precision: 0.3166 - recall: 0.7581\n",
      "Epoch 356/500\n",
      "49/49 [==============================] - 0s 593us/step - loss: 0.2923 - accuracy: 0.7664 - precision: 0.3148 - recall: 0.7456\n",
      "Epoch 357/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2906 - accuracy: 0.7716 - precision: 0.3190 - recall: 0.7368\n",
      "Epoch 358/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2905 - accuracy: 0.7711 - precision: 0.3205 - recall: 0.7494\n",
      "Epoch 359/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2891 - accuracy: 0.7694 - precision: 0.3184 - recall: 0.7481\n",
      "Epoch 360/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2897 - accuracy: 0.7631 - precision: 0.3123 - recall: 0.7519\n",
      "Epoch 361/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2875 - accuracy: 0.7592 - precision: 0.3089 - recall: 0.7556\n",
      "Epoch 362/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2887 - accuracy: 0.7666 - precision: 0.3156 - recall: 0.7494\n",
      "Epoch 363/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2890 - accuracy: 0.7692 - precision: 0.3186 - recall: 0.7506\n",
      "Epoch 364/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2888 - accuracy: 0.7720 - precision: 0.3223 - recall: 0.7544\n",
      "Epoch 365/500\n",
      "49/49 [==============================] - 0s 594us/step - loss: 0.2886 - accuracy: 0.7697 - precision: 0.3176 - recall: 0.7406\n",
      "Epoch 366/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2881 - accuracy: 0.7655 - precision: 0.3182 - recall: 0.7744\n",
      "Epoch 367/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2899 - accuracy: 0.7675 - precision: 0.3154 - recall: 0.7419\n",
      "Epoch 368/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2879 - accuracy: 0.7702 - precision: 0.3215 - recall: 0.7632\n",
      "Epoch 369/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2941 - accuracy: 0.7666 - precision: 0.3179 - recall: 0.7644\n",
      "Epoch 370/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2877 - accuracy: 0.7751 - precision: 0.3235 - recall: 0.7393\n",
      "Epoch 371/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2888 - accuracy: 0.7717 - precision: 0.3219 - recall: 0.7544\n",
      "Epoch 372/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2892 - accuracy: 0.7717 - precision: 0.3223 - recall: 0.7569\n",
      "Epoch 373/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2908 - accuracy: 0.7716 - precision: 0.3185 - recall: 0.7331\n",
      "Epoch 374/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2885 - accuracy: 0.7649 - precision: 0.3153 - recall: 0.7594\n",
      "Epoch 375/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2889 - accuracy: 0.7726 - precision: 0.3208 - recall: 0.7406\n",
      "Epoch 376/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2892 - accuracy: 0.7688 - precision: 0.3179 - recall: 0.7494\n",
      "Epoch 377/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2885 - accuracy: 0.7634 - precision: 0.3148 - recall: 0.7657\n",
      "Epoch 378/500\n",
      "49/49 [==============================] - 0s 575us/step - loss: 0.2898 - accuracy: 0.7698 - precision: 0.3203 - recall: 0.7569\n",
      "Epoch 379/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2890 - accuracy: 0.7658 - precision: 0.3159 - recall: 0.7569\n",
      "Epoch 380/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2883 - accuracy: 0.7688 - precision: 0.3198 - recall: 0.7619\n",
      "Epoch 381/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2879 - accuracy: 0.7672 - precision: 0.3180 - recall: 0.7607\n",
      "Epoch 382/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2936 - accuracy: 0.7703 - precision: 0.3206 - recall: 0.7556\n",
      "Epoch 383/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2928 - accuracy: 0.7624 - precision: 0.3142 - recall: 0.7694\n",
      "Epoch 384/500\n",
      "49/49 [==============================] - 0s 583us/step - loss: 0.2883 - accuracy: 0.7673 - precision: 0.3168 - recall: 0.7519\n",
      "Epoch 385/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2940 - accuracy: 0.7653 - precision: 0.3169 - recall: 0.7669\n",
      "Epoch 386/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2881 - accuracy: 0.7691 - precision: 0.3183 - recall: 0.7494\n",
      "Epoch 387/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2880 - accuracy: 0.7649 - precision: 0.3170 - recall: 0.7707\n",
      "Epoch 388/500\n",
      "49/49 [==============================] - 0s 575us/step - loss: 0.2915 - accuracy: 0.7672 - precision: 0.3155 - recall: 0.7444\n",
      "Epoch 389/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2875 - accuracy: 0.7636 - precision: 0.3111 - recall: 0.7406\n",
      "Epoch 390/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2899 - accuracy: 0.7653 - precision: 0.3158 - recall: 0.7594\n",
      "Epoch 391/500\n",
      "49/49 [==============================] - 0s 655us/step - loss: 0.2922 - accuracy: 0.7675 - precision: 0.3175 - recall: 0.7556\n",
      "Epoch 392/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2912 - accuracy: 0.7712 - precision: 0.3201 - recall: 0.7456\n",
      "Epoch 393/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2925 - accuracy: 0.7669 - precision: 0.3141 - recall: 0.7381\n",
      "Epoch 394/500\n",
      "49/49 [==============================] - 0s 593us/step - loss: 0.2900 - accuracy: 0.7688 - precision: 0.3171 - recall: 0.7444\n",
      "Epoch 395/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2889 - accuracy: 0.7661 - precision: 0.3147 - recall: 0.7469\n",
      "Epoch 396/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2912 - accuracy: 0.7681 - precision: 0.3159 - recall: 0.7406\n",
      "Epoch 397/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2907 - accuracy: 0.7725 - precision: 0.3181 - recall: 0.7243\n",
      "Epoch 398/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2902 - accuracy: 0.7716 - precision: 0.3216 - recall: 0.7531\n",
      "Epoch 399/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2883 - accuracy: 0.7645 - precision: 0.3128 - recall: 0.7456\n",
      "Epoch 400/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2890 - accuracy: 0.7698 - precision: 0.3193 - recall: 0.7506\n",
      "Epoch 401/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2916 - accuracy: 0.7652 - precision: 0.3149 - recall: 0.7544\n",
      "Epoch 402/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2900 - accuracy: 0.7748 - precision: 0.3235 - recall: 0.7419\n",
      "Epoch 403/500\n",
      "49/49 [==============================] - 0s 617us/step - loss: 0.2897 - accuracy: 0.7688 - precision: 0.3195 - recall: 0.7594\n",
      "Epoch 404/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2883 - accuracy: 0.7716 - precision: 0.3219 - recall: 0.7556\n",
      "Epoch 405/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2911 - accuracy: 0.7675 - precision: 0.3177 - recall: 0.7569\n",
      "Epoch 406/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2886 - accuracy: 0.7712 - precision: 0.3218 - recall: 0.7569\n",
      "Epoch 407/500\n",
      "49/49 [==============================] - 0s 655us/step - loss: 0.2913 - accuracy: 0.7683 - precision: 0.3188 - recall: 0.7581\n",
      "Epoch 408/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 618us/step - loss: 0.2896 - accuracy: 0.7653 - precision: 0.3164 - recall: 0.7632\n",
      "Epoch 409/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2891 - accuracy: 0.7681 - precision: 0.3184 - recall: 0.7569\n",
      "Epoch 410/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2894 - accuracy: 0.7739 - precision: 0.3217 - recall: 0.7368\n",
      "Epoch 411/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2902 - accuracy: 0.7741 - precision: 0.3240 - recall: 0.7506\n",
      "Epoch 412/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2889 - accuracy: 0.7711 - precision: 0.3209 - recall: 0.7519\n",
      "Epoch 413/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2902 - accuracy: 0.7705 - precision: 0.3178 - recall: 0.7368\n",
      "Epoch 414/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2889 - accuracy: 0.7644 - precision: 0.3152 - recall: 0.7619\n",
      "Epoch 415/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2913 - accuracy: 0.7697 - precision: 0.3180 - recall: 0.7431\n",
      "Epoch 416/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2895 - accuracy: 0.7711 - precision: 0.3201 - recall: 0.7469\n",
      "Epoch 417/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2902 - accuracy: 0.7650 - precision: 0.3153 - recall: 0.7581\n",
      "Epoch 418/500\n",
      "49/49 [==============================] - 0s 593us/step - loss: 0.2883 - accuracy: 0.7702 - precision: 0.3214 - recall: 0.7619\n",
      "Epoch 419/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2886 - accuracy: 0.7702 - precision: 0.3196 - recall: 0.7506\n",
      "Epoch 420/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2894 - accuracy: 0.7708 - precision: 0.3201 - recall: 0.7494\n",
      "Epoch 421/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2894 - accuracy: 0.7622 - precision: 0.3114 - recall: 0.7519\n",
      "Epoch 422/500\n",
      "49/49 [==============================] - 0s 655us/step - loss: 0.2891 - accuracy: 0.7655 - precision: 0.3161 - recall: 0.7607\n",
      "Epoch 423/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2893 - accuracy: 0.7678 - precision: 0.3201 - recall: 0.7707\n",
      "Epoch 424/500\n",
      "49/49 [==============================] - 0s 643us/step - loss: 0.2908 - accuracy: 0.7666 - precision: 0.3167 - recall: 0.7569\n",
      "Epoch 425/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2919 - accuracy: 0.7663 - precision: 0.3162 - recall: 0.7556\n",
      "Epoch 426/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2911 - accuracy: 0.7689 - precision: 0.3177 - recall: 0.7469\n",
      "Epoch 427/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2893 - accuracy: 0.7706 - precision: 0.3176 - recall: 0.7343\n",
      "Epoch 428/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2885 - accuracy: 0.7667 - precision: 0.3163 - recall: 0.7531\n",
      "Epoch 429/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2897 - accuracy: 0.7666 - precision: 0.3173 - recall: 0.7607\n",
      "Epoch 430/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2924 - accuracy: 0.7658 - precision: 0.3151 - recall: 0.7519\n",
      "Epoch 431/500\n",
      "49/49 [==============================] - 0s 575us/step - loss: 0.2902 - accuracy: 0.7666 - precision: 0.3169 - recall: 0.7581\n",
      "Epoch 432/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2886 - accuracy: 0.7673 - precision: 0.3138 - recall: 0.7331\n",
      "Epoch 433/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2887 - accuracy: 0.7711 - precision: 0.3210 - recall: 0.7531\n",
      "Epoch 434/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2884 - accuracy: 0.7647 - precision: 0.3172 - recall: 0.7732\n",
      "Epoch 435/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2902 - accuracy: 0.7664 - precision: 0.3162 - recall: 0.7544\n",
      "Epoch 436/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2889 - accuracy: 0.7689 - precision: 0.3167 - recall: 0.7406\n",
      "Epoch 437/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2875 - accuracy: 0.7742 - precision: 0.3216 - recall: 0.7343\n",
      "Epoch 438/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2886 - accuracy: 0.7705 - precision: 0.3209 - recall: 0.7569\n",
      "Epoch 439/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2890 - accuracy: 0.7688 - precision: 0.3173 - recall: 0.7456\n",
      "Epoch 440/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2880 - accuracy: 0.7694 - precision: 0.3203 - recall: 0.7607\n",
      "Epoch 441/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2920 - accuracy: 0.7666 - precision: 0.3184 - recall: 0.7682\n",
      "Epoch 442/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2898 - accuracy: 0.7658 - precision: 0.3140 - recall: 0.7444\n",
      "Epoch 443/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2911 - accuracy: 0.7734 - precision: 0.3227 - recall: 0.7469\n",
      "Epoch 444/500\n",
      "49/49 [==============================] - 0s 583us/step - loss: 0.2891 - accuracy: 0.7667 - precision: 0.3167 - recall: 0.7556\n",
      "Epoch 445/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2894 - accuracy: 0.7720 - precision: 0.3225 - recall: 0.7556\n",
      "Epoch 446/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2890 - accuracy: 0.7688 - precision: 0.3177 - recall: 0.7481\n",
      "Epoch 447/500\n",
      "49/49 [==============================] - 0s 593us/step - loss: 0.2882 - accuracy: 0.7747 - precision: 0.3224 - recall: 0.7356\n",
      "Epoch 448/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2879 - accuracy: 0.7733 - precision: 0.3223 - recall: 0.7456\n",
      "Epoch 449/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2945 - accuracy: 0.7708 - precision: 0.3197 - recall: 0.7469\n",
      "Epoch 450/500\n",
      "49/49 [==============================] - 0s 595us/step - loss: 0.2890 - accuracy: 0.7614 - precision: 0.3133 - recall: 0.7694\n",
      "Epoch 451/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2873 - accuracy: 0.7694 - precision: 0.3207 - recall: 0.7632\n",
      "Epoch 452/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2875 - accuracy: 0.7764 - precision: 0.3235 - recall: 0.7306\n",
      "Epoch 453/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2905 - accuracy: 0.7716 - precision: 0.3216 - recall: 0.7531\n",
      "Epoch 454/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2915 - accuracy: 0.7723 - precision: 0.3222 - recall: 0.7519\n",
      "Epoch 455/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2888 - accuracy: 0.7739 - precision: 0.3219 - recall: 0.7381\n",
      "Epoch 456/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2903 - accuracy: 0.7698 - precision: 0.3179 - recall: 0.7419\n",
      "Epoch 457/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2885 - accuracy: 0.7714 - precision: 0.3195 - recall: 0.7406\n",
      "Epoch 458/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2902 - accuracy: 0.7673 - precision: 0.3181 - recall: 0.7607\n",
      "Epoch 459/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2908 - accuracy: 0.7702 - precision: 0.3208 - recall: 0.7581\n",
      "Epoch 460/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2894 - accuracy: 0.7709 - precision: 0.3213 - recall: 0.7556\n",
      "Epoch 461/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2904 - accuracy: 0.7733 - precision: 0.3242 - recall: 0.7581\n",
      "Epoch 462/500\n",
      "49/49 [==============================] - 0s 640us/step - loss: 0.2895 - accuracy: 0.7658 - precision: 0.3184 - recall: 0.7732\n",
      "Epoch 463/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2891 - accuracy: 0.7753 - precision: 0.3225 - recall: 0.7318\n",
      "Epoch 464/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2898 - accuracy: 0.7684 - precision: 0.3187 - recall: 0.7569\n",
      "Epoch 465/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2892 - accuracy: 0.7652 - precision: 0.3129 - recall: 0.7419\n",
      "Epoch 466/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 600us/step - loss: 0.2921 - accuracy: 0.7697 - precision: 0.3212 - recall: 0.7644\n",
      "Epoch 467/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2890 - accuracy: 0.7661 - precision: 0.3153 - recall: 0.7506\n",
      "Epoch 468/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2888 - accuracy: 0.7684 - precision: 0.3210 - recall: 0.7719\n",
      "Epoch 469/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2879 - accuracy: 0.7694 - precision: 0.3154 - recall: 0.7293\n",
      "Epoch 470/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2918 - accuracy: 0.7672 - precision: 0.3160 - recall: 0.7481\n",
      "Epoch 471/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2871 - accuracy: 0.7677 - precision: 0.3175 - recall: 0.7544\n",
      "Epoch 472/500\n",
      "49/49 [==============================] - 0s 601us/step - loss: 0.2882 - accuracy: 0.7720 - precision: 0.3192 - recall: 0.7343\n",
      "Epoch 473/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2906 - accuracy: 0.7717 - precision: 0.3208 - recall: 0.7469\n",
      "Epoch 474/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2886 - accuracy: 0.7706 - precision: 0.3207 - recall: 0.7544\n",
      "Epoch 475/500\n",
      "49/49 [==============================] - 0s 583us/step - loss: 0.2888 - accuracy: 0.7680 - precision: 0.3194 - recall: 0.7644\n",
      "Epoch 476/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2886 - accuracy: 0.7692 - precision: 0.3182 - recall: 0.7481\n",
      "Epoch 477/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2918 - accuracy: 0.7627 - precision: 0.3145 - recall: 0.7694\n",
      "Epoch 478/500\n",
      "49/49 [==============================] - 0s 579us/step - loss: 0.2919 - accuracy: 0.7678 - precision: 0.3173 - recall: 0.7519\n",
      "Epoch 479/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2880 - accuracy: 0.7673 - precision: 0.3172 - recall: 0.7544\n",
      "Epoch 480/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2880 - accuracy: 0.7694 - precision: 0.3182 - recall: 0.7469\n",
      "Epoch 481/500\n",
      "49/49 [==============================] - 0s 593us/step - loss: 0.2894 - accuracy: 0.7680 - precision: 0.3184 - recall: 0.7581\n",
      "Epoch 482/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2898 - accuracy: 0.7673 - precision: 0.3134 - recall: 0.7306\n",
      "Epoch 483/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2930 - accuracy: 0.7697 - precision: 0.3210 - recall: 0.7632\n",
      "Epoch 484/500\n",
      "49/49 [==============================] - 0s 655us/step - loss: 0.2896 - accuracy: 0.7691 - precision: 0.3194 - recall: 0.7569\n",
      "Epoch 485/500\n",
      "49/49 [==============================] - 0s 673us/step - loss: 0.2886 - accuracy: 0.7711 - precision: 0.3207 - recall: 0.7506\n",
      "Epoch 486/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2890 - accuracy: 0.7709 - precision: 0.3231 - recall: 0.7682\n",
      "Epoch 487/500\n",
      "49/49 [==============================] - 0s 618us/step - loss: 0.2907 - accuracy: 0.7691 - precision: 0.3194 - recall: 0.7569\n",
      "Epoch 488/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2886 - accuracy: 0.7659 - precision: 0.3157 - recall: 0.7544\n",
      "Epoch 489/500\n",
      "49/49 [==============================] - 0s 637us/step - loss: 0.2895 - accuracy: 0.7692 - precision: 0.3169 - recall: 0.7393\n",
      "Epoch 490/500\n",
      "49/49 [==============================] - 0s 580us/step - loss: 0.2888 - accuracy: 0.7659 - precision: 0.3135 - recall: 0.7406\n",
      "Epoch 491/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2922 - accuracy: 0.7700 - precision: 0.3193 - recall: 0.7494\n",
      "Epoch 492/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2899 - accuracy: 0.7731 - precision: 0.3188 - recall: 0.7243\n",
      "Epoch 493/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2908 - accuracy: 0.7706 - precision: 0.3201 - recall: 0.7506\n",
      "Epoch 494/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2912 - accuracy: 0.7722 - precision: 0.3219 - recall: 0.7506\n",
      "Epoch 495/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2928 - accuracy: 0.7697 - precision: 0.3203 - recall: 0.7581\n",
      "Epoch 496/500\n",
      "49/49 [==============================] - 0s 600us/step - loss: 0.2897 - accuracy: 0.7712 - precision: 0.3218 - recall: 0.7569\n",
      "Epoch 497/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2893 - accuracy: 0.7686 - precision: 0.3162 - recall: 0.7393\n",
      "Epoch 498/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2883 - accuracy: 0.7705 - precision: 0.3207 - recall: 0.7556\n",
      "Epoch 499/500\n",
      "49/49 [==============================] - 0s 582us/step - loss: 0.2885 - accuracy: 0.7678 - precision: 0.3163 - recall: 0.7456\n",
      "Epoch 500/500\n",
      "49/49 [==============================] - 0s 564us/step - loss: 0.2962 - accuracy: 0.7695 - precision: 0.3207 - recall: 0.7619\n"
     ]
    }
   ],
   "source": [
    "epochs, hist = train_model2(my_model, data_train, epochs, \n",
    "                          label_name, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluate the new model against the test set:\n",
      "13/13 [==============================] - 0s 548us/step - loss: 0.2853 - accuracy: 0.7902 - precision: 0.3493 - recall: 0.7463\n",
      "(146, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_test</th>\n",
       "      <th>1_test</th>\n",
       "      <th>2_test</th>\n",
       "      <th>3_test</th>\n",
       "      <th>4_test</th>\n",
       "      <th>Class5_test</th>\n",
       "      <th>Class6_test</th>\n",
       "      <th>Class7_test</th>\n",
       "      <th>Class8_test</th>\n",
       "      <th>Class9_test</th>\n",
       "      <th>...</th>\n",
       "      <th>Class1</th>\n",
       "      <th>Class2</th>\n",
       "      <th>Class3</th>\n",
       "      <th>Class4</th>\n",
       "      <th>Class5</th>\n",
       "      <th>Class6</th>\n",
       "      <th>Class7</th>\n",
       "      <th>Class8</th>\n",
       "      <th>Class9</th>\n",
       "      <th>Class10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.211887</td>\n",
       "      <td>0.253394</td>\n",
       "      <td>0.402898</td>\n",
       "      <td>0.084645</td>\n",
       "      <td>0.038762</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.189280</td>\n",
       "      <td>0.195154</td>\n",
       "      <td>0.342920</td>\n",
       "      <td>0.070965</td>\n",
       "      <td>0.032187</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.079162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.211887</td>\n",
       "      <td>0.253394</td>\n",
       "      <td>0.402898</td>\n",
       "      <td>0.084645</td>\n",
       "      <td>0.038762</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.189280</td>\n",
       "      <td>0.195154</td>\n",
       "      <td>0.342920</td>\n",
       "      <td>0.070965</td>\n",
       "      <td>0.032187</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.079162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.189280</td>\n",
       "      <td>0.195154</td>\n",
       "      <td>0.342920</td>\n",
       "      <td>0.070965</td>\n",
       "      <td>0.032187</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.079162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.189280</td>\n",
       "      <td>0.195154</td>\n",
       "      <td>0.342920</td>\n",
       "      <td>0.070965</td>\n",
       "      <td>0.032187</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.079159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.211887</td>\n",
       "      <td>0.253394</td>\n",
       "      <td>0.402898</td>\n",
       "      <td>0.084645</td>\n",
       "      <td>0.038762</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.189280</td>\n",
       "      <td>0.195154</td>\n",
       "      <td>0.342920</td>\n",
       "      <td>0.070965</td>\n",
       "      <td>0.032187</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.079161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.189280</td>\n",
       "      <td>0.195154</td>\n",
       "      <td>0.342920</td>\n",
       "      <td>0.070965</td>\n",
       "      <td>0.032187</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.079162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.189280</td>\n",
       "      <td>0.195154</td>\n",
       "      <td>0.342920</td>\n",
       "      <td>0.070965</td>\n",
       "      <td>0.032187</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.079161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.211887</td>\n",
       "      <td>0.253394</td>\n",
       "      <td>0.402898</td>\n",
       "      <td>0.084645</td>\n",
       "      <td>0.038762</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.211887</td>\n",
       "      <td>0.253394</td>\n",
       "      <td>0.402898</td>\n",
       "      <td>0.084645</td>\n",
       "      <td>0.038762</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.189280</td>\n",
       "      <td>0.195154</td>\n",
       "      <td>0.342920</td>\n",
       "      <td>0.070965</td>\n",
       "      <td>0.032187</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.079159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.211918</td>\n",
       "      <td>0.253221</td>\n",
       "      <td>0.402626</td>\n",
       "      <td>0.084646</td>\n",
       "      <td>0.038769</td>\n",
       "      <td>0.008783</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.189280</td>\n",
       "      <td>0.195154</td>\n",
       "      <td>0.342920</td>\n",
       "      <td>0.070965</td>\n",
       "      <td>0.032187</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.079161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.211887</td>\n",
       "      <td>0.253394</td>\n",
       "      <td>0.402898</td>\n",
       "      <td>0.084645</td>\n",
       "      <td>0.038762</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.211887</td>\n",
       "      <td>0.253394</td>\n",
       "      <td>0.402898</td>\n",
       "      <td>0.084645</td>\n",
       "      <td>0.038762</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.211918</td>\n",
       "      <td>0.253221</td>\n",
       "      <td>0.402626</td>\n",
       "      <td>0.084646</td>\n",
       "      <td>0.038769</td>\n",
       "      <td>0.008783</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.189280</td>\n",
       "      <td>0.195154</td>\n",
       "      <td>0.342920</td>\n",
       "      <td>0.070965</td>\n",
       "      <td>0.032187</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.079159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.189280</td>\n",
       "      <td>0.195154</td>\n",
       "      <td>0.342920</td>\n",
       "      <td>0.070965</td>\n",
       "      <td>0.032187</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.079161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.211918</td>\n",
       "      <td>0.253221</td>\n",
       "      <td>0.402626</td>\n",
       "      <td>0.084646</td>\n",
       "      <td>0.038769</td>\n",
       "      <td>0.008783</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.211887</td>\n",
       "      <td>0.253394</td>\n",
       "      <td>0.402898</td>\n",
       "      <td>0.084645</td>\n",
       "      <td>0.038762</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.199123</td>\n",
       "      <td>0.212665</td>\n",
       "      <td>0.360618</td>\n",
       "      <td>0.076151</td>\n",
       "      <td>0.034777</td>\n",
       "      <td>0.106107</td>\n",
       "      <td>0.009725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.189280</td>\n",
       "      <td>0.195154</td>\n",
       "      <td>0.342920</td>\n",
       "      <td>0.070965</td>\n",
       "      <td>0.032187</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.079161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.199123</td>\n",
       "      <td>0.212665</td>\n",
       "      <td>0.360618</td>\n",
       "      <td>0.076151</td>\n",
       "      <td>0.034777</td>\n",
       "      <td>0.106107</td>\n",
       "      <td>0.009725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.189280</td>\n",
       "      <td>0.195154</td>\n",
       "      <td>0.342920</td>\n",
       "      <td>0.070965</td>\n",
       "      <td>0.032187</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.079161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.189280</td>\n",
       "      <td>0.195154</td>\n",
       "      <td>0.342920</td>\n",
       "      <td>0.070965</td>\n",
       "      <td>0.032187</td>\n",
       "      <td>0.087597</td>\n",
       "      <td>0.079159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.193618</td>\n",
       "      <td>0.217697</td>\n",
       "      <td>0.350477</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.035158</td>\n",
       "      <td>0.126288</td>\n",
       "      <td>0.000411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0_test  1_test  2_test  3_test  4_test  Class5_test  Class6_test  \\\n",
       "0      0.0     0.0     0.0     0.0     0.0          0.0          1.0   \n",
       "1      0.0     0.0     0.0     0.0     0.0          0.0          1.0   \n",
       "2      0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "3      0.0     0.0     0.0     0.0     1.0          0.0          1.0   \n",
       "4      0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "5      0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "6      0.0     0.0     0.0     0.0     1.0          1.0          1.0   \n",
       "7      0.0     0.0     0.0     0.0     0.0          1.0          1.0   \n",
       "8      0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "9      0.0     0.0     0.0     0.0     1.0          0.0          1.0   \n",
       "10     0.0     0.0     0.0     0.0     1.0          0.0          0.0   \n",
       "11     0.0     0.0     0.0     0.0     1.0          0.0          0.0   \n",
       "12     0.0     0.0     0.0     0.0     1.0          1.0          0.0   \n",
       "13     0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "14     0.0     0.0     0.0     0.0     0.0          1.0          0.0   \n",
       "15     0.0     0.0     0.0     0.0     0.0          1.0          0.0   \n",
       "16     0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "17     0.0     0.0     0.0     0.0     1.0          0.0          0.0   \n",
       "18     0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "19     0.0     0.0     0.0     0.0     0.0          1.0          0.0   \n",
       "20     0.0     0.0     0.0     0.0     1.0          0.0          1.0   \n",
       "21     0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "22     0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "23     0.0     0.0     0.0     0.0     0.0          1.0          0.0   \n",
       "24     0.0     0.0     0.0     0.0     0.0          1.0          0.0   \n",
       "25     0.0     0.0     0.0     0.0     0.0          0.0          1.0   \n",
       "26     0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "27     0.0     0.0     0.0     0.0     1.0          1.0          0.0   \n",
       "28     0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "29     0.0     0.0     0.0     0.0     1.0          1.0          0.0   \n",
       "30     0.0     0.0     0.0     0.0     1.0          0.0          1.0   \n",
       "31     0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "32     0.0     0.0     0.0     0.0     1.0          0.0          1.0   \n",
       "33     0.0     0.0     0.0     0.0     1.0          0.0          0.0   \n",
       "34     0.0     0.0     0.0     0.0     0.0          1.0          0.0   \n",
       "35     0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "36     0.0     0.0     0.0     0.0     1.0          0.0          0.0   \n",
       "37     0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "38     0.0     0.0     0.0     0.0     1.0          0.0          0.0   \n",
       "39     0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "40     0.0     0.0     0.0     0.0     1.0          0.0          0.0   \n",
       "41     0.0     0.0     0.0     0.0     1.0          0.0          1.0   \n",
       "42     0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "43     0.0     0.0     0.0     0.0     1.0          1.0          1.0   \n",
       "44     0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "45     0.0     0.0     0.0     0.0     0.0          0.0          0.0   \n",
       "46     0.0     0.0     0.0     0.0     0.0          0.0          1.0   \n",
       "47     0.0     0.0     0.0     0.0     0.0          0.0          1.0   \n",
       "48     0.0     0.0     0.0     0.0     0.0          1.0          0.0   \n",
       "49     0.0     0.0     0.0     0.0     0.0          1.0          1.0   \n",
       "\n",
       "    Class7_test  Class8_test  Class9_test  ...    Class1    Class2    Class3  \\\n",
       "0           0.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "1           0.0          0.0          0.0  ...  0.000006  0.000008  0.000009   \n",
       "2           0.0          0.0          0.0  ...  0.000467  0.000762  0.000783   \n",
       "3           0.0          0.0          0.0  ...  0.000006  0.000008  0.000009   \n",
       "4           0.0          0.0          0.0  ...  0.000467  0.000762  0.000783   \n",
       "5           0.0          0.0          0.0  ...  0.000467  0.000762  0.000783   \n",
       "6           0.0          0.0          0.0  ...  0.000467  0.000762  0.000783   \n",
       "7           1.0          0.0          1.0  ...  0.000024  0.000035  0.000037   \n",
       "8           0.0          0.0          0.0  ...  0.000006  0.000008  0.000009   \n",
       "9           1.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "10          1.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "11          0.0          1.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "12          0.0          0.0          0.0  ...  0.000467  0.000762  0.000783   \n",
       "13          0.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "14          0.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "15          1.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "16          0.0          0.0          0.0  ...  0.000467  0.000762  0.000783   \n",
       "17          0.0          0.0          1.0  ...  0.000467  0.000762  0.000783   \n",
       "18          0.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "19          1.0          0.0          0.0  ...  0.000006  0.000008  0.000009   \n",
       "20          0.0          0.0          0.0  ...  0.000006  0.000008  0.000009   \n",
       "21          0.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "22          1.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "23          0.0          0.0          0.0  ...  0.000467  0.000762  0.000783   \n",
       "24          1.0          0.0          0.0  ...  0.000006  0.000009  0.000009   \n",
       "25          0.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "26          0.0          0.0          0.0  ...  0.000467  0.000762  0.000783   \n",
       "27          0.0          0.0          1.0  ...  0.000024  0.000035  0.000037   \n",
       "28          0.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "29          0.0          0.0          0.0  ...  0.000006  0.000008  0.000009   \n",
       "30          0.0          0.0          0.0  ...  0.000006  0.000008  0.000009   \n",
       "31          0.0          0.0          0.0  ...  0.000006  0.000009  0.000009   \n",
       "32          0.0          0.0          1.0  ...  0.000467  0.000762  0.000783   \n",
       "33          0.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "34          0.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "35          0.0          0.0          0.0  ...  0.000467  0.000762  0.000783   \n",
       "36          0.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "37          1.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "38          1.0          0.0          0.0  ...  0.000006  0.000009  0.000009   \n",
       "39          0.0          0.0          0.0  ...  0.000006  0.000008  0.000009   \n",
       "40          0.0          0.0          0.0  ...  0.000146  0.000226  0.000235   \n",
       "41          0.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "42          1.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "43          0.0          0.0          0.0  ...  0.000467  0.000762  0.000783   \n",
       "44          0.0          0.0          0.0  ...  0.000146  0.000226  0.000235   \n",
       "45          0.0          0.0          0.0  ...  0.000467  0.000762  0.000783   \n",
       "46          0.0          0.0          0.0  ...  0.000467  0.000762  0.000783   \n",
       "47          0.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "48          0.0          0.0          0.0  ...  0.000024  0.000035  0.000037   \n",
       "49          0.0          0.0          1.0  ...  0.000024  0.000035  0.000037   \n",
       "\n",
       "      Class4    Class5    Class6    Class7    Class8    Class9   Class10  \n",
       "0   0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "1   0.211887  0.253394  0.402898  0.084645  0.038762  0.008377  0.000003  \n",
       "2   0.189280  0.195154  0.342920  0.070965  0.032187  0.087597  0.079162  \n",
       "3   0.211887  0.253394  0.402898  0.084645  0.038762  0.008377  0.000003  \n",
       "4   0.189280  0.195154  0.342920  0.070965  0.032187  0.087597  0.079162  \n",
       "5   0.189280  0.195154  0.342920  0.070965  0.032187  0.087597  0.079162  \n",
       "6   0.189280  0.195154  0.342920  0.070965  0.032187  0.087597  0.079159  \n",
       "7   0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "8   0.211887  0.253394  0.402898  0.084645  0.038762  0.008377  0.000003  \n",
       "9   0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "10  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "11  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "12  0.189280  0.195154  0.342920  0.070965  0.032187  0.087597  0.079161  \n",
       "13  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "14  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "15  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "16  0.189280  0.195154  0.342920  0.070965  0.032187  0.087597  0.079162  \n",
       "17  0.189280  0.195154  0.342920  0.070965  0.032187  0.087597  0.079161  \n",
       "18  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "19  0.211887  0.253394  0.402898  0.084645  0.038762  0.008377  0.000003  \n",
       "20  0.211887  0.253394  0.402898  0.084645  0.038762  0.008377  0.000003  \n",
       "21  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "22  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "23  0.189280  0.195154  0.342920  0.070965  0.032187  0.087597  0.079159  \n",
       "24  0.211918  0.253221  0.402626  0.084646  0.038769  0.008783  0.000003  \n",
       "25  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "26  0.189280  0.195154  0.342920  0.070965  0.032187  0.087597  0.079161  \n",
       "27  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "28  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "29  0.211887  0.253394  0.402898  0.084645  0.038762  0.008377  0.000003  \n",
       "30  0.211887  0.253394  0.402898  0.084645  0.038762  0.008377  0.000003  \n",
       "31  0.211918  0.253221  0.402626  0.084646  0.038769  0.008783  0.000003  \n",
       "32  0.189280  0.195154  0.342920  0.070965  0.032187  0.087597  0.079159  \n",
       "33  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "34  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "35  0.189280  0.195154  0.342920  0.070965  0.032187  0.087597  0.079161  \n",
       "36  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "37  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "38  0.211918  0.253221  0.402626  0.084646  0.038769  0.008783  0.000003  \n",
       "39  0.211887  0.253394  0.402898  0.084645  0.038762  0.008377  0.000003  \n",
       "40  0.199123  0.212665  0.360618  0.076151  0.034777  0.106107  0.009725  \n",
       "41  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "42  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "43  0.189280  0.195154  0.342920  0.070965  0.032187  0.087597  0.079161  \n",
       "44  0.199123  0.212665  0.360618  0.076151  0.034777  0.106107  0.009725  \n",
       "45  0.189280  0.195154  0.342920  0.070965  0.032187  0.087597  0.079161  \n",
       "46  0.189280  0.195154  0.342920  0.070965  0.032187  0.087597  0.079159  \n",
       "47  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "48  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "49  0.193618  0.217697  0.350477  0.076218  0.035158  0.126288  0.000411  \n",
       "\n",
       "[50 rows x 22 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features={\"no_of_max\":data_test[\"no_of_max\"].to_numpy()}\n",
    "label=data_test[label_name].to_numpy()\n",
    "#print(label.reshape((len(label),6,3)))\n",
    "\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "evaluation=my_model.evaluate(x = features, y = label, batch_size=batch_size)\n",
    "\n",
    "predicted = my_model.predict(features)\n",
    "\n",
    "#label=label.reshape((len(label),6,3))\n",
    "\n",
    "#predicted=predicted.reshape((len(predicted),6,3))\n",
    "print(predicted.shape)\n",
    "\n",
    "\n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "df_test=pd.DataFrame(label,columns=[\"0_test\",\"1_test\",\"2_test\",\"3_test\",\"4_test\",\"Class5_test\",\"Class6_test\",\"Class7_test\",\"Class8_test\",\"Class9_test\",\"Class10_test\"])\n",
    "#df_test=pd.DataFrame(label,columns=[\"k6a1_0.25_label\",\"k6a1_0.5_label\",\"k6a1_0.75_label\"])\n",
    "df_predict=pd.DataFrame(predicted,columns=[\"Class0\",\"Class1\",\"Class2\",\"Class3\",\"Class4\",\"Class5\",\"Class6\",\"Class7\",\"Class8\",\"Class9\",\"Class10\"])\n",
    "#df_predict=pd.DataFrame(predicted,columns=[\"k6a1_0.25\",\"k6a1_0.5\",\"k6a1_0.75\"])\n",
    "#df_test = df_test.round(0)\n",
    "#df_predict = df_predict.round(0)\n",
    "pd.concat([df_test,df_predict], axis=1).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd3gU1frA8e/ZTa8kIRAggdBrCIQWOqggKIIgxQqo2L3qtRds6M9y9VqxYQMLHVFEioTeJfQSOgFCSSC9Zzc7vz8mu9lNNiEgK3r3/TwPD5mZMzNnZ2fOO6fMrNI0DSGEEO7LcKUzIIQQ4sqSQCCEEG5OAoEQQrg5CQRCCOHmJBAIIYSb87jSGbhYtWvX1qKjo690NoQQ4h9l69at5zVNC3e27B8XCKKjo0lMTLzS2RBCiH8UpdTxqpa5tGlIKTVIKXVAKXVYKfWsk+UNlVIrlVLblVK7lFLXuTI/QgghKnNZIFBKGYFPgMFAG+AWpVSbCskmArM1TesI3Ax86qr8CCGEcM6VNYKuwGFN045qmlYCzASGVUijAUFlfwcDp12YHyGEEE64MhA0AE7aTaeUzbP3CnC7UioFWAT8y9mGlFL3KqUSlVKJ586dc0VehRDCbbkyECgn8yq+2OgWYKqmaZHAdcD3SqlKedI0bYqmaZ01TescHu6001sIIcQlcmUgSAGi7KYjqdz0czcwG0DTtI2AD1DbhXkSQghRgSsDwRaguVKqsVLKC70zeEGFNCeAqwGUUq3RA4G0/QghxF/IZYFA0zQz8DCwFEhCHx20Vyk1SSk1tCzZE8A9SqmdwAxgvCbvxf6ftyR5CZlFmVc6G0KIMuqfVu527txZc+UDZc+tfY7jOceZfv10l+3DnaUXptNvdj/i6sQxbfC0K50d8SedzT9Lck4y8fXir3RWxAUopbZqmtbZ2bJ/3JPFrrbw6MIrnYX/aQXmAgAOZR66wjkRl8OoX0eRVZzF7nG7r3RWxJ8gL537h8oqymJ72vYrnY2LlleSB0ChufAK50RcDlnFWQD801oWhCMJBP9QM/bPYPyS8WQUZdR4HVOpiTELx7Dh1AaX5CmtII0b5t/AiZwTDvMLTAWMWzyOned2kmfSA4FZM19we1/u+pKJ6ya6JK/i8jJZTFc6C+JPkEBQA6fyTpGan3rBdEWmUv44Vl4wp+bkMGrBGPae33vZ85RelI5Fs7Du1Loar5NWmMa+9H28sP6Fy54fgCXHlpCck8z3+753mL/n/B62pW1j4rqJthpBTXy0/SN+OfLL5c7m/6yn5uxk+mbHIPzesoPsSsmqcp0icxFJ6Ul/et9FpUUXTGOxaBSZSv/0vq60E+kFfLT8EBZL5VpQkamU/OIL3+T83bhNINhxMov3lx28pHUHzRvENXOvwaJZOJBxoMp0r/+2j9FfbOToOb2w6/Xhd+zP3MfbW96ulHZf+j7uW3YfJaUll5SnnOIcAH4+/DMpuSlkFWVxf8L9toCVV5JX6c48tyQXgGJzsdNtHk7LY//ZHId5y5NS2Z2S7TAvr9jMdxuTK10I3kZvQO8HsB6nw5mH2XV+l57nkhxbjcC67HTeafJK8nho+UOczD1JVbKLs0nJTeHxWTt4bGbNm8QOnM11yGdKZgE5ReV3r0lncnhyzk56/2cFmfmO38W5gnPcv+x+MooyyMgvYdqGZIrN5QXZrC0nSDqTQ06RiR82HSfutWWcSNf7QPadzuGXHacctlditvDcT7tt58fqg+fILjRx9Fweaw6eQ9O0CzaxZBdn245TWm4Rv+zeyczEg2TmlzBnawrPzy9vqz+fV8xHyw8xdPJ6ikyl7Dudw+gvNpJVUELCvlT2n83htU2vMXrhaM4XnictpwhTqQWA/WdzGPPFRrILqr7Tt89rVecUwNOrn2bdqXW8+/sBWr24hMKSUjRNw1y2r0thsWj0fGsF364/ZptXatF4e8l+3lyUdNFNVSWlJRzOPMyK/am8Z1dObDuRSVpuEcuTUhnx6XqSz+fzws+7eW/ZQT5fc4R5W1Nsafeeyib+3e9o/+rvl/y5rFIyC2zfxV/BbTqLt5/I5MPlhxjTJYr6tXydprE/eTRNQynHh6Pf/eNTvt//Be/3msY1TeMqrZ+YrA+JPJyWR4MQX0otepw1WyrfIbyy4RWSMpI4lHmItrXbOiwrtZTy0oaXGBg5kt8SDXRtrvHL1nzeuakTPp4Gavl5kVOiF9hbzm5h8E+DebrL06w/tZ6bZ79Bj4YtWJDyOQC7xu5CKUWppZRd5/QCuai0iF8PLyY59zD/6vgvCktKySwo4Zr3Vuv5/7/BeBgNrD2xjYd+/wjOj+KF69vRs2kYecVmhk5eD0BkiC9Xtapry7e13X/BkQX6vxsXMHzBcNvyrOIs0grSbNPDFwwnyCuIgPwRnPZcQ4BnAG/3qRw0C82FjFgwgrSCNHKT3gLg9eExBHhXf/r+cSyD0V9s5MnrwjlmmcXrvV6n19srqRvkzebnr2HJnrPc/8NWW/rl+9NYc/AcIX6evHRDW2YkzWD96fXM2j+LH5a05FRWId4eBkbERZJTZOKZebsJ8PagV7PaLNl7FoA+76xk2l1d+SDhINtPZLH20Hlu7NCAXs1rk3DgKDO37uV4ej7/NzyGcd/8QaMwPzLyS8gtMtOibgANQ/354o5OlFo0/jiWQUJSKk9d25L04tPM35LHT2lPkFGSwqabtzPko3UURv6b0qJ6fLWmvJZnKrVg0TT+PWuHbd6YKZvILTRx9Hw+CUlpPDlnJwAt4vT/E0+c5NHvTxFZpxBfFcGeU/r51e/dldwUF8lz17XGaCi/Hk5mFDDy8w22l8YUlxaz/2wOs7ekcEvXKJrXDeR0ViETf9nBFhazOHkxliPvADDu2z/IKzKz70wOe1691vY9FpSY8fPS/z6afZTooGgUitmJJxnQJgIvDwODP1xDl0ahPD6wBaeyCnn1130UmSxEhvgSGeLLZ6uOANAozJ+Bbeuy73QOnaNDOHA2l+Z1A0krPEF0cDQFJRZMZgsh/l4APLR0IpvOLSbv4ES00gCGxtYnOsyPEZ86NqM+PGMbhrJy4T9L9JudI+fyGNg2glEz3sSn/iK0Yw9SZCpl+cklnMo7xY2Nx/Lwj9tpXS+Qpwe1wr/CeVtkKuW3XWdoUTeQmMhgzmQX0uvtlQBc1aoOozpFEt8kjEAfDzyMrrl3d5tA0LlRKABbkjNYffAcozpF0b1pGKZSC4nJmcQ3CXXowJy8aj9eBi/aR/nY5n27dRke/nDfj2s5/GJHW6DYlZLFqgPnyC/RC/yDqbkOhVSxueqq4ucbtnB/p0jaNgi2zTuZe5IFRxaw8OB6sg8+zuL8FzDlxxD/5m0AHHh9EElpqXgbvSku1e/EjqTpVfMzOZm2IACw7MBRfAyBfHv0GRJT9WG3JouJ59c/DcC/Ov6Lu6dtYcORdNs6S/aepW+LcB5d/gSetc6Td74/L/68Bw+DwmBXGOw8mU3/lnXQNPh112lSCssLeYDtqY4jSSyahaVH9WDz2TWf8UDCA+SU5JCWkYpPXTDizYcJhxjTtR57zpY/hJ5RmGELIN715lJ8ZjhPzdnJC9e3Zsqao7SMCMTbw8icxJME+njQoJYve07n2KroX+79AIvfbix57YHapOYUs+bgOZ77aZdD/qyFI8C2E1mcLE2DEDiWmcaprIYAPPvTbp79qfxz5RWbbUHA6vFZOwgtK2Dmbk1h7tYUPrk1jme3DyegBWxIeov+764C4HhZDQLgYGoeB1PzuPq/q0i2m28mj18yJ1CS2Q2vEP0OtNc7i8jMUwQCRp8zHD5WXtP6ZOVh8ovNrD103u670puHPIK28VzCOpRHDJo5mJSMUoy+8NCstRiCTpEWvIj8o48A9QHILDDx1bpjfLXuGM3qmbm9S1taRPgxb/tRUnOKCSwLBDd+uorooCYkHs9kwc5TLH2sD28v2c+KA6cIbKmnyS/Ra1OH0/LIKKt93fDxOqJC/agb6M1P+xO4pbeisV9XPkh6gCc6PUETr+t5Zt5u3ly8n7HdozmZUcjJjFP8tL28tvX2kv0ADGlfD4BQfy+en7/boXYEMLa3Pz+ff4RJ8e/wxLRS6tfyYc1T/Uk6k8uGU5sxeIEyFqCVBjDk47UM7ugFWLBvONmfsx6DVzrQj9ioWuw8mcWnq47w6aoj+EbpQUh55DHk43Wkhulv3k/cEccfyRn8kZzBtE2H6dtjNXe1eYCfEwtoUTdQz7+hGC+PUkZ2bMU6u+9txf40Vuwvv66+uKMT17aN4HJzm0DQql4gvp5G5iSmsO7weX7adooBberi5WHgt11nePumGLo2Kz8c/122Dyw+GLxS8W+qz1MG/eRVnllM2bCNyb+fR0MvDOx9vCaRQM/aKKVXq5POZtH1/xL46cEeNKjly9K9ZzmTrRfcqzL/y7Ifj7L9sZfZePQ8c7emMKqnvp65VEN55APgEbDPtv1+7y4hJ/ws/pZ2jGody5wj3zBj+xa8QgGjYxX9/pm/ozzy8Ity/uzFc/MT2XAkHWXMR7N442304u0l+/lq7TGKPRUGbzB4ZlFqqo3ZoqEMWegvjFV8uPwQR9JT2Xgkh/Rc8Km3D89a5dt+efl08IOete7n1WtGMnDeQJKytqNpRrxKWmNJvRVD3ekYffUmrMNpBUxPPMhnSS/jGVR+EU/8bZXtb69aiTTyvIrFe4ws3uNYAAME+3qy1nSeYnNZtVqZKSEHD+DXnWewvsFk7Dd/AHqtpnW9ILw9DCzcdQYfTwNFJgu7T2XjGWLBB1iw+xCexi6YSu2aG5QZDMVQ6g/ArHvjeXLuTk5mFJJVaCK9QjPTQ9O3Edja6VcAwKtD2xITGcyITzfYgkDj2v6cL8jgt/MvghE8/A/b0mebT4Oh/L1bBp8UBndL47fVHfkgwXFoboC3h+0c9W0wW58ZsZDcpP9Ds3jqH8cjl8YRxZzRwCMgCZM5GK3ss7WpF8S+1DRSa73Cm390w+iTgtH3FPCWbR8Zhfmcz8ykcW1/jp3P59NVR/hlx2l8fByvjfE9ookO8+OVX/XzOTnrNMfO6zdBga2n8stxsJhnYfCAqVtX0ahUf3N9VoGJj5ZXP+R44a4ztK4XxNQ7u9DtjeWVlv+StAUtXOPpX5ZTaunLyYxC7v9hK2m5xaAfBpSxgFu6RjFr10YScifjWWs4pqxuALw5IoY3kvTC/bme93Fnj2bMTjzJ03P1GwpfbxMmAIsXh9PyCAzTt/n7vlRiGgTTo1kYX239jW0ZCfyx+BSFKeNseYto/Sn5llSmb9aPaZ8W4bx8QxtmbznJF2uOlqcLKr8xvZzcpo/A02ggvpk/W3K/BoNeCC/bl8pvu84A8My83Vw7ubxj0svDwtjujVCedh1tBr2Q9a0/l8mHx5NbXFwpCPRoXYxX4zfwD03kkWuiy+ZaSMst5vn5e4h/czkPzFhLel55QaECt9Phkwe5a+omFu0+yz3TV1iX0CjcYvvbKif8dQweeWTne/DdCv3O01qYKmO+Q348vDPxDjyCphmdHpeZ2/cAGgEtXiO08Sx+nNCNU5mF7DiZRd1A/QI1eGUAFppEnSGg+Zt41vqjbG2NlbkTKa31K60iAlEeFTqC/fRO8mU7S/llax5Gcx19rVIfRn+xicICPWp4Bunpdp05UzbteCeXWPyWw/S/BzS1/e3tUX4K//pwL7ZOvIZ9kwZx5I3rWPivXgRHT8PDT/9hJoNPCl61E4gM0ZsG4xrWYt0zV/Hl2M60j9Q/a5foUFAmvOv8htFHv/s2ehby1bguxEbp+f35oZ506jKfwBavAXoB0a1JGAsf7s3c+7tTSiHeET9zX7+6PHZNc65qVQco71v4/PY4XrmhDff0bmybN65HNHENQ2gVEQjAhzd3YMUTfYlqtAOzUT8umsXLlv7qGAM/3tu+/FA3+ow1abP1AGXn14d7sevlgdzdqzHXtK7jsKxjtCdY9H6dVg0Uw9rpha53nWUEtHiNsLJazdfjO9O7pR4UvEI2lwUB+GFCefNonSAjnkbFS0PaEOzrydfrjuFhUEwZF2NLE9H0V65q60lMpH4cjQH7CWj+JkZ/x747g4ceCM/mlLD64DkGtY2gbpA3BqXfEds78sZ1TBrWljGdo1DGPO7sGU1dJ4XlsA71KdT046g8sxnXvRGdGoWwbF8qh9PysF5f3Zt78+aI9jw6pCwv3mfp3iSM7S8O4OYu5a9O8w/Ua6yjO0fx9bjOfDO+M0H+Zf0pBjOeIRttaW+Pb8h/R8dyR89gQqOW6jOVmQ9v7sBD/Ruz+N+dyLdYB6Po1/vt3RrSNDyA565rzZE3rmPNU/35/d99bOfg5eY2NQKA2GYZJB7eiDmvJaX5rWzzx16dx/Qth/FuMNM2r0OH1QTUP8ZjkUFM2aPP8/AwObw+tXurUkp9t1HX0INnB/QmNaeIs+aNPLMWGkUn0ap+HByCxuF+1PWuw4r9aRh8ThDQwvH3dzz8ksEvGXNua0qLGuDbYAYAgd5e3NAqiKlHwWgw8uzgVry1eD8Ga4Fr8cazVK8mWi9OT9/TDnkc19eH43kaO8/UowC9YHso9hE+2fkRANd38Of163rRexaYvHcTGHSO8dfvJ9Q0iE2FAZw7B8GBOfSOPU5C2hcA1K13kLkPPM8zv6xim5ZBnVrHWTKqDwNnfcAZJ4NHNHMgbyzaj29kLTwCz9oKn0f6duXrE+XHQhlzCfS58Cnp7V3E8if64uNpxM/TSNLZHCKCfGgSHuCQrlldbyw+5XeR3rX1Zqln4x7l4R/34he2nQ2nFT3q96BJ7QBQxUSG+NGr0x525q+1rdc0opS+LcLpEFWLnEITUaF+HFyh9y2M79GI4R319pFgP086R4dyVfwOtmRuwjM4nse6342madz87WKsdbpB7erZtj00toHDZ35+eABrTyRxTdt+aGh4exdBWSWvQYg3Z8taLzs2LaVOeWsiyqAHmi/GtqOgyAeFIszfSLMILwpLC3hxiF7Ix9g9zP3GqGje2BjGjnTo38YXZXAMIglP9MaAkWA/T54f0ojRFZ61jAgrv+l4f0xbutaLx2hQdIkOISEpjeZ1A6kdVH4Dk++1ngxtEL2ihvHUtS3ZW7CXteehY9skukdZmLrPcfsGjPRrGc4nt8VhNChKzBa8PAxMvL41+cWlxDWqhdGgGNs9mnkHfmZR/uvERM8B4MFBFo5mnWJYi0EsP/M9HjlhGLz115gN7uDLq/3bYdEsnMzKItQ3kJt+/ZAzBelc31E/qFkmvbZ5X69YHu9S/tS0xRyAwSOPPI4BfQDo3SIEDQ3DLv3kf3NEK17bUv6DjK/fqAfDu5c+RhH6dusGeTKsQwPe3zqb0Uu+saVd/ER76vrVszUtAhgNitpB8OzaFxhjHkPPBj253NymRgDQoLZ+F/5A/yheuK4139/dlUeuasb806/jaxcEAJJyV/P9vu+ZsucT2zxPT8dml56xp0gqmkdMy2Qa1PIlrmEIp/L0Ajm9MN3Wfm+yFPHe6Bge6NeUzs2qHgkQGrmGYf3L26n9fUtpWV+/kDwMYApaxNd3NbMtV8Y8Zk64msHR19vmaap8lIeHwYOZh75l45kNdI5sYpt/f4d7eKPXGwAMivXlTP4Z27LJ2ycz58hUigIWk1OijxYKDt9lCwIADUIVEcE+DO+uf5YzBSmk5qeSbT5FhG/DSp/rs5v7A2AxhZT9X4sRcQ14tF8cXcP7c23Eg7QP64jyyLPdhVYnqziLpuEBNKjlS4i/Fz2a1q4UBAD+tcLpz1sQ01Bx09X72Fn0Bfctuw+ARhF5BLZ6mbg2R/EIOOyQPqtYHxJ8JGc3k/e8ymc7P7Mte+765vh46rUts8WM2WIm36Df4TYKC8JUakIpxeODw2zr2A9KiIkMJrq2v236oVXjmHl0MvHT43l2zbMcLl5sW3a2MNn2d4Ep32EEllWbSG+Gd4zkxo4NmH7iVbr+2JX46fGcK6j8Lsdzhefw9NSviczi9EpDQDenriLx/BoAskuyK61/LKd8xE5haSFaWa3nuhg90FksWqUHB/NN+SgFnmEriC6roBzI3cjUfVMrbb9PqwDiYjdhsujXkVdZ7W9C7yY8ek1zWjbQ+HTHp5gsJn499jMA287pAfr748+zPvsTdufPZmnKbCKjkqgdotfurf1N/038L0MW9MVgLMHbQ/8O883650zOTtYzYXC85v089TakotLyIHjvsnvpNaOX7eE6H2/HIbKlllKH/QJEhflgtphZk7LGIW1eaZpDELA+n3G+8DwrT67kfOF5XMGtAkFaWWdm07qe3NOnCb2bh/P4wJY1Xr/E4nhSW8e4219kyTnJAJzIPcH+DL0T60z+GZ5c9zDPDGrF8E51qUqR8RArTpc3T2UUZpBelF6272Km7JrCd0fesC23mINoGRHIbW1uqbStb679hnvb32ubDvUJ5cnOTzKujd4ueVXDqwCYd2iew1h9a4fyoaxDtofV0ip0Ah/PPUZ2cTaHssqfj5i6dyoF5gJubTMS0IeSNgjQ75QHtm7El2M70yRUr70MaR3L2ze1RynF19d9xLvXPkBMnTb4+OTwwa0tHPY1IWYCMbVjCPAsL+hr+sI667GraFHyIn4//Z1t+ru93/HcumcA2Hh2NdnFjoVeVnEWpZZSvtz9JYuOLeLTHeW1mCJzeeE5fsl4On7fkX3p+q3tlF1TiPshjvOF523nAujDay2apdJ+KlqcXB4Eorwdm0TyzflOn8koMOnNKhWfMbHeoNh7aPlDbDm7BdAHKNh/FoCn1jzFYysfY/6h+aw8sbLS+oezygPmoysfpe+svgAMbleP2Kggnr2uVaVAkJydzBOrn+Dj7R9Xet6kovWn1/Hl7i/5es/XTpd/t/c7Ptv5GT1n9LQ9q7MjbYdDmk1nNgHg4VlMiSHVlgeTxcR3+/RzIH56vO26zSrKYu7BubbrILckl5ySHNvIP2XUj5H1OANsTd1KUWmRrdD+ePvHDnmw3hA6Bm4LIxeMdDiGACm55cNRz+afJf7HeF7d+KrtXA7zDcMV3CoQWMfYF5gLWH9qPdtStzksjw6KpuDk+ErrNQ923st3Nl+v5tlH6RM5J2gc3BgP5eFwom8+s5nNZzbz4/6pNcrrmJZjMGtmjmUfc5i/N10/4Z/s9DTL7nwNH08j7cLaVVo/1CeUCe0mMLTpUFsex7Udx5NdngTA39MfPw8/ElMT+THpR9t61pN1w+kNVRZUBaYCZuyfwe7zu237/iHpBwAGNx5MXJ04/tPnP8y4fgazh8xGKcWANnW5q7teRe4V1QnPCsPghjcbTikm1qTOQ9n1hzwQ+wDTr5/OghvL32BuDQRbzm5hbcpaqlJoKqR9ePtK8+0LcoB3Et/hQKY+FHDZ8WUczjpMw8Dymo2GRlZxlkO+7I+FRbNgspjYeW6nwzLr+ZGan+rwOpD46fFM2jiJXjN78eL6F9E0jQVHFlT5YFet/Fu5tln3SvvNN+VXSjt5+2SKzEUczHRsd79j8R2sOLGiUnqrQ5mHqnztx0sbXmL6/sovYaz4vijrkOYSLY+MsOc5x6pKx3rmgZksO77MYV6EfwRXN7y6yrw5y3dSehKrU/SmvkJzIW3C2tAqtBVHso84pDuWpV8/W1O3UmguJK5OHLmmXL7a/ZXTfZ0rPMerG1+1Ta88sZKeM3pyf8L9mCwm2zHKN+fbanahPqEO27B+71YJJxL0p+rtAveOczsq5RVgzsE5tu3uSNtBiaWEuQfnMnbxWADCfCQQ/GnWqlmBqYD7E+5n3JJxDsu9jF68eF3lguOWNqMqzWsSXN7Ucq6wvEaQVZxFi5AW3BVzV6V1Jvw+geM5x2uU12719JEKFR9gs95djGp5E41CyzrdDEbm3DCH//b9ry1dhH8EnkZPnu7yNNFB0dzX/r5K+6jjV6fSPIDrGl9n+3tYU8efmV44fCH1/OtxJOsI+9L30bFuR9uF8EavN4jwj2Da4Glc1fAqQnxCaB1WHkRHNBvB5Ksm24KTvZahLekS0aXSRe9l1KvJtbzLO8kyi/VAcNfSu3hw+YMUlxbz8PKH6T+7P0ezjhIzLYbEs4kUmgtpFNjI6WfsVLeT0/lWzUOaO0xnFmWSXZJN93rdHdYdOG8gsd/FEvd9ecepv6c/9f3r26a/3fsta1LWYLD78b15h+YB+gOBT65+khfWvcDohaMr5WNgo4GsffA5moSUby/UJ5QCcwG5ptxK6VelrOKXw784ffCxuhcq5ppyK910XIi15mPPVGri1yO/klWcxWubXmP3+epfRhdTO4ZlI5fxfr/3eav3W07TpBaUP9U//9B8YqbFMHrhaNtd/E9Df2La4Gm0Cm1FdlE2vx751Zbe+ioTa83g4Y4P0zq0dZU3ENZtWlnvxDef2UxWUfnAkbkH53LjLzdSYCogpziHkS1G8vVA5zWXF9a9wO2Lbr/g09ftwtqx+/xu2+d1duxq+7rmd7vcKhBYD/Dp/Io/lKbzMnjRvoFjxP1u8HfU9avcnBNTu3w0xM5zO20XXm5JLoFegfSo3+NP5TXcVx8aaL1TrbjMz9PPYV6r0FbE1XUsjACCvYP5dfivdKjTodJ27JtbrMJ8wogOigZgbJuxTOo5if5Reht/oFcgjYIaEe4XzrbUbRSXFtM6tDVTB03l+8Hfc0PTG6r9TEaDkb5RfSs9qGfVp0EfjmQfQav0i6bgafTkq4FfUc+/HhlFGQ5PZC88spDVKas5X3jeVqWfsX8GRaVFBHsH0yiocjD49OpPuaFJ1fltVquZw3RGUQY5xTlVbs/hcyojIT4htumlyUsxWUwMaDTAIV3HOh0B+P141U+i1vPX29vtC4AGAQ3YfGYzW87ozTrWJ7qtXt/8OhPX6+9osg9I9oHIQ5V3UPt66KOorA8c1pSz5qaFRxc6fZK+KtamDqUU1zW+zum1ll2cbaud/nToJ4dl4b7htqBdy7sWaYVpPL/ueYc0gZ76SCyFokVIC5rValZlgDqcedjpfKh8fI5mH2Vp8lLMmpnooGi6RHSp7qNeUMe6+vmwLXUbH2z9wNZ0Zc/+vLqc3DIQVGwSsmoU3D4LmhkAACAASURBVKjSRdWxTkfbhWLP2uRgPcnuWqrXAKyBoGJ18WIYlMF24ZssJtqEteH1nq/zag+9ylpVFT7UJ5R6/vV4tuuzTpdX5KzAHd58uC3INA5ujEEZuLnlzUB5p1e4b7it36BhUEMaBzd2GmguVkx4TLXLu9XrRkztGI7nHOdQVnmzhH2brLV54mz+WQrNhfh6+LJw+EJmDpnJE52esKXz8/Rz+r1aVawtZRRlkF2cTbB3MBF+1T/QY1RGavlUHuY3sNFAh+nqmkOs6gXogcB6YwAQ5BVEobmQxcmLUSjb9+PM0pFLHT4DwNNdnmb72O18c60+WsVa+Do7H6z7q6mXNrwEwIBGA2x9RNWxL/iVUiSMSnB6E3Uw8yC/HP6FyMBIh/n2d9nB3sEVVwNgYvxEGgU14pmuzxDsHUzTWuXDjz++qvzcCfcNp8RSfoNhvYatwXTNKceOXSi/UQv2Dq50g2MN4s70btDb9neH8A78X6//Y1D0IACeWfuMrV/k9ta3O6znYXDNQE+3GT5aYCqwvWvnaPZRh/keBg9CvEOY2G2iwwian4bqdx+BXoGVtjekyRCUUgR6BfLU6qfIKcnhy11fUmIpIcgryCEQPNf1Od78481q8xcdFE0t71rcH3s/DQIaEO4X7rBsWLNhpBfq1dSqgoxBGfh9ZM3fc2J/4b/f732yirO4sdmNaJqGv6c/w5vpr4ewFmoWTR8lZJ+3mlzsNVXxTntUi8pNcs1CmrHs+DISz+p3/i91f4lJGyfZllvfw2N9v5E1qLUNa0vbsLY0D2luu0CtywZFD2JJ8hKH/QR7BzNlwBS8jd6MWzKOtafWklmcSZBXEEHe1ReMSilCvR2/oyc7P0n9gPoO8yIDIulUtxPF5mKGNx/Oa5v05xLubnc3w5oNY+e5nbbgYX/M7WuDIT4hdKvXjWn7yseFRvhHcDb/bKVCxNoxbA2A1rtxi2YhKjCqync9+Xr40rN+T1vn9bNdnyXIK6jSnbdVVGAUb/d+G0+jJzHTqg/u9k2sVl8M+IJJGycx5+AcwnzCSC9KZ8LvE7BolkpNI/Y1Q/vmQ3v9ovpxXZPy5k77fUb4lwf1FiEtHJp5rTcV1ze5nq92f8VPh37CqIzU9atra1Ww9sUEe1UOQuPajuOtP8qbu+r41eH9fu9jVEbyTHmsPaU3TxkNRoY2Her0xZbXN7ne1v/mSm4TCOyHbtn77dhvmC1mRrUcRYBXAN5Feo0gwDPAVuVsXqs5j3R8hAj/CNvJ7+fpZyuoFIonVz/JR9v1sfmBnoEOd1FDmg65YCCYO3RupdpIkFcQOSU5toI/zDeMd/q+Q+vQah5RvQh1fOuwj33MHjLboS0fYGSLkba/Q7z16mipVl4jAD3wXM7OK+t+AKZfN91pDaFZrWZoaMw9OJc6vnW4qflNTN4+2Xa3ezDjIG3C2tjaryve9duPwfbz8LOl+ebab5i8fTLb0vTaopfBi+71u2PRLHgZvFhwRO+sDvYOdtp8Ye/e9vfSvnZ7Fh5daAu2o1uOtr0EsEFAA25vfTt9ovpwdSO9VmAqNdkCwR1t7iDMN4zGweUPnNmfT/afqZ5/PXo16MXEbhN5ffPrACwb6dgZ++213/Li+hdJydNHpPh4+NjWBbit9W1sObulykDg5+nHf/r+h//0/Y9tXnUjnh7s8CCeRs8ql9ur2BdjZa2RRfhHkFOS4zCMEuDD/h/y6MpHbX1m4DwQ+Hn4VWpGtQ/I9udci9AWrD+tv0fr9ta3ExUYxZt/vMmd7e5k3qF5ZBRlMKblGA5kHrAFAmtwdVYDrFirjPCPcBi88HL3l3l146u2GyxnI4LsA5W1BucKbtM0ZN/hZM96N+lj1C8Oa+ek9X/QI/Y97e+psg28f1R/nuz8pG060CvQoZpoLXBAHw1kbeKxVzEIQPmJYX+CDIoedME26pp6teerPNv1WVqFtqo2nfUkt+bDenday7tWle39l8J+WxVfxGdlzWtyTjId63bEoAwOF9ye9D00DW5qG+FTXfOPdZnRYKRLRBemDZ7Gjc1utM0DPdjZ3437e/pzVcOrqmyO2T1uN7e1vo2Y8Bi231E+UsjXw9eWzwkxE7i9ze14GsoLS/uC01mNr6rj7O/pj1KKMa3GVPk5O0d05rNryp99sH5uXw9fdo/bza2tbyU6OLrK9ce2GVtpXrB3MPX969MtolulZdU1iVRk30xjzzpgId+Uj1E5PhXfPKQ5/aL6AXrtyT5P9lqFtmL+sPmV8xdQnj/7NvcWIeVDl+9qdxe3tr6VnWN3OrQItApt5fS3F5w1S1WsvVTsk7NOWwOBs2Yf++D2Z/sgquM2NQJrIKjrV5fUglQaBDRw6Oyy3iVZXUwbv5fRi3Ftx/HBtg8wW8yVmpLsv+AhTYY4RHnQ3xDqTFRgFMeyj11UG+3FCPUJ5bbWt10wna+HLy/Gv2j7Xdo+kX24ueXNtpFNl9MXA76g2Fzs0LFpLyowikCvQHJLcm2jjyoenx4NerDs+DKKSotsAd4Zbw89+Brs7oee6vIUDQMb0rN+ec3B/gGwInMRBmXgvtj7mHnA8SHEiqzBxCrEJ8T2NtjqVLX8g34fEOIT4jDc177mVp3o4Ghbk5GvsXJwjAyIdLIW/H7T7w4Fp71FIxZhUAZ6zuxpa3YFx0Awa8gskrOTGdx4MK9sfMWhs/eFbi9UeZ01DGrIy91fJqZ2DP9e9W9O5p6ke73ubDyzEX8PfwzKUOlYVixohzUdVqk5Dsr79cDxhq9z3fKf87U2/1nPQ2vNIzo4utLbhIc3G24bYGG1a+yuSkN4K96UWAd0WAMBQNeIrvxx9g/btLXsGNx4cKXPcTm5TSCwNg01Dm5MakEq7cPbM3/YfPrO6kuhudBWYNT1q8tjcY9d0oEP8AwgqzjLFghCvEMq3T0EeQU5FFwR/hFVXvgtQ1qyJmXN3+LXn0a3LB/aGOoTygvxrvlxm5qMtvrsms/YlrqNPpH6I/4VC4Ce9Xvi7eFNUWlRtTUCawFvf/yDvIK4p/09DumsQxC71+vO8OZ6v0nF7bYObc3z3Zy3mdurLgh80P8Dh1pCRdZmJGtH4jt93mFQ40G25f/X6/8cnn+oKMwnjLP5Zx0KPytrJ6xRGSnVSnm95+sMaDSgUrOKPWugiwqMYl/6PprVasbhrMMONbQ2YW1oE6a/3uLl7i8zMX6ibajtza2q7uSG8iD38VUfszplNXkleWw8s9F201bxWFr32y2iG5vPbqZd7crP1zhbz8r+Bq1iDT3AM4B8Uz4NAxvarsfHOz2On4ef09qYUqrSNmoSCD695lM2nt7o8FT8tju2VaoVXW5uEwhubnkzfSL72B4kifCLwNfDF18PXz0Q2J1cd8fcXd2mquTv6U9WcZbt4kkYlVApTZB3EH6efjwa9yg96/esNEzR3oSYCRSVFlUay+/uYsNjiQ2PtU1XbI8O8QnB26BfhL6e1QSCsvb7qmofVsOaDuPL3V/ybr93bRevfU1j062b8PPwc1rAvNP3HVteLqQmo4ig/EdgKnZaO3s+w16EfwR70/dSYC6otMza6R/gFcDSm5baPmdNjGoxilc3vsq7fd+lnn+9Kke2GJThgsfamaa1mtK0VlOmJ+kPtVV1YxTmG2b7LgrMBdV+htq+tW2DL+zFhsdWejAQ9JuPlSdXEuoTaqsR9Kjfg5ahVb+ZoGIrQ9+ovg7T1oBsHwi8jd6V+jqquzm4XNwmEAR4BdDCqwVDmw6lyFzEtdHXAuVR2lkbvTP/7ftfpz80A3qzzxe7vrB1QDm787LWBibETLjgvvw8/Xi6y9M1ypc7c3ahWI99dTUC6wV4obuthzo8xNg2Yx1qcvbNPtUVONYhgZdT9/rd2Xx2c7V3/85MjJ9IHb86dK/XvdIy691wv8h+FxUEQL9zv6rhVTVuTv2g3wdOg9GFWNvcK74Kw5417xf6DItHLLZ9/59d8xmn8/TO32+u/cahA9qqeUhzW8e2tSnRWZkR6Bloe9DPutzb6M2ykcsqPQNgDYoVzz9nz/e4mtsEAqteDXrRq0Ev27S1oKgYvasyMHpglcse7PAgo1uOduhcrMhZcBB/jv0dqLX913oRVtdHYL2wnb2Gwp7R4Py5gIc6POTQrvxXubPdndzQ9IYqnwyvSm3f2lU2X3kYPFg1etUl90ddTJ+atYnrYlkHK1T1HM3FsL/e7csDL6PXBa/R9/q9x+yDs2kYVDkQJ4xKsI2us+5jUPQgpw+CtQxpyc0tb+b2No7DfAO8JBD85S62RlCdiiNYxF/DPhB8M0gfYmftCK6q9gYQXy+eRSMWERUYVWWa6twfe/8lrfdnueo8c9ULzS4X61Dl6moEf4UmtZpU+dCmfZ+Kr4cvy0Yuq/K4Gg1Gp31tzp5bcjW3GT5aFWvUvtQfka8pV3f2uDPr6xKe6fKMbXjpG73eYFD0IFqEtqhu1UsOAuKvF+qr1zrsX6XydxfhH3HRbfzW4ebOhua6itvXCDqEd2DL2S1VPp5+uawYvcLlwcZd3R1zN4ezDjOkyRDbvKa1mvJO33euYK7E5RbkFcTPw36+rE+z/x0ppVhw44ILPrh4WfdpP0b6n6Bz585aYqLz39+9FKWWUvak73EYhSKEEP9rlFJbNU1z2qnl9k1DRoNRgoAQwq25fSAQQgh3J4FACCHcnAQCIYRwcy4NBEqpQUqpA0qpw0qpSgNvlVLvK6V2lP07qJTKcrYdIYQQruOy4aNKKSPwCTAASAG2KKUWaJpm+6FTTdP+bZf+X0BHV+VHCCGEc66sEXQFDmuadlTTtBJgJlDd29NuAWa4MD9CCCGccGUgaADY/+RRStm8SpRSjYDGwAoX5kcIIYQTrgwEzl76XdXTazcDczWt7G1NFTek1L1KqUSlVOK5c+ecJRFCCHGJXBkIUgD7F7lEAqerSHsz1TQLaZo2RdO0zpqmdQ4Pr/rNnkIIIS6eKwPBFqC5UqqxUsoLvbBfUDGRUqolEAJsdGFehBBCVMFlgUDTNDPwMLAUSAJma5q2Vyk1SSll/1NKtwAztX/aS4+EEOJ/hEvfPqpp2iJgUYV5L1WYfsWVeRBCCFE9ebJYCCHcnAQCIYRwcxIIhBDCzUkgEEIINyeBQAgh3JwEAiGEcHMSCIQQws1JIBBCCDcngUAIIdycBAIhhHBzEgiEEMLNSSAQQgg3J4FACCHcnAQCIYRwcxIIhBDCzUkgEEIINyeBQAgh3JwEAiGEcHMSCIQQws1JIBBCCDcngUAIIdycBAIhhHBzEgiEEMLNSSAQQgg3J4FACCHcnAQCIYRwcxIIhBDCzUkgEEIINyeBQAgh3JwEAiGEcHMSCIQQws1JIBBCCDcngUAIIdychys3rpQaBHwIGIGvNE17y0ma0cArgAbs1DTtVlfmSQjx92YymUhJSaGoqOhKZ+UfycfHh8jISDw9PWu8jssCgVLKCHwCDABSgC1KqQWapu2zS9MceA7oqWlaplKqjqvyI4T4Z0hJSSEwMJDo6GiUUlc6O/8omqaRnp5OSkoKjRs3rvF6rmwa6goc1jTtqKZpJcBMYFiFNPcAn2ialgmgaVqaC/MjhPgHKCoqIiwsTILAJVBKERYWdtG1KVcGggbASbvplLJ59loALZRS65VSm8qakipRSt2rlEpUSiWeO3fORdkVQvxdSBC4dJdy7FwZCJzlRqsw7QE0B/oBtwBfKaVqVVpJ06ZomtZZ07TO4eHhlz2jQgjhzlwZCFKAKLvpSOC0kzS/aJpm0jTtGHAAPTAIIYT4i7gyEGwBmiulGiulvICbgQUV0vwM9AdQStVGbyo66sI8CSHE34LZbL7SWbBxWSDQNM0MPAwsBZKA2Zqm7VVKTVJKDS1LthRIV0rtA1YCT2malu6qPAkhRE3ceOONdOrUibZt2zJlyhQAlixZQlxcHLGxsVx99dUA5OXlceeddxITE0P79u2ZN28eAAEBAbZtzZ07l/HjxwMwfvx4Hn/8cfr3788zzzzDH3/8QY8ePejYsSM9evTgwIEDAJSWlvLkk0/atvvxxx+zfPlyhg8fbtvusmXLGDFixGX5vC59jkDTtEXAogrzXrL7WwMeL/snhBAOXv11L/tO51zWbbapH8TLN7StNs0333xDaGgohYWFdOnShWHDhnHPPfewZs0aGjduTEZGBgCvvfYawcHB7N69G4DMzMwL7v/gwYMkJCRgNBrJyclhzZo1eHh4kJCQwPPPP8+8efOYMmUKx44dY/v27Xh4eJCRkUFISAgPPfQQ586dIzw8nG+//ZY777zzzx8QXBwIhBDin+ijjz5i/vz5AJw8eZIpU6bQp08f29j80NBQABISEpg5c6ZtvZCQkAtue9SoURiNRgCys7MZN24chw4dQimFyWSybff+++/Hw8PDYX933HEHP/zwA3feeScbN27ku+++uyyfVwKBEOJv60J37q6watUqEhIS2LhxI35+fvTr14/Y2Fhbs409TdOcDte0n1dxTL+/v7/t7xdffJH+/fszf/58kpOT6devX7XbvfPOO7nhhhvw8fFh1KhRtkDxZ8m7hoQQwk52djYhISH4+fmxf/9+Nm3aRHFxMatXr+bYsWMAtqahgQMHMnnyZNu61qahunXrkpSUhMVisdUsqtpXgwb641VTp061zR84cCCff/65rUPZur/69etTv359Xn/9dVu/w+UggUAIIewMGjQIs9lM+/btefHFF4mPjyc8PJwpU6YwYsQIYmNjGTNmDAATJ04kMzOTdu3aERsby8qVKwF46623GDJkCFdddRX16tWrcl9PP/00zz33HD179qS0tNQ2f8KECTRs2JD27dsTGxvL9OnTbctuu+02oqKiaNOmzWX7zErvr60mgVJ1gTeA+pqmDVZKtQG6a5r29WXLxUXo3LmzlpiYeCV2LYT4CyQlJdG6desrnY2/rYcffpiOHTty9913V5nG2TFUSm3VNK2zs/Q1qRFMRR/mWb9s+iDwWE0yLIQQ4vLp1KkTu3bt4vbbb7+s261JT0NtTdNmK6WeA/35AKVU6YVWEkIIcXlt3brVJdutSY0gXykVRtl7gpRS8UC2S3IjhBDiL1eTGsHj6K+GaKqUWg+EAyNdmishhBB/mQsGAk3Ttiml+gIt0d8oekDTNJPLcyaEEOIvccFAoJQaW2FWnFIKTdMuzyNtQgghrqia9BF0sfvXG/33hYdWt4IQQghHPXr0qHb5ddddR1ZW1l+UG0c1aRr6l/20UioY+N5lORJCiL+50tJS2/uCamrDhg3VLl+0aFG1y13pUp4sLkB+PEYI8T8qOTmZVq1aMW7cONq3b8/IkSMpKCggOjqaSZMm0atXL+bMmcORI0cYNGgQnTp1onfv3uzfvx+A1NRUhg8fTmxsLLGxsbYAYH019ZkzZ+jTpw8dOnSgXbt2rF27FoDo6GjOnz8PwHvvvUe7du1o164dH3zwgS1frVu35p577qFt27YMHDiQwsLCy/KZa9JH8CvlPzFpANoAsy/L3oUQojqLn4Wzuy/vNiNiYPBb1SY5cOAAX3/9NT179uSuu+7i008/BcDHx4d169YBcPXVV/P555/TvHlzNm/ezIMPPsiKFSt45JFH6Nu3L/Pnz6e0tJS8vDyHbU+fPp1rr72WF154gdLSUgoKChyWb926lW+//ZbNmzejaRrdunWjb9++hISEcOjQIWbMmMGXX37J6NGjmTdv3mV5uKwmw0fftfvbDBzXNC3lT+9ZCCH+pqKioujZsycAt99+Ox999BGA7R1DeXl5bNiwgVGjRtnWKS4uBmDFihW210MbjUaCg4Mdtt2lSxfuuusuTCYTN954Ix06dHBYvm7dOoYPH257S+mIESNYu3YtQ4cOpXHjxrb0nTp1Ijk5+bJ83pr0Eay+LHsSQoiLdYE7d1ep+Apo67S1cLZYLNSqVYsdO3Zc9Lb79OnDmjVr+O2337jjjjt46qmnGDu2fHBmde9/8/b2tv1tNBovW9NQlX0ESqlcpVSOk3+5SqnL+5NBQgjxN3LixAk2btwIwIwZM+jVq5fD8qCgIBo3bsycOXMAvfDeuXMnoDcZffbZZ4DeqZyT41hcHj9+nDp16nDPPfdw9913s23bNoflffr04eeff6agoID8/Hzmz59P7969XfI5raoMBJqmBWqaFuTkX6CmaUEuzZUQQlxBrVu3Ztq0abRv356MjAweeOCBSml+/PFHvv76a2JjY2nbti2//PILAB9++CErV64kJiaGTp06sXfvXof1Vq1aRYcOHejYsSPz5s3j0UcfdVgeFxfH+PHj6dq1K926dWPChAl07NjRdR+WGryG2pZQqTqAj3Va07QTrspUdeQ11EL8b7vSr6FOTk5myJAh7Nmz54rl4c+67K+hVkoNVUodAo4Bq4FkYPGfz6oQQoi/g5o8R/AaEA8c1DStMXA1sN6luRJCiCskOjr6H10buBQ1CQQmTdPSAYNSyqBp2kqgw4VWEkII8c9Qk+cIspRSAcBa4EelVBr68wRCCCH+B1Q3fHSyUqonMAz9tRKPAUuAI8ANf032hBBCuFp1NYJD6E8V1wNmATM0TZv2l+RKCCHEX6a65wg+1DStO9AXyAC+VUolKaVeVEq1+MtyKIQQ/3DJycm0a9cO0J8jGDJkyBXOkaMLdhZrmnZc07S3NU3rCNwKjACSXJ4zIYS4wjRNw2KxXOlsuFxNniPwVErdoJT6Ef35gYPATS7PmRBCXAHW1z0/+OCDxMXF8f3339O9e3fi4uIYNWqU7W2iW7ZsoUePHsTGxtK1a1dyc3NJTk6md+/exMXFERcXd8HfIPi7qLKPQCk1ALgFuB74A5gJ3KtpWv5flDchhJt7+4+32Z+x/7Jus1VoK57p+ky1aQ4cOMC3337LpEmTGDFiBAkJCfj7+/P222/z3nvv8eyzzzJmzBhmzZpFly5dyMnJwdfXlzp16rBs2TJ8fHw4dOgQt9xyC/+ENyFU11n8PDAdeFLTtIy/KD9CCHHFNWrUiPj4eBYuXMi+fftsr6QuKSmhe/fuHDhwgHr16tGlSxdAfwkdQH5+Pg8//DA7duzAaDRy8ODBK/YZLkaVgUDTtP5/ZUaEEKKiC925u4r1ddOapjFgwABmzJjhsHzXrl2VXlUN8P7771O3bl127tyJxWLBx8enUpq/o0v5qcoaU0oNUkodUEodVko962T5eKXUOaXUjrJ/E1yZHyGEuBjx8fGsX7+ew4cPA1BQUMDBgwdp1aoVp0+fZsuWLQDk5uZiNpvJzs6mXr16GAwGvv/+e0pLS69k9mvMZYFAKWUEPgEGo/+85S1KqTZOks7SNK1D2b+vXJUfIYS4WOHh4UydOpVbbrmF9u3bEx8fz/79+/Hy8mLWrFn861//IjY2lgEDBlBUVMSDDz7ItGnTiI+P5+DBg7aaxd9djV9DfdEbVqo78IqmadeWTT8HoGnam3ZpxgOdNU17uKbblddQC/G/7Uq/hvp/wWV/DfWf0AA4aTedUjavopuUUruUUnOVUlHONqSUulcplaiUSjx37pwr8iqEEG7LlYGgck8KVKx+/ApEa5rWHkgAnL7CQtO0KZqmddY0rXN4ePhlzqYQQrg3VwaCFMD+Dj8SOG2fQNO0dE3TissmvwQ6uTA/Qoh/CFc1WbuDSzl2rgwEW4DmSqnGSikv4GZggX0CpVQ9u8mhyKsrhHB7Pj4+pKenSzC4BJqmkZ6eftHDVmvyewSXRNM0s1LqYWApYAS+0TRtr1JqEpCoadoC4BGl1FD03zfIAMa7Kj9CiH+GyMhIUlJSkP7AS+Pj40NkZORFreOyUUOuIqOGhBDi4l2pUUNCCCH+ASQQCCGEm5NAIIQQbk4CgRBCuDkJBEII4eYkEAghhJuTQCCEEG5OAoEQQrg5CQRCCOHmJBAIIYSbk0AghBBuTgKBEEK4OQkEQgjh5iQQCCGEm5NAIIQQbk4CgRBCuDkJBEII4eYkEAghhJuTQCCEEG5OAoEQQrg5CQRCCOHmJBAIIYSbk0AghBBuTgKBEEK4OQkEQgjh5iQQCCGEm5NAIIQQbk4CgRBCuDkJBEII4eYkEAghhJuTQCCEEG5OAoEQQrg5lwYCpdQgpdQBpdRhpdSz1aQbqZTSlFKdXZkfIYQQlbksECiljMAnwGCgDXCLUqqNk3SBwCPAZlflRQghRNVcWSPoChzWNO2opmklwExgmJN0rwH/AYpcmBchhBBVcGUgaACctJtOKZtno5TqCERpmrawug0ppe5VSiUqpRLPnTt3+XMqhBBuzJWBQDmZp9kWKmUA3geeuNCGNE2bomlaZ03TOoeHh1/GLAohhHBlIEgBouymI4HTdtOBQDtglVIqGYgHFkiHsRBC/LVcGQi2AM2VUo2VUl7AzcAC60JN07I1TautaVq0pmnRwCZgqKZpiS7MkxBCiApcFgg0TTMDDwNLgSRgtqZpe5VSk5RSQ121XyGEEBfHw5Ub1zRtEbCowryXqkjbz5V5EUII4Zw8WSyEEG5OAoEQQrg5CQRCCOHmJBAIIYSbk0AghBBuTgKBEEK4OQkEQgjh5iQQCCGEm5NAIIQQbk4CgRBCuDkJBEII4eYkEAghhJuTQCCEEG5OAoEQQrg5CQRCCOHmJBAIIYSbk0AghBBuTgKBEEK4OQkEQgjh5iQQCCGEm5NAIIQQbk4CgRBCuDkJBEII4eYkEAgBoGlXOgeusXc+ZKdc6VyIvzkJBKJqmgYJr8Duua7dT9YJyDlds7R7f4YDS2qWVtNg1xwwFVafbt378E4zKMio2XadyTkNM2+D/PSar3N2D5hLLn2foH/G7BSwWGDOnZD4bfmyomyYMx6mDqm8XkFG5eNiLoZja/9cfi6UV4vl8m3PUlo5gJea9fPJ1UpNleelbIW8NNfv2wXcJxCYiuDkH67dx8kt+kVXkq9P/zASpt1QSYeDzQAAE7JJREFU/To1KQhMhXBiU83ycGYXFOdVv9xUCMfWVL4of3sSZt1RPr3tO72QXPJc+byzu2HzFNj+g/O76IyjNcunvQ9i4L3WNUs7ZxzMGFOztAcWw08TYM275fM2fgKHEsqn89P1YFdwHk5thXUfwPGNeiGTm1qeLucMHE7QC5qKn3vd+3r+9y+EbdP0ebvnwi8P69uxKsjQg8WZnfp+P+8JM26GzV/o8yt+H4eW6fNLCpx/viMr4dVa8H5bWP4q7P0JFj6mL9M0/bMCZB6DzGT9GjiwRC/w/9NYPz/tLXgEpg2B9CP6v1l36EEmJdH5/p0dC3umQljzDhTl6Mdy+SR4M1I/n0xFVa9XnVITpCXpx3VSqH59HVmpX3ugH88PYhzPw+yUC9f4rN9TYaZ+jlvTL38NfntCP0d+fVTfz5av4PW6euA9vlFPl3EMvroK3m0O23/UvzNnwcLpZzJDwqswe6zj/LN79Ov0zC592lxcs+1dAg+XbfnvZt17+kn51BHwC71welMRHF+vnxiLnoT71uqFRf2OVa+z5Bm9MDm6CqK6weFl+vz8dPAPq5w+/Qh8HAejv4c2Qysv1zT9hN6/EJa9BF3vhdotoPPd+snYsBuENtELqXNJ0LgvfNEbml4Fo6aCT7BeeFks0GIgnN4BU/qWb3/4FIgdoxda+36BpAX6/OMboGF32Fp2d+nlrxeqhVl6QWMuu4iDI6FJP/3vwky9wPhxJPR4RM/3Na/AxsnQ7BpoXRYQDyfohUKLQbD0OWht97nNJeDh5fzYVryDtR5TSykYjPq0qQBqRenHTSk4va388yx9AbrdB0uf1+c9exISXobd88q3mbRAD36gf38nN8OL5/XprwdCdtmdZnBDGPkN/D5R39/uOeXbWP4qBDXQ91NwHuq0ga73gNFT/w73LwRlgMjOevojy/V/oBeSsWNgwCT9puXHsoJ6/QfQ/3nYOlUPDnVaQ8N4+OGm8v2u/8DxWJ3dBavfLp/3eR/9O9jxA/jXKTsu62DPT3r+AyNg10x9/uJnIOs4nD9Yfk6M/Fb/Hn2C9ON7fD1Mv1k/FzrcAkM/1m9Afn1UP3bd7oUNk2Hl63r6te+Buez72/QppO3Tz9seD+sFrzLCyU1w09cQ1tTunCjWC8kud+vzf7pHb+66+iV9efJa/R/o34n1mlvzLsQ/oAed5LX6dzJiiv5Ze/0b/r+9Mw+Pqjob+O8NhF3WgCAQAgYVUIgQVhERwSKiRUURcflo+KyKItYqav0suENtXT6pnwvYpy2oVfSBKi5h0yJViGIUqiFhKUWiEFkiiCyZ8/3xnnGGySQkkiFm5v09zzxz7rln7j3vveecdzl37mmWFjrHge/gwTbaXr/+F3z2N73HPa+Cf4QZEQD/WgD7vOe49lX9DLodPngqVGb+DfoBaH4i3PSRXtPcF6Dn1Xp/k2rp/pznQ8ob9Foc10br+dYdofw6jeDAHr3OPSMURhUgrobFRjMzM11OThkWSnlsyYHnzoGLn4NTL9bOKKL7AgFNlxzUhn/iEHh+hA6ukfS8RjvJWXdAsw4w51Jo0ALa9YYNS1URtO8LSbW1swBkXAmtT4PeE6BWbdj2BRRvUSti4a+hXR/IekcbQcEibagAcy+D/HegbmPYXxyqw89nwvyJpet2wRPw90maliTIGAer/+LrME4H583/DJU/7VJVLrOGHX6cFunQYywsuQ+SG8LBvdGvadNU6PNLWPEE7Pk6epkgIx+FtEHwZK+yywx/GJY9BFnZsD1Pr2Hd43SQK1iknWrHei077F5o1Bre+JUqoy8/hm+36jEWTQMXgJIIC6pWHSjxHtgJPUOKolkaJCXDN/ml61S3sX6KKxFnT0rWgf9gmCXfewKsmacKMxoNWqiRUbCo9L72fXWwmTNalfKBMjy+jCt1oE8doO0wUn5Q4+D73RWXJZykZDjlfPi2UJVkOMMf1vb7yRzdvuEDmDtG+0o4Pa+BLatUEUTjlJEQOATJDWDUH+H9x7VNtEhXZfT0meXXsW4T6DYq5JmFU7+5DuK168MpI7R9te+r92TJfRW7BkHO/4P2/3Bl3GGgKtdITh0Na15RI65onRpZrbqq8fWdNzRSToaiPOg4SLc3vqeD/8HvtC0HyVoE7XtXrq4eEfnIOZcZdV/CKIJAAH5/EjTrCFt8iOjkEdq4Xp2g1m7r7vDBTL1J2/PAlZR/zLKoVVc79sBb1MpZ463OK+dB+37qlpdEhISSGxw+cERuV4Q2PTTsUB7pwwCnnW3DssP3Ne8EQ6fB33x4qNNg7ZgLf63bSckQqKC7O/gueG+Gnqcsgh3ziIjWbcd66HS2WlPRBsyySG4IJ56t1ng4LbvAhGy1WLPvUQ/ouBNUoZRH7//W8F9hLmxbq3mdzlZl++1WPRbA6VepJxXs7AC9/gv2Fmldwq/nXYVQpwE8Mxi2roYh/6PGw8pnQ1YuwLhXQp4CQOYvIGe2pq9dpr8PZ8JitaSnp/ntJaokFk9ThSJJoYFm9Gx45ReabpIa8oDGvhQ9HJeZBTmzDs9rkQ7fFEDamWqJn3QerHszdMzxb0DB4sOt4LKo3xy+33X4QBjJLWvVcg4EIO8NGD5dDb1HOh9eLnUAbF5x5HMCnDFZB+mdG0N5zdLgktka/gGYtFrHkmUP6ZxElwu0v3wyN9RfwunzSzhvuhouwfsVXrfxC2FeVmisaNIerlsOtevCA61DZX+7K2TAVhJTBEFyZsPrt1SsbPpQdcMKPw11gmDj/tmDoRBDJFe8DJ2HhW7WoQN63remVGyQCXLqaLUg9nytnW3oVPj8dfgyQvZzH9BG+tK4UB3H/DUUzml9qoYK9n+rFnC/GyC5vrrAT/UPHSdrEbTtqYPswtth5dMaXkpuCHMvhba99LfzstTNLv4y9Nvrlmu4YfWfYcn9mvebr1XZ7S9Wtzh8ED7pPDjrdq33jI6ad8rI0gN1r/FapssFOpjt2a5hve1fwFMDtMywe0MDL6hFddV87aDvzYDrV6jnsG8HPJmpirjTWRo2Oe1SuOQ5/d2nL6tB0OUCaNxOY+rBAazHFdrZL3gM9m6HDv7c7z+u5758rhoVwXu+4klVKmNfgpR0DfMEva4p/4b6TVWRBGP1AFO9lb73G7UaO/h78+YU+PD/1CsZ+lsdfL/6TD3XkoPQ/0Z4JF3L3r0d/rdXaAAPP+6bUzQMdOsXeo8DAdi4TMOJkqQhnuT6MLVJ6Hc7Nuo5Wp6k9a3TEHJfhAYpkH6Oyru3CN6doUbThndhxAz4+2T1BOo0UnmL1qnXOPIxSK6nx5o/UZXn0geh+2XQ9zoNta19FW76WAfixdNC1z93rqbP/73G7AFOPAeuelXTgRJVoG17ab0+fBqK8tUga9JWw6I5s1SJdzwT2mSoB7hqFuQt9H0gW9ts2kDd/mN/9Vwmf6beL4SuT3kDcrBMq67Q8mRom6l9JykJigpgZh8Y9ZSGFZ8/T/v66Fna5mb21Xtx7v0w4CY9zuYPAFFZUtKjn7MCmCIIcugA3N+ydH6HgdAwBb54I2ShXfwcdL9U08Ebe8dmjd31Gq/ucf1mav2snqPxxB0bo8f6Ae5rVdpV73s99Lhcwx+FufDKeM0fcjcMuk3TgYAOYg1TQvWv1xS6j9HB+u7tGld/KBX274YLn9S6VIT8bFUQ3S46vFEf/F4H5W4X6UDw/uMa863XVC3xjmdpDLponcZ2e4RZi8WFOli26R7KCwQ0Zpr7gsbm6zUO7dv4Dw1VNDoeZg3VvNT+GsIKDprReP9xLde+j05yr3pW5zy6jVIrKhBQiy483rxjg1pazsHSByDjCu2ooHHit+/U696knZaZ5s89tYxQSqBEQ1IVcdXz3lRrOSXCUt32uQ6YTdtH/92y6bDsQR3wf/ZA9DLhg/e3X2msfttaVc5BheKcWtbB2HRZbFquIZPUfkeWqSwWTNLQzAmnq5dSUQ58p15Ko1b69M0jnbXN3Zqn/a1ZmrbTd2fonEZlYuWrZqk1Hi20sus/2gZbn3p4/ve7daL5+G6hvE3L9Qmx7peVfa7NH2hbH/lYdGVxaL+2Ued0rq/rKGjkx6VDB9Sg6nGFeohViCmCcB49TS2moVN18vGjP8HkNdpB9u1USz8/G24rCHWavDehdj0NL/xY7mup1sa4eZD/Ngy+s/SkdbCBlEf+Ih28Gp+goaO6x2n++qUaOjn96rInXKuTQ/vVemzSNvr+QEAt2wGTIHO8TqS37Vnx4+/bqYq5KslfpEqh1SlVe9zK8N7v1Msadh+cMSl6md1bVGE373hs61YWhbnw9CAdzC566sjly2LtayGr+mg5uE/n7NKHHv2xaijVpghEZDjwOFALeM4593DE/uuAiUAJsAe41jlXxiySctSKYN9O1fTBpwaCT5gECQQ0rl3Vg+mGd9WSyBhbtcc14pvvizWMdfZdGpqpKaxfohPyZXl0xjGnWhSBiNQC1gHDgC3AKmBs+EAvIo2dc8U+fSFwg3NueHnHPWpFYBiGkYCUpwhi+YeyPkCBc26Dc+4A8CLw8/ACQSXgaQjUrDiVYRhGHBDLP5S1Bf4Ttr0F6BtZSEQmAr8C6gBDoh1IRK4FrgVITU2t8ooahmEkMrH0CKI9W1XK4nfOzXTOnQhMAe6OdiDn3DPOuUznXGbLllGe+jEMwzB+NLFUBFuA8Gfi2gHlPUT/IjAqhvUxDMMwohBLRbAK6CwiHUWkDnA5sCC8gIiEP1R9PhDlP/6GYRhGLInZHIFz7pCI3Ai8jT4+Ots5t1ZE7gVynHMLgBtFZChwENgJXBOr+hiGYRjRienbR51zC4GFEXn3hKVvjuX5DcMwjCOTOOsRGIZhGFGpca+YEJHtwL+PWDA6KUDREUvFFyZzYmAyJwZHI3MH51zUxy5rnCI4GkQkp6x/1sUrJnNiYDInBrGS2UJDhmEYCY4pAsMwjAQn0RTBM9VdgWrAZE4MTObEICYyJ9QcgWEYhlGaRPMIDMMwjAhMERiGYSQ4CaEIRGS4iOSJSIGI3FHd9akqRGS2iGwTkTVhec1FJFtE8v13M58vIvKEvwafikgl1oH86SAi7UVkqYh8LiJrReRmnx+3cotIPRFZKSK5XuZpPr+jiHzoZX7Jv9MLEanrtwv8/rTqrP/RICK1RGS1iLzutxNB5k0i8pmIfCIiOT4vpu077hWBXyltJnAe0BUYKyJdq7dWVcafgMgV3e4AFjvnOgOL/Tao/J3951rgKBaTrVYOAbc657oA/YCJ/n7Gs9z7gSHOuR5ABjBcRPoB04FHvcw7gSxfPgvY6ZxLBx715WoqNwOfh20ngswAZzvnMsL+MxDb9u2ci+sP0B94O2z7TuDO6q5XFcqXBqwJ284D2vh0GyDPp59GlwotVa4mf4D56HKoCSE30AD4GF3kqQio7fN/aOfoix77+3RtX06qu+4/QtZ2ftAbAryOrnES1zL7+m8CUiLyYtq+494jIPpKaW2rqS7HguOdc4UA/ruVz4+76+Dd/9OBD4lzuX2I5BNgG5ANrAd2OecO+SLhcv0gs9+/G2hxbGtcJTwG3A4E/HYL4l9m0AW83hGRj/zqjBDj9h3Tt4/+RKjQSmkJQFxdBxFpBMwDJjvnikWiiadFo+TVOLmdcyVAhog0BV4DukQr5r9rvMwiMhLY5pz7SEQGB7OjFI0bmcM4wzm3VURaAdki8kU5ZatE7kTwCCq7UlpN52sRaQPgv7f5/Li5DiKSjCqBOc65V3123MsN4JzbBSxD50eaikjQmAuX6weZ/f4mwI5jW9Oj5gzgQhHZhK5eOAT1EOJZZgCcc1v99zZU6fchxu07ERTBEVdKizMWEFrg5xo0hh7Mv9o/ZdAP2B10NWsSoqb/LOBz59wfwnbFrdwi0tJ7AohIfWAoOoG6FBjti0XKHLwWo4ElzgeQawrOuTudc+2cc2lon13inBtHHMsMICINReS4YBo4F1hDrNt3dU+MHKPJlxHAOjSu+pvqrk8VyvUCUIiu8LYFfXKiBTrBlu+/m/uygj49tR74DMis7vr/SJkHoq7vp8An/jMinuUGugOrvcxrgHt8fidgJVAAvAzU9fn1/HaB39+pumU4SvkHA68ngsxevlz/WRscr2Ldvu0VE4ZhGAlOIoSGDMMwjHIwRWAYhpHgmCIwDMNIcEwRGIZhJDimCAzDMBIcUwSGEYGIlPg3PwY/VfbGWhFJk7C3xRrGT4FEeMWEYVSWfc65jOquhGEcK8wjMIwK4t8TP92vDbBSRNJ9fgcRWezfB79YRFJ9/vEi8ppfRyBXRAb4Q9USkWf92gLv+H8LG0a1YYrAMEpTPyI0NCZsX7Fzrg/wJPruG3z6z8657sAc4Amf/wTwrtN1BHqi/xQFfXf8TOdcN2AXcEmM5TGMcrF/FhtGBCKyxznXKEr+JnSBmA3+xXdfOedaiEgR+g74gz6/0DmXIiLbgXbOuf1hx0gDsp0uMIKITAGSnXP3x14yw4iOeQSGUTlcGemyykRjf1i6BJurM6oZUwSGUTnGhH3/06dXoG/IBBgHLPfpxcD18MPCMo2PVSUNozKYJWIYpanvVwML8pZzLvgIaV0R+RA1osb6vEnAbBG5DdgOjPf5NwPPiEgWavlfj74t1jB+UtgcgWFUED9HkOmcK6ruuhhGVWKhIcMwjATHPALDMIwExzwCwzCMBMcUgWEYRoJjisAwDCPBMUVgGIaR4JgiMAzDSHD+H6pHzU6gnbGOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_metrics_to_plot = ['accuracy', 'precision', 'recall'] \n",
    "\n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
