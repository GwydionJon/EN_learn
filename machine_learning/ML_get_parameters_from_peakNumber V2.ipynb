{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import kerastuner as kt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input with all maxima\n",
    "df_spectra_all=pd.read_csv(\"spectrum_energy_input_numberOfPeaks.csv\",index_col=[0])\n",
    "x_data_string=df_spectra_all[\"all_maxima\"].values\n",
    "\n",
    "max_nr_of_max=df_spectra_all['no_of_max'].max()\n",
    "len_array=len(df_spectra_all[\"all_maxima\"].values)\n",
    "\n",
    "print(max_nr_of_max,len_array)\n",
    "x_data_padded=np.zeros((len_array,max_nr_of_max))\n",
    "\n",
    "for i,x_string in enumerate(x_data_string):\n",
    "    x_string=x_string.replace('\\n','').replace('[','').replace(']','')\n",
    "    x_split= x_string.split(' ')\n",
    "    j=0\n",
    "    for x in (x_split):\n",
    "        if(x!=''):\n",
    "            #print(x)\n",
    "            x_data_padded[i,j]=float(x)\n",
    "            j=j+1\n",
    "    #print(\"cut\")\n",
    "\n",
    "y_data=df_spectra_all[[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"]].to_numpy()\n",
    "\n",
    "\n",
    "y_data=y_data-0.5\n",
    "y_data=y_data*4\n",
    "\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(x_data_padded, y_data, test_size=0.20, random_state=42)\n",
    "   \n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def transform_data(df,index):\n",
    "    data=df[index].to_numpy()\n",
    "    new_data=np.zeros((len(data),max(data)+1))    \n",
    "    for i in range(len(data)):\n",
    "        new_data[i,data[i]]=1\n",
    "    return new_data\n",
    "    \n",
    "df_spectra=pd.read_csv(\"spectrum_energy_input_numberOfPeaks.csv\",index_col=[0])\n",
    "df_new=df_spectra.loc[df_spectra[\"k11\"] == 0]\n",
    "\n",
    "\n",
    "x_data=transform_data(df_new,\"no_of_max\")\n",
    "print(x_data[0:2])\n",
    "\n",
    "\n",
    "\n",
    "y_data=df_spectra[[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"]].to_numpy()\n",
    "\n",
    "\n",
    "y_data=y_data-0.5\n",
    "y_data=y_data*4\n",
    "print(y_data[0:10])\n",
    "\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(x_data, y_data, test_size=0.20, random_state=42)\n",
    "\n",
    "print(data_train.shape)\n",
    "print(labels_train.shape)\n",
    "\n",
    "#df_training = df_spectra.sample(frac=0.8,random_state=10)\n",
    "#df_testing=df_spectra.drop(df_training.index)\n",
    "#df_spectra.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns_A = []\n",
    "\n",
    "no_of_max = tf.feature_column.numeric_column(\"all_maxima\")\n",
    "my_feature_layer_A = tf.keras.layers.DenseFeatures(no_of_max)\n",
    "\n",
    "#transform data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_the_loss_curve(epochs, mse,val_mse):\n",
    "    \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "    plt.plot(epochs, mse, label=\"Loss\")\n",
    "    plt.plot(epochs, val_mse, label=\"Val Loss\")\n",
    "\n",
    "    plt.legend()\n",
    "    #plt.ylim([0, 15])\n",
    "    plt.show()  \n",
    "\n",
    "def create_model(my_learning_rate, my_feature_layer):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    # Add the layer containing the feature columns to the model.\n",
    "    # Define the first hidden layer with 10 nodes.   \n",
    "   # model.add(my_feature_layer_A)\n",
    "    #layers=[5,10]\n",
    "    layers=[600,600,600,600]\n",
    "    for layer in layers:\n",
    "        model.add(tf.keras.layers.Dense(units = layer, activation = 'sigmoid',kernel_regularizer=tf.keras.regularizers.l1(0.0)))\n",
    "    # Define the output layer.\n",
    "    model.add(tf.keras.layers.Dense(units=6,  \n",
    "                                    name='Output',activation=\"sigmoid\"))#,kernel_regularizer=tf.keras.regularizers.l2(0.04)))                              \n",
    "  \n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(lr=my_learning_rate,momentum=0.1),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, x,y, epochs, label_name,\n",
    "                batch_size=None):\n",
    "\n",
    "    #features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    #label=dataset[label_name].to_numpy()\n",
    "    history = model.fit(x=x, y=y, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True, verbose=1,validation_split=0.2) \n",
    "    \n",
    "    epochs = history.epoch\n",
    "  \n",
    "    df_hist = pd.DataFrame(history.history)\n",
    "    #hist.head()\n",
    "    #mse = hist[\"mean_squared_error\"]\n",
    "    mae = df_hist[\"mean_squared_error\"].to_numpy()\n",
    "    val_mae = df_hist[\"val_mean_squared_error\"].to_numpy()\n",
    "    return epochs,mae,val_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "#data_train, data_test, labels_train, labels_test\n",
    "learning_rate = 1e-1\n",
    "epochs = 500\n",
    "batch_size = 400\n",
    "\n",
    "label_name = [\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"]\n",
    "\n",
    "my_model = create_model(learning_rate, my_feature_layer_A)\n",
    "\n",
    "epochs, mse,val_mse = train_model(my_model, data_train,labels_train, epochs, \n",
    "                          label_name, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = {name:np.array(value) for name, value in df_test.items()}\n",
    "#label=df_test[label_name].to_numpy()\n",
    "#data_train, data_test, labels_train, labels_test\n",
    "plot_the_loss_curve(epochs, mse,val_mse)\n",
    "evaluation=my_model.evaluate(x = data_test, y = labels_test, batch_size=batch_size)\n",
    "predicted = my_model.predict(data_test)\n",
    "print(evaluation)\n",
    "\n",
    "\n",
    "\n",
    "df_test=pd.DataFrame(labels_test,columns=[\"k6a1_test\",\"k6a2_test\",\"k11_test\",\"k12_test\",\"k9a1_test\",\"k9a2_test\"])\n",
    "df_predict=pd.DataFrame(predicted,columns=[\"k6a1_hat\",\"k6a2_hat\",\"k11_hat\",\"k12_hat\",\"k9a1_hat\",\"k9a2_hat\"])\n",
    "pd.concat([df_test,df_predict], axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_test[0:5])\n",
    "print(labels_test[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new try with 2d Data:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import kerastuner as kt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def get_array_from_strings(data_string, padded_array):\n",
    "    use_paddad_array=padded_array.copy()\n",
    "    for i,x_string in enumerate(data_string):\n",
    "        x_string=x_string.replace('\\n','').replace('[','').replace(']','')\n",
    "        x_split= x_string.split(' ')\n",
    "        j=0\n",
    "        for x in (x_split):\n",
    "            if(x!=''):\n",
    "                #print(x)\n",
    "                use_paddad_array[i,j]=float(x)\n",
    "                j=j+1\n",
    "\n",
    "    return use_paddad_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(583, 22, 2)\n",
      "(583, 6)\n"
     ]
    }
   ],
   "source": [
    "#input with all maxima\n",
    "df_spectra_all=pd.read_csv(\"spectrum_energy_input_numberOfPeaks_with_intensity.csv\",index_col=[0])\n",
    "x_Energy_string=df_spectra_all[\"all_maxima\"].values\n",
    "x_Intensity_string=df_spectra_all[\"Intensity\"].values\n",
    "\n",
    "\n",
    "\n",
    "max_nr_of_max=df_spectra_all['no_of_max'].max()\n",
    "len_array=len(df_spectra_all[\"all_maxima\"].values)\n",
    "\n",
    "#print(max_nr_of_max,len_array)\n",
    "x_data_padded=np.zeros((len_array,max_nr_of_max))\n",
    "\n",
    "\n",
    "x_energy_padded=get_array_from_strings(x_Energy_string,x_data_padded)\n",
    "x_Intensity_padded=get_array_from_strings(x_Intensity_string,x_data_padded)\n",
    "\n",
    "#print(x_data_padded[:5])    \n",
    "#print(x_energy_padded[:5])    \n",
    "#print(x_Intensity_padded[:5])    \n",
    "\n",
    "#make 2d Array from Energy and Intensity\n",
    "x_data_2d =np.zeros((len(x_energy_padded),len(x_energy_padded[0]),2))\n",
    "for i in range(len(x_energy_padded)):\n",
    "    for j in range(len(x_energy_padded[i])):\n",
    "        x_data_2d[i,j,0]=x_energy_padded[i,j]\n",
    "        x_data_2d[i,j,1]=x_Intensity_padded[i,j]\n",
    "\n",
    "#print(x_data_2d[:5]) \n",
    "\n",
    "#convert y_data into 2d array where -1=[1,0] 0=[0,0] 1=[0,1]\n",
    "y_data=df_spectra_all[[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"]].to_numpy()\n",
    "y_data=y_data-0.5\n",
    "y_data=y_data*4\n",
    "#y_data=y_data+2\n",
    "y_data_2d =np.zeros((len(y_data),len(y_data[0]),2))\n",
    "#print(y_data_2d.shape)\n",
    "for i in range(len(y_data)):\n",
    "    for j in range(len(y_data[i])):\n",
    "        if(y_data[i,j]==-1):\n",
    "            y_data_2d[i,j,0]=1\n",
    "        elif(y_data[i,j]==1):\n",
    "            y_data_2d[i,j,1]=1\n",
    "            \n",
    "#print(y_data[:5])\n",
    "#print(y_data_2d[:5])\n",
    "\n",
    "\n",
    "#y_data_cat=to_categorical(y_data)\n",
    "#print(y_data_cat.shape)\n",
    "y_data_2d=y_data_2d.reshape(y_data_2d.shape[0],y_data_2d.shape[2]*y_data_2d.shape[1])\n",
    "#print(y_data_cat.shape)\n",
    "\n",
    "#this is with 1d label\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(x_data_2d, y_data, test_size=0.20, random_state=43)\n",
    "#this is with 2d label\n",
    "#data_train, data_test, labels_train, labels_test = train_test_split(x_data_2d, y_data_2d, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "#maybe reshape data into [nr,x,y,channel=1?]   \n",
    "print(data_train.shape)\n",
    "print(labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_the_loss_curve(epochs, mse,val_mse):\n",
    "    \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "    plt.plot(epochs, mse, label=\"Loss\")\n",
    "    plt.plot(epochs, val_mse, label=\"Val Loss\")\n",
    "\n",
    "    plt.legend()\n",
    "    #plt.ylim([0, 15])\n",
    "    plt.show()  \n",
    "\n",
    "def create_2d_model(my_learning_rate, input_shape):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    # Add the layer containing the feature columns to the model.\n",
    "    # Define the first hidden layer with 10 nodes.   \n",
    "    \n",
    "    model.add(tf.keras.layers.Conv1D(64, 2, activation='relu',input_shape=input_shape))\n",
    "    #model.add(tf.keras.layers.Conv1D(128, 4, activation='linear'))\n",
    "   # model.add(tf.keras.layers.MaxPooling1D(2,2))\n",
    "\n",
    "   #print model.add(tf.keras.layers.Conv1D(256, 4, activation='exponential'))\n",
    "    #model.add(tf.keras.layers.MaxPooling1D(128))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units = 128, activation = 'softmax',kernel_regularizer=tf.keras.regularizers.l1(0)))\n",
    "   # model.add(tf.keras.layers.Dropout(0.5))\n",
    "   # model.add(tf.keras.layers.Dense(units = 6, activation = 'relu',kernel_regularizer=tf.keras.regularizers.l1(0)))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units = 6, activation = 'softmax',kernel_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "    #model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(lr=my_learning_rate,momentum=0.1),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, x,y, epochs,\n",
    "                batch_size=None):\n",
    "\n",
    "    #features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    #label=dataset[label_name].to_numpy()\n",
    "    history = model.fit(x=x, y=y, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True, verbose=1,validation_split=0.1) \n",
    "    \n",
    "    epochs = history.epoch\n",
    "  \n",
    "    df_hist = pd.DataFrame(history.history)\n",
    "    #hist.head()\n",
    "    #mse = hist[\"mean_squared_error\"]\n",
    "    mae = df_hist[\"mean_squared_error\"].to_numpy()\n",
    "    val_mae = df_hist[\"val_mean_squared_error\"].to_numpy()\n",
    "    return epochs,mae,val_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 2)\n"
     ]
    }
   ],
   "source": [
    "#data_train,data_test,labels_train,labels_test\n",
    "input_shape=(data_train.shape[1:3])\n",
    "print(input_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1.  1.  1.  1.  0.]\n",
      " [ 0.  1.  1. -1.  0.  1.]\n",
      " [-1.  0.  1. -1.  1.  0.]\n",
      " [ 0.  0. -1. -1.  1. -1.]\n",
      " [ 1.  0.  1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "two_d_model=create_2d_model(0.01,(22,2))\n",
    "print(labels_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_164 (Conv1D)          (None, 21, 64)            320       \n",
      "_________________________________________________________________\n",
      "flatten_46 (Flatten)         (None, 1344)              0         \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 128)               172160    \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 173,254\n",
      "Trainable params: 173,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "two_d_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5630 - mean_squared_error: 0.73 - 0s 5ms/step - loss: 1.5159 - mean_squared_error: 0.6959 - val_loss: 1.5310 - val_mean_squared_error: 0.7190\n",
      "Epoch 2/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5238 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 1.5015 - mean_squared_error: 0.6959 - val_loss: 1.5167 - val_mean_squared_error: 0.7190\n",
      "Epoch 3/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5128 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 1.4873 - mean_squared_error: 0.6959 - val_loss: 1.5026 - val_mean_squared_error: 0.7190\n",
      "Epoch 4/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.5408 - mean_squared_error: 0.75 - 0s 2ms/step - loss: 1.4732 - mean_squared_error: 0.6959 - val_loss: 1.4885 - val_mean_squared_error: 0.7190\n",
      "Epoch 5/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4985 - mean_squared_error: 0.72 - 0s 2ms/step - loss: 1.4591 - mean_squared_error: 0.6959 - val_loss: 1.4745 - val_mean_squared_error: 0.7190\n",
      "Epoch 6/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4463 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 1.4451 - mean_squared_error: 0.6959 - val_loss: 1.4606 - val_mean_squared_error: 0.7190\n",
      "Epoch 7/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4463 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 1.4312 - mean_squared_error: 0.6959 - val_loss: 1.4468 - val_mean_squared_error: 0.7190\n",
      "Epoch 8/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4189 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 1.4175 - mean_squared_error: 0.6959 - val_loss: 1.4331 - val_mean_squared_error: 0.7190\n",
      "Epoch 9/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4221 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 1.4038 - mean_squared_error: 0.6959 - val_loss: 1.4196 - val_mean_squared_error: 0.7190\n",
      "Epoch 10/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4125 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 1.3903 - mean_squared_error: 0.6959 - val_loss: 1.4061 - val_mean_squared_error: 0.7190\n",
      "Epoch 11/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3564 - mean_squared_error: 0.66 - 0s 2ms/step - loss: 1.3769 - mean_squared_error: 0.6959 - val_loss: 1.3927 - val_mean_squared_error: 0.7190\n",
      "Epoch 12/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4098 - mean_squared_error: 0.73 - 0s 2ms/step - loss: 1.3636 - mean_squared_error: 0.6959 - val_loss: 1.3795 - val_mean_squared_error: 0.7191\n",
      "Epoch 13/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4206 - mean_squared_error: 0.76 - 0s 2ms/step - loss: 1.3504 - mean_squared_error: 0.6959 - val_loss: 1.3664 - val_mean_squared_error: 0.7191\n",
      "Epoch 14/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2725 - mean_squared_error: 0.62 - 0s 2ms/step - loss: 1.3373 - mean_squared_error: 0.6959 - val_loss: 1.3534 - val_mean_squared_error: 0.7191\n",
      "Epoch 15/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.4122 - mean_squared_error: 0.77 - 0s 2ms/step - loss: 1.3244 - mean_squared_error: 0.6959 - val_loss: 1.3406 - val_mean_squared_error: 0.7191\n",
      "Epoch 16/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3505 - mean_squared_error: 0.72 - 0s 2ms/step - loss: 1.3116 - mean_squared_error: 0.6959 - val_loss: 1.3279 - val_mean_squared_error: 0.7191\n",
      "Epoch 17/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3277 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 1.2990 - mean_squared_error: 0.6959 - val_loss: 1.3154 - val_mean_squared_error: 0.7191\n",
      "Epoch 18/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2420 - mean_squared_error: 0.64 - 0s 2ms/step - loss: 1.2865 - mean_squared_error: 0.6959 - val_loss: 1.3030 - val_mean_squared_error: 0.7191\n",
      "Epoch 19/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2992 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 1.2741 - mean_squared_error: 0.6959 - val_loss: 1.2906 - val_mean_squared_error: 0.7192\n",
      "Epoch 20/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.3320 - mean_squared_error: 0.76 - 0s 2ms/step - loss: 1.2618 - mean_squared_error: 0.6959 - val_loss: 1.2784 - val_mean_squared_error: 0.7192\n",
      "Epoch 21/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2360 - mean_squared_error: 0.67 - 0s 2ms/step - loss: 1.2495 - mean_squared_error: 0.6959 - val_loss: 1.2662 - val_mean_squared_error: 0.7192\n",
      "Epoch 22/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2548 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 1.2374 - mean_squared_error: 0.6959 - val_loss: 1.2542 - val_mean_squared_error: 0.7193\n",
      "Epoch 23/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1875 - mean_squared_error: 0.65 - 0s 2ms/step - loss: 1.2254 - mean_squared_error: 0.6958 - val_loss: 1.2423 - val_mean_squared_error: 0.7193\n",
      "Epoch 24/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2730 - mean_squared_error: 0.75 - 0s 2ms/step - loss: 1.2136 - mean_squared_error: 0.6958 - val_loss: 1.2306 - val_mean_squared_error: 0.7193\n",
      "Epoch 25/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1601 - mean_squared_error: 0.64 - 0s 2ms/step - loss: 1.2020 - mean_squared_error: 0.6959 - val_loss: 1.2192 - val_mean_squared_error: 0.7193\n",
      "Epoch 26/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.2046 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 1.1906 - mean_squared_error: 0.6958 - val_loss: 1.2079 - val_mean_squared_error: 0.7193\n",
      "Epoch 27/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1697 - mean_squared_error: 0.68 - 0s 2ms/step - loss: 1.1794 - mean_squared_error: 0.6958 - val_loss: 1.1968 - val_mean_squared_error: 0.7193\n",
      "Epoch 28/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1271 - mean_squared_error: 0.64 - 0s 2ms/step - loss: 1.1683 - mean_squared_error: 0.6958 - val_loss: 1.1858 - val_mean_squared_error: 0.7193\n",
      "Epoch 29/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0536 - mean_squared_error: 0.58 - 0s 2ms/step - loss: 1.1573 - mean_squared_error: 0.6958 - val_loss: 1.1748 - val_mean_squared_error: 0.7193\n",
      "Epoch 30/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1429 - mean_squared_error: 0.68 - 0s 2ms/step - loss: 1.1464 - mean_squared_error: 0.6958 - val_loss: 1.1641 - val_mean_squared_error: 0.7193\n",
      "Epoch 31/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1365 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 1.1357 - mean_squared_error: 0.6958 - val_loss: 1.1535 - val_mean_squared_error: 0.7193\n",
      "Epoch 32/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0736 - mean_squared_error: 0.63 - 0s 2ms/step - loss: 1.1252 - mean_squared_error: 0.6958 - val_loss: 1.1430 - val_mean_squared_error: 0.7194\n",
      "Epoch 33/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0662 - mean_squared_error: 0.64 - 0s 2ms/step - loss: 1.1148 - mean_squared_error: 0.6958 - val_loss: 1.1327 - val_mean_squared_error: 0.7194\n",
      "Epoch 34/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1422 - mean_squared_error: 0.72 - 0s 2ms/step - loss: 1.1045 - mean_squared_error: 0.6958 - val_loss: 1.1225 - val_mean_squared_error: 0.7194\n",
      "Epoch 35/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1258 - mean_squared_error: 0.72 - 0s 2ms/step - loss: 1.0944 - mean_squared_error: 0.6958 - val_loss: 1.1124 - val_mean_squared_error: 0.7194\n",
      "Epoch 36/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0768 - mean_squared_error: 0.68 - 0s 2ms/step - loss: 1.0843 - mean_squared_error: 0.6958 - val_loss: 1.1024 - val_mean_squared_error: 0.7194\n",
      "Epoch 37/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0498 - mean_squared_error: 0.66 - 0s 2ms/step - loss: 1.0744 - mean_squared_error: 0.6958 - val_loss: 1.0925 - val_mean_squared_error: 0.7194\n",
      "Epoch 38/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.1891 - mean_squared_error: 0.81 - 0s 2ms/step - loss: 1.0646 - mean_squared_error: 0.6958 - val_loss: 1.0828 - val_mean_squared_error: 0.7194\n",
      "Epoch 39/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 1.0132 - mean_squared_error: 0.64 - 0s 2ms/step - loss: 1.0549 - mean_squared_error: 0.6958 - val_loss: 1.0731 - val_mean_squared_error: 0.7194\n",
      "Epoch 40/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0479 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 1.0452 - mean_squared_error: 0.6958 - val_loss: 1.0636 - val_mean_squared_error: 0.7194\n",
      "Epoch 41/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0073 - mean_squared_error: 0.66 - 0s 2ms/step - loss: 1.0358 - mean_squared_error: 0.6958 - val_loss: 1.0542 - val_mean_squared_error: 0.7194\n",
      "Epoch 42/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0398 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 1.0265 - mean_squared_error: 0.6958 - val_loss: 1.0449 - val_mean_squared_error: 0.7194\n",
      "Epoch 43/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0025 - mean_squared_error: 0.67 - 0s 2ms/step - loss: 1.0173 - mean_squared_error: 0.6958 - val_loss: 1.0359 - val_mean_squared_error: 0.7194\n",
      "Epoch 44/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0385 - mean_squared_error: 0.72 - 0s 2ms/step - loss: 1.0083 - mean_squared_error: 0.6958 - val_loss: 1.0270 - val_mean_squared_error: 0.7193\n",
      "Epoch 45/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9955 - mean_squared_error: 0.68 - 0s 2ms/step - loss: 0.9995 - mean_squared_error: 0.6958 - val_loss: 1.0182 - val_mean_squared_error: 0.7193\n",
      "Epoch 46/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9785 - mean_squared_error: 0.67 - 0s 2ms/step - loss: 0.9907 - mean_squared_error: 0.6958 - val_loss: 1.0095 - val_mean_squared_error: 0.7193\n",
      "Epoch 47/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9255 - mean_squared_error: 0.63 - 0s 16ms/step - loss: 0.9821 - mean_squared_error: 0.6958 - val_loss: 1.0009 - val_mean_squared_error: 0.7193\n",
      "Epoch 48/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9998 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 0.9736 - mean_squared_error: 0.6958 - val_loss: 0.9925 - val_mean_squared_error: 0.7193\n",
      "Epoch 49/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9500 - mean_squared_error: 0.67 - 0s 2ms/step - loss: 0.9651 - mean_squared_error: 0.6958 - val_loss: 0.9841 - val_mean_squared_error: 0.7193\n",
      "Epoch 50/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 1.0009 - mean_squared_error: 0.73 - 0s 2ms/step - loss: 0.9568 - mean_squared_error: 0.6958 - val_loss: 0.9759 - val_mean_squared_error: 0.7193\n",
      "Epoch 51/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8677 - mean_squared_error: 0.61 - 0s 2ms/step - loss: 0.9486 - mean_squared_error: 0.6958 - val_loss: 0.9677 - val_mean_squared_error: 0.7193\n",
      "Epoch 52/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9141 - mean_squared_error: 0.66 - 0s 2ms/step - loss: 0.9405 - mean_squared_error: 0.6958 - val_loss: 0.9596 - val_mean_squared_error: 0.7193\n",
      "Epoch 53/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9170 - mean_squared_error: 0.67 - 0s 2ms/step - loss: 0.9324 - mean_squared_error: 0.6958 - val_loss: 0.9516 - val_mean_squared_error: 0.7193\n",
      "Epoch 54/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9338 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 0.9246 - mean_squared_error: 0.6958 - val_loss: 0.9439 - val_mean_squared_error: 0.7193\n",
      "Epoch 55/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9503 - mean_squared_error: 0.72 - 0s 2ms/step - loss: 0.9170 - mean_squared_error: 0.6958 - val_loss: 0.9364 - val_mean_squared_error: 0.7193\n",
      "Epoch 56/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9395 - mean_squared_error: 0.72 - 0s 2ms/step - loss: 0.9095 - mean_squared_error: 0.6958 - val_loss: 0.9290 - val_mean_squared_error: 0.7193\n",
      "Epoch 57/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9251 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 0.9022 - mean_squared_error: 0.6958 - val_loss: 0.9217 - val_mean_squared_error: 0.7193\n",
      "Epoch 58/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9070 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 0.8950 - mean_squared_error: 0.6958 - val_loss: 0.9146 - val_mean_squared_error: 0.7193\n",
      "Epoch 59/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8652 - mean_squared_error: 0.66 - 0s 2ms/step - loss: 0.8879 - mean_squared_error: 0.6958 - val_loss: 0.9075 - val_mean_squared_error: 0.7193\n",
      "Epoch 60/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9522 - mean_squared_error: 0.76 - 0s 2ms/step - loss: 0.8808 - mean_squared_error: 0.6958 - val_loss: 0.9005 - val_mean_squared_error: 0.7193\n",
      "Epoch 61/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8896 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 0.8739 - mean_squared_error: 0.6958 - val_loss: 0.8937 - val_mean_squared_error: 0.7193\n",
      "Epoch 62/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8680 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 0.8671 - mean_squared_error: 0.6958 - val_loss: 0.8870 - val_mean_squared_error: 0.7193\n",
      "Epoch 63/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8965 - mean_squared_error: 0.72 - 0s 2ms/step - loss: 0.8605 - mean_squared_error: 0.6958 - val_loss: 0.8804 - val_mean_squared_error: 0.7193\n",
      "Epoch 64/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9008 - mean_squared_error: 0.73 - 0s 2ms/step - loss: 0.8540 - mean_squared_error: 0.6958 - val_loss: 0.8739 - val_mean_squared_error: 0.7193\n",
      "Epoch 65/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.9016 - mean_squared_error: 0.74 - 0s 2ms/step - loss: 0.8476 - mean_squared_error: 0.6958 - val_loss: 0.8676 - val_mean_squared_error: 0.7193\n",
      "Epoch 66/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8773 - mean_squared_error: 0.72 - 0s 2ms/step - loss: 0.8413 - mean_squared_error: 0.6958 - val_loss: 0.8614 - val_mean_squared_error: 0.7193\n",
      "Epoch 67/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8499 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 0.8352 - mean_squared_error: 0.6958 - val_loss: 0.8553 - val_mean_squared_error: 0.7193\n",
      "Epoch 68/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8790 - mean_squared_error: 0.74 - 0s 2ms/step - loss: 0.8291 - mean_squared_error: 0.6958 - val_loss: 0.8493 - val_mean_squared_error: 0.7193\n",
      "Epoch 69/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8004 - mean_squared_error: 0.67 - 0s 2ms/step - loss: 0.8233 - mean_squared_error: 0.6958 - val_loss: 0.8436 - val_mean_squared_error: 0.7193\n",
      "Epoch 70/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8403 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 0.8176 - mean_squared_error: 0.6958 - val_loss: 0.8380 - val_mean_squared_error: 0.7193\n",
      "Epoch 71/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7713 - mean_squared_error: 0.65 - 0s 2ms/step - loss: 0.8121 - mean_squared_error: 0.6958 - val_loss: 0.8325 - val_mean_squared_error: 0.7193\n",
      "Epoch 72/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7272 - mean_squared_error: 0.61 - 0s 2ms/step - loss: 0.8067 - mean_squared_error: 0.6958 - val_loss: 0.8273 - val_mean_squared_error: 0.7193\n",
      "Epoch 73/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7677 - mean_squared_error: 0.65 - 0s 2ms/step - loss: 0.8015 - mean_squared_error: 0.6958 - val_loss: 0.8222 - val_mean_squared_error: 0.7193\n",
      "Epoch 74/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7487 - mean_squared_error: 0.64 - 0s 2ms/step - loss: 0.7965 - mean_squared_error: 0.6958 - val_loss: 0.8172 - val_mean_squared_error: 0.7193\n",
      "Epoch 75/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7988 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 0.7916 - mean_squared_error: 0.6958 - val_loss: 0.8123 - val_mean_squared_error: 0.7193\n",
      "Epoch 76/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7669 - mean_squared_error: 0.67 - 0s 2ms/step - loss: 0.7868 - mean_squared_error: 0.6958 - val_loss: 0.8076 - val_mean_squared_error: 0.7192\n",
      "Epoch 77/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 0.7170 - mean_squared_error: 0.62 - 0s 2ms/step - loss: 0.7821 - mean_squared_error: 0.6958 - val_loss: 0.8030 - val_mean_squared_error: 0.7192\n",
      "Epoch 78/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7601 - mean_squared_error: 0.67 - 0s 2ms/step - loss: 0.7776 - mean_squared_error: 0.6958 - val_loss: 0.7986 - val_mean_squared_error: 0.7192\n",
      "Epoch 79/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7144 - mean_squared_error: 0.63 - 0s 2ms/step - loss: 0.7731 - mean_squared_error: 0.6958 - val_loss: 0.7942 - val_mean_squared_error: 0.7192\n",
      "Epoch 80/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7100 - mean_squared_error: 0.63 - 0s 2ms/step - loss: 0.7688 - mean_squared_error: 0.6958 - val_loss: 0.7899 - val_mean_squared_error: 0.7192\n",
      "Epoch 81/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7579 - mean_squared_error: 0.68 - 0s 2ms/step - loss: 0.7646 - mean_squared_error: 0.6958 - val_loss: 0.7857 - val_mean_squared_error: 0.7192\n",
      "Epoch 82/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7227 - mean_squared_error: 0.65 - 0s 2ms/step - loss: 0.7605 - mean_squared_error: 0.6958 - val_loss: 0.7818 - val_mean_squared_error: 0.7192\n",
      "Epoch 83/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8055 - mean_squared_error: 0.74 - 0s 2ms/step - loss: 0.7566 - mean_squared_error: 0.6958 - val_loss: 0.7780 - val_mean_squared_error: 0.7192\n",
      "Epoch 84/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7879 - mean_squared_error: 0.72 - 0s 2ms/step - loss: 0.7528 - mean_squared_error: 0.6958 - val_loss: 0.7742 - val_mean_squared_error: 0.7192\n",
      "Epoch 85/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8149 - mean_squared_error: 0.76 - 0s 2ms/step - loss: 0.7491 - mean_squared_error: 0.6958 - val_loss: 0.7705 - val_mean_squared_error: 0.7192\n",
      "Epoch 86/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7315 - mean_squared_error: 0.68 - 0s 2ms/step - loss: 0.7454 - mean_squared_error: 0.6958 - val_loss: 0.7669 - val_mean_squared_error: 0.7192\n",
      "Epoch 87/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6658 - mean_squared_error: 0.61 - 0s 2ms/step - loss: 0.7419 - mean_squared_error: 0.6958 - val_loss: 0.7634 - val_mean_squared_error: 0.7192\n",
      "Epoch 88/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7941 - mean_squared_error: 0.74 - 0s 2ms/step - loss: 0.7385 - mean_squared_error: 0.6958 - val_loss: 0.7601 - val_mean_squared_error: 0.7192\n",
      "Epoch 89/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7732 - mean_squared_error: 0.73 - 0s 2ms/step - loss: 0.7352 - mean_squared_error: 0.6958 - val_loss: 0.7568 - val_mean_squared_error: 0.7192\n",
      "Epoch 90/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7732 - mean_squared_error: 0.73 - 0s 2ms/step - loss: 0.7320 - mean_squared_error: 0.6958 - val_loss: 0.7537 - val_mean_squared_error: 0.7192\n",
      "Epoch 91/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6701 - mean_squared_error: 0.63 - 0s 2ms/step - loss: 0.7290 - mean_squared_error: 0.6958 - val_loss: 0.7507 - val_mean_squared_error: 0.7192\n",
      "Epoch 92/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7225 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 0.7261 - mean_squared_error: 0.6958 - val_loss: 0.7479 - val_mean_squared_error: 0.7192\n",
      "Epoch 93/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6953 - mean_squared_error: 0.66 - 0s 2ms/step - loss: 0.7233 - mean_squared_error: 0.6958 - val_loss: 0.7452 - val_mean_squared_error: 0.7192\n",
      "Epoch 94/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6955 - mean_squared_error: 0.66 - 0s 2ms/step - loss: 0.7206 - mean_squared_error: 0.6958 - val_loss: 0.7426 - val_mean_squared_error: 0.7192\n",
      "Epoch 95/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7313 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 0.7181 - mean_squared_error: 0.6958 - val_loss: 0.7401 - val_mean_squared_error: 0.7192\n",
      "Epoch 96/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7363 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 0.7158 - mean_squared_error: 0.6958 - val_loss: 0.7379 - val_mean_squared_error: 0.7192\n",
      "Epoch 97/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7788 - mean_squared_error: 0.76 - 0s 2ms/step - loss: 0.7135 - mean_squared_error: 0.6958 - val_loss: 0.7357 - val_mean_squared_error: 0.7192\n",
      "Epoch 98/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6548 - mean_squared_error: 0.63 - 0s 2ms/step - loss: 0.7115 - mean_squared_error: 0.6958 - val_loss: 0.7337 - val_mean_squared_error: 0.7192\n",
      "Epoch 99/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7052 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 0.7095 - mean_squared_error: 0.6958 - val_loss: 0.7319 - val_mean_squared_error: 0.7192\n",
      "Epoch 100/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6968 - mean_squared_error: 0.68 - 0s 2ms/step - loss: 0.7077 - mean_squared_error: 0.6958 - val_loss: 0.7301 - val_mean_squared_error: 0.7192\n",
      "Epoch 101/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7436 - mean_squared_error: 0.73 - 0s 2ms/step - loss: 0.7061 - mean_squared_error: 0.6958 - val_loss: 0.7285 - val_mean_squared_error: 0.7192\n",
      "Epoch 102/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7557 - mean_squared_error: 0.74 - 0s 2ms/step - loss: 0.7045 - mean_squared_error: 0.6958 - val_loss: 0.7271 - val_mean_squared_error: 0.7192\n",
      "Epoch 103/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7019 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 0.7031 - mean_squared_error: 0.6958 - val_loss: 0.7257 - val_mean_squared_error: 0.7192\n",
      "Epoch 104/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7219 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 0.7018 - mean_squared_error: 0.6958 - val_loss: 0.7245 - val_mean_squared_error: 0.7192\n",
      "Epoch 105/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6718 - mean_squared_error: 0.66 - 0s 2ms/step - loss: 0.7007 - mean_squared_error: 0.6958 - val_loss: 0.7235 - val_mean_squared_error: 0.7191\n",
      "Epoch 106/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7299 - mean_squared_error: 0.72 - 0s 2ms/step - loss: 0.6998 - mean_squared_error: 0.6958 - val_loss: 0.7226 - val_mean_squared_error: 0.7191\n",
      "Epoch 107/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7566 - mean_squared_error: 0.75 - 0s 2ms/step - loss: 0.6989 - mean_squared_error: 0.6958 - val_loss: 0.7218 - val_mean_squared_error: 0.7191\n",
      "Epoch 108/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6862 - mean_squared_error: 0.68 - 0s 2ms/step - loss: 0.6981 - mean_squared_error: 0.6958 - val_loss: 0.7211 - val_mean_squared_error: 0.7191\n",
      "Epoch 109/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6615 - mean_squared_error: 0.65 - 0s 2ms/step - loss: 0.6975 - mean_squared_error: 0.6958 - val_loss: 0.7205 - val_mean_squared_error: 0.7191\n",
      "Epoch 110/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6752 - mean_squared_error: 0.67 - 0s 2ms/step - loss: 0.6970 - mean_squared_error: 0.6958 - val_loss: 0.7200 - val_mean_squared_error: 0.7191\n",
      "Epoch 111/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7198 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 0.6966 - mean_squared_error: 0.6958 - val_loss: 0.7197 - val_mean_squared_error: 0.7191\n",
      "Epoch 112/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6777 - mean_squared_error: 0.67 - 0s 2ms/step - loss: 0.6963 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7191\n",
      "Epoch 113/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6947 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 0.6963 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 114/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7437 - mean_squared_error: 0.74 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 115/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 0.7190 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 116/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7088 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 117/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6809 - mean_squared_error: 0.68 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 118/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7189 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 119/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6635 - mean_squared_error: 0.66 - 0s 3ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 120/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7505 - mean_squared_error: 0.75 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 121/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6981 - mean_squared_error: 0.69 - 0s 3ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 122/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6784 - mean_squared_error: 0.67 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 123/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6914 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 124/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6740 - mean_squared_error: 0.67 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 125/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7125 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 126/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6909 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 127/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7049 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 128/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6874 - mean_squared_error: 0.68 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 129/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6914 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 130/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6295 - mean_squared_error: 0.62 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 131/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6943 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 132/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7368 - mean_squared_error: 0.73 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 133/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6874 - mean_squared_error: 0.68 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 134/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6635 - mean_squared_error: 0.66 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 135/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6841 - mean_squared_error: 0.68 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7191\n",
      "Epoch 136/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6948 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 137/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7015 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 138/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7219 - mean_squared_error: 0.72 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 139/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7809 - mean_squared_error: 0.78 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 140/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6990 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 141/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6919 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 142/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6494 - mean_squared_error: 0.64 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 143/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6289 - mean_squared_error: 0.62 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 144/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7288 - mean_squared_error: 0.72 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 145/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6775 - mean_squared_error: 0.67 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 146/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6639 - mean_squared_error: 0.66 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 147/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6640 - mean_squared_error: 0.66 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 148/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6883 - mean_squared_error: 0.68 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 149/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7431 - mean_squared_error: 0.74 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 150/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5974 - mean_squared_error: 0.59 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 151/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6872 - mean_squared_error: 0.68 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 152/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6073 - mean_squared_error: 0.60 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 0.7465 - mean_squared_error: 0.74 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 154/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6560 - mean_squared_error: 0.65 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 155/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7125 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 156/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6980 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 157/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6603 - mean_squared_error: 0.65 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 158/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7874 - mean_squared_error: 0.78 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 159/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7055 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 160/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6529 - mean_squared_error: 0.65 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 161/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7365 - mean_squared_error: 0.73 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 162/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7295 - mean_squared_error: 0.72 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 163/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7284 - mean_squared_error: 0.72 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 164/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6841 - mean_squared_error: 0.68 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 165/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6563 - mean_squared_error: 0.65 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 166/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7011 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7195 - val_mean_squared_error: 0.7192\n",
      "Epoch 167/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7121 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 168/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6618 - mean_squared_error: 0.66 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 169/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7148 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 170/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7188 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 171/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6248 - mean_squared_error: 0.62 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 172/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6950 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 173/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7816 - mean_squared_error: 0.78 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 174/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6491 - mean_squared_error: 0.64 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 175/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6596 - mean_squared_error: 0.65 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 176/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7319 - mean_squared_error: 0.73 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 177/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6773 - mean_squared_error: 0.67 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 178/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7329 - mean_squared_error: 0.73 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 179/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7326 - mean_squared_error: 0.73 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 180/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7630 - mean_squared_error: 0.76 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 181/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6991 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 182/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6492 - mean_squared_error: 0.64 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 183/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7088 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 184/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6664 - mean_squared_error: 0.66 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 185/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6120 - mean_squared_error: 0.61 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 186/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6457 - mean_squared_error: 0.64 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 187/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7095 - mean_squared_error: 0.70 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 188/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7605 - mean_squared_error: 0.76 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 189/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6947 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 190/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6426 - mean_squared_error: 0.64 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 191/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - ETA: 0s - loss: 0.6523 - mean_squared_error: 0.65 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 192/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6978 - mean_squared_error: 0.69 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 193/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6221 - mean_squared_error: 0.62 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 194/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7357 - mean_squared_error: 0.73 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 195/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7570 - mean_squared_error: 0.75 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 196/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7221 - mean_squared_error: 0.72 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 197/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7946 - mean_squared_error: 0.79 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 198/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7130 - mean_squared_error: 0.71 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 199/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7435 - mean_squared_error: 0.74 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n",
      "Epoch 200/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7393 - mean_squared_error: 0.73 - 0s 2ms/step - loss: 0.6962 - mean_squared_error: 0.6958 - val_loss: 0.7196 - val_mean_squared_error: 0.7192\n"
     ]
    }
   ],
   "source": [
    "epochs, mse,val_mse =train_model(two_d_model,data_train,labels_train,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k6a1_test</th>\n",
       "      <th>k6a2_test</th>\n",
       "      <th>k11_test</th>\n",
       "      <th>k12_test</th>\n",
       "      <th>k9a1_test</th>\n",
       "      <th>k9a2_test</th>\n",
       "      <th>k6a1_hat</th>\n",
       "      <th>k6a2_hat</th>\n",
       "      <th>k11_hat</th>\n",
       "      <th>k12_hat</th>\n",
       "      <th>k9a1_hat</th>\n",
       "      <th>k9a2_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.173475</td>\n",
       "      <td>0.168639</td>\n",
       "      <td>0.163112</td>\n",
       "      <td>0.161114</td>\n",
       "      <td>0.162344</td>\n",
       "      <td>0.171314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.173475</td>\n",
       "      <td>0.168642</td>\n",
       "      <td>0.163112</td>\n",
       "      <td>0.161114</td>\n",
       "      <td>0.162346</td>\n",
       "      <td>0.171312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.173475</td>\n",
       "      <td>0.168642</td>\n",
       "      <td>0.163111</td>\n",
       "      <td>0.161115</td>\n",
       "      <td>0.162345</td>\n",
       "      <td>0.171312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.173476</td>\n",
       "      <td>0.168639</td>\n",
       "      <td>0.163113</td>\n",
       "      <td>0.161114</td>\n",
       "      <td>0.162345</td>\n",
       "      <td>0.171314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.173475</td>\n",
       "      <td>0.168639</td>\n",
       "      <td>0.163113</td>\n",
       "      <td>0.161114</td>\n",
       "      <td>0.162345</td>\n",
       "      <td>0.171314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173475</td>\n",
       "      <td>0.168640</td>\n",
       "      <td>0.163112</td>\n",
       "      <td>0.161116</td>\n",
       "      <td>0.162345</td>\n",
       "      <td>0.171313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.173476</td>\n",
       "      <td>0.168639</td>\n",
       "      <td>0.163113</td>\n",
       "      <td>0.161114</td>\n",
       "      <td>0.162345</td>\n",
       "      <td>0.171314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.173475</td>\n",
       "      <td>0.168638</td>\n",
       "      <td>0.163113</td>\n",
       "      <td>0.161114</td>\n",
       "      <td>0.162345</td>\n",
       "      <td>0.171314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173476</td>\n",
       "      <td>0.168641</td>\n",
       "      <td>0.163112</td>\n",
       "      <td>0.161114</td>\n",
       "      <td>0.162344</td>\n",
       "      <td>0.171314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173476</td>\n",
       "      <td>0.168637</td>\n",
       "      <td>0.163114</td>\n",
       "      <td>0.161112</td>\n",
       "      <td>0.162346</td>\n",
       "      <td>0.171315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.173475</td>\n",
       "      <td>0.168642</td>\n",
       "      <td>0.163111</td>\n",
       "      <td>0.161115</td>\n",
       "      <td>0.162345</td>\n",
       "      <td>0.171312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173475</td>\n",
       "      <td>0.168638</td>\n",
       "      <td>0.163114</td>\n",
       "      <td>0.161113</td>\n",
       "      <td>0.162346</td>\n",
       "      <td>0.171315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.173475</td>\n",
       "      <td>0.168639</td>\n",
       "      <td>0.163112</td>\n",
       "      <td>0.161114</td>\n",
       "      <td>0.162344</td>\n",
       "      <td>0.171314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.173475</td>\n",
       "      <td>0.168642</td>\n",
       "      <td>0.163112</td>\n",
       "      <td>0.161114</td>\n",
       "      <td>0.162346</td>\n",
       "      <td>0.171312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.173475</td>\n",
       "      <td>0.168641</td>\n",
       "      <td>0.163111</td>\n",
       "      <td>0.161115</td>\n",
       "      <td>0.162345</td>\n",
       "      <td>0.171312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.173475</td>\n",
       "      <td>0.168642</td>\n",
       "      <td>0.163112</td>\n",
       "      <td>0.161115</td>\n",
       "      <td>0.162345</td>\n",
       "      <td>0.171312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.173476</td>\n",
       "      <td>0.168639</td>\n",
       "      <td>0.163113</td>\n",
       "      <td>0.161114</td>\n",
       "      <td>0.162345</td>\n",
       "      <td>0.171314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.173475</td>\n",
       "      <td>0.168642</td>\n",
       "      <td>0.163112</td>\n",
       "      <td>0.161115</td>\n",
       "      <td>0.162345</td>\n",
       "      <td>0.171312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.173475</td>\n",
       "      <td>0.168642</td>\n",
       "      <td>0.163112</td>\n",
       "      <td>0.161114</td>\n",
       "      <td>0.162346</td>\n",
       "      <td>0.171312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173476</td>\n",
       "      <td>0.168637</td>\n",
       "      <td>0.163114</td>\n",
       "      <td>0.161112</td>\n",
       "      <td>0.162346</td>\n",
       "      <td>0.171315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    k6a1_test  k6a2_test  k11_test  k12_test  k9a1_test  k9a2_test  k6a1_hat  \\\n",
       "0         1.0       -1.0       1.0       0.0       -1.0       -1.0  0.173475   \n",
       "1         1.0       -1.0       0.0      -1.0       -1.0        1.0  0.173475   \n",
       "2         1.0       -1.0       0.0      -1.0        0.0        1.0  0.173475   \n",
       "3        -1.0        1.0       1.0       0.0        0.0       -1.0  0.173476   \n",
       "4         0.0        0.0       1.0       1.0        0.0       -1.0  0.173475   \n",
       "5         1.0       -1.0       0.0       0.0        1.0        0.0  0.173475   \n",
       "6        -1.0       -1.0      -1.0       0.0       -1.0        1.0  0.173476   \n",
       "7        -1.0        0.0      -1.0       1.0        1.0       -1.0  0.173475   \n",
       "8        -1.0       -1.0       1.0       0.0        0.0        0.0  0.173476   \n",
       "9         1.0        0.0       1.0      -1.0       -1.0        0.0  0.173476   \n",
       "10       -1.0        1.0      -1.0       1.0        1.0       -1.0  0.173475   \n",
       "11        1.0        0.0       1.0       1.0        1.0        0.0  0.173475   \n",
       "12       -1.0        1.0       1.0       0.0        1.0        1.0  0.173475   \n",
       "13       -1.0        1.0      -1.0       1.0        0.0       -1.0  0.173475   \n",
       "14       -1.0        1.0      -1.0       1.0       -1.0       -1.0  0.173475   \n",
       "15        0.0        1.0      -1.0       1.0       -1.0        1.0  0.173475   \n",
       "16        0.0       -1.0       0.0       0.0        0.0        1.0  0.173476   \n",
       "17        1.0        1.0       1.0       1.0        1.0       -1.0  0.173475   \n",
       "18        0.0        1.0       0.0      -1.0        1.0        1.0  0.173475   \n",
       "19        0.0        0.0       0.0       1.0        0.0        0.0  0.173476   \n",
       "\n",
       "    k6a2_hat   k11_hat   k12_hat  k9a1_hat  k9a2_hat  \n",
       "0   0.168639  0.163112  0.161114  0.162344  0.171314  \n",
       "1   0.168642  0.163112  0.161114  0.162346  0.171312  \n",
       "2   0.168642  0.163111  0.161115  0.162345  0.171312  \n",
       "3   0.168639  0.163113  0.161114  0.162345  0.171314  \n",
       "4   0.168639  0.163113  0.161114  0.162345  0.171314  \n",
       "5   0.168640  0.163112  0.161116  0.162345  0.171313  \n",
       "6   0.168639  0.163113  0.161114  0.162345  0.171314  \n",
       "7   0.168638  0.163113  0.161114  0.162345  0.171314  \n",
       "8   0.168641  0.163112  0.161114  0.162344  0.171314  \n",
       "9   0.168637  0.163114  0.161112  0.162346  0.171315  \n",
       "10  0.168642  0.163111  0.161115  0.162345  0.171312  \n",
       "11  0.168638  0.163114  0.161113  0.162346  0.171315  \n",
       "12  0.168639  0.163112  0.161114  0.162344  0.171314  \n",
       "13  0.168642  0.163112  0.161114  0.162346  0.171312  \n",
       "14  0.168641  0.163111  0.161115  0.162345  0.171312  \n",
       "15  0.168642  0.163112  0.161115  0.162345  0.171312  \n",
       "16  0.168639  0.163113  0.161114  0.162345  0.171314  \n",
       "17  0.168642  0.163112  0.161115  0.162345  0.171312  \n",
       "18  0.168642  0.163112  0.161114  0.162346  0.171312  \n",
       "19  0.168637  0.163114  0.161112  0.162346  0.171315  "
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#features = {name:np.array(value) for name, value in df_test.items()}\n",
    "#label=df_test[label_name].to_numpy()\n",
    "#data_train, data_test, labels_train, labels_test\n",
    "plot_the_loss_curve(epochs, mse,val_mse)\n",
    "#evaluation=two_d_model.evaluate(x = data_test, y = labels_test)\n",
    "\n",
    "#print(\"Hello\",data_test[:3])\n",
    "predicted = two_d_model.predict(data_test)\n",
    "#predicted=predicted.reshape(len(data_test),6,2)\n",
    "#print(predicted)\n",
    "\n",
    "\n",
    "#decode labels:\n",
    "#print(predicted)\n",
    "\n",
    "\n",
    "\n",
    "df_test=pd.DataFrame(labels_test,columns=[\"k6a1_test\",\"k6a2_test\",\"k11_test\",\"k12_test\",\"k9a1_test\",\"k9a2_test\"])\n",
    "df_predict=pd.DataFrame(predicted,columns=[\"k6a1_hat\",\"k6a2_hat\",\"k11_hat\",\"k12_hat\",\"k9a1_hat\",\"k9a2_hat\"])\n",
    "pd.concat([df_test,df_predict], axis=1).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
