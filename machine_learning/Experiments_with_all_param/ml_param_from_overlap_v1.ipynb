{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import kerastuner as kt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_curve(epochs, hist, list_of_metrics,name):\n",
    "    \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch \"+name)\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "    for m in list_of_metrics:\n",
    "        x = hist[m]\n",
    "        plt.plot(epochs[1:], x[1:], label=m)\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "# for activation functions check https://keras.io/api/layers/activations/\n",
    "def create_model2(my_learning_rate,momentum,layers,my_metrics,my_act_function = \"softmax\"):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    #model.add(my_feature_layer)\n",
    "\n",
    "    for layer in layers:\n",
    "        model.add(tf.keras.layers.Dense(units = layer, activation = my_act_function))\n",
    "    model.add(tf.keras.layers.Dense(units=6,name='Output', activation = 'softmax'))                             \n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(lr=my_learning_rate,momentum=momentum),                                       \n",
    "                loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "                metrics=my_metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model,x_data, y_data, epochs, label_name,\n",
    "                batch_size=None,shuffle=True):\n",
    "    #features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    history = model.fit(x=x_data, y=y_data, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle,validation_split=0.2,\n",
    "                       callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)])\n",
    "  \n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    return epochs, hist\n",
    "    \n",
    "    \n",
    "#returns dataframe\n",
    "def test_model(model,x_data, y_data ,label_name):\n",
    "    evaluation=model.evaluate(x = x_data, y = y_data, batch_size=batch_size)\n",
    "    predicted = model.predict(x_data)\n",
    "    df_test=pd.DataFrame(y_data,columns=[label_name])\n",
    "   # print(predicted)\n",
    "    df_predict=pd.DataFrame(predicted,columns=[label+\"_pred\" for label in label_name])\n",
    "    return pd.concat([df_test,df_predict], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k6a1</th>\n",
       "      <th>k6a2</th>\n",
       "      <th>k11</th>\n",
       "      <th>k12</th>\n",
       "      <th>k9a1</th>\n",
       "      <th>k9a2</th>\n",
       "      <th>delta</th>\n",
       "      <th>lambda</th>\n",
       "      <th>overlap_s0_s2_k6a</th>\n",
       "      <th>overlap_s0_s2_k1</th>\n",
       "      <th>overlap_s0_s2_k9a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.5232, 0.3386, 0.1098, 0.0239, 0.004]</td>\n",
       "      <td>[0.7501, 0.2155, 0.031, 0.003, 0.0002]</td>\n",
       "      <td>[0.8102, 0.1704, 0.018, 0.0013, 0.0001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0333</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.5232, 0.3386, 0.1098, 0.0239, 0.004]</td>\n",
       "      <td>[0.7501, 0.2155, 0.031, 0.003, 0.0002]</td>\n",
       "      <td>[0.9769, 0.0228, 0.0003, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.5232, 0.3386, 0.1098, 0.0239, 0.004]</td>\n",
       "      <td>[0.7501, 0.2155, 0.031, 0.003, 0.0002]</td>\n",
       "      <td>[0.8102, 0.1704, 0.018, 0.0013, 0.0001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.5232, 0.3386, 0.1098, 0.0239, 0.004]</td>\n",
       "      <td>[0.7501, 0.2155, 0.031, 0.003, 0.0002]</td>\n",
       "      <td>[0.8102, 0.1704, 0.018, 0.0013, 0.0001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.5232, 0.3386, 0.1098, 0.0239, 0.004]</td>\n",
       "      <td>[0.7501, 0.2155, 0.031, 0.003, 0.0002]</td>\n",
       "      <td>[0.8102, 0.1704, 0.018, 0.0013, 0.0001]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k6a1  k6a2  k11  k12  k9a1    k9a2   delta  lambda  \\\n",
       "0  -0.1  -0.1 -0.1 -0.1  -0.1 -0.1000  0.6000  0.1000   \n",
       "1  -0.1  -0.1 -0.1 -0.1  -0.1 -0.0333  0.6000  0.2333   \n",
       "2  -0.1  -0.1 -0.1 -0.1  -0.1 -0.1000  0.6000  0.2333   \n",
       "3  -0.1  -0.1 -0.1 -0.1  -0.1 -0.1000  0.5333  0.2333   \n",
       "4  -0.1  -0.1 -0.1 -0.1  -0.1 -0.1000  0.5333  0.1000   \n",
       "\n",
       "                         overlap_s0_s2_k6a  \\\n",
       "0  [0.5232, 0.3386, 0.1098, 0.0239, 0.004]   \n",
       "1  [0.5232, 0.3386, 0.1098, 0.0239, 0.004]   \n",
       "2  [0.5232, 0.3386, 0.1098, 0.0239, 0.004]   \n",
       "3  [0.5232, 0.3386, 0.1098, 0.0239, 0.004]   \n",
       "4  [0.5232, 0.3386, 0.1098, 0.0239, 0.004]   \n",
       "\n",
       "                         overlap_s0_s2_k1  \\\n",
       "0  [0.7501, 0.2155, 0.031, 0.003, 0.0002]   \n",
       "1  [0.7501, 0.2155, 0.031, 0.003, 0.0002]   \n",
       "2  [0.7501, 0.2155, 0.031, 0.003, 0.0002]   \n",
       "3  [0.7501, 0.2155, 0.031, 0.003, 0.0002]   \n",
       "4  [0.7501, 0.2155, 0.031, 0.003, 0.0002]   \n",
       "\n",
       "                         overlap_s0_s2_k9a  \n",
       "0  [0.8102, 0.1704, 0.018, 0.0013, 0.0001]  \n",
       "1       [0.9769, 0.0228, 0.0003, 0.0, 0.0]  \n",
       "2  [0.8102, 0.1704, 0.018, 0.0013, 0.0001]  \n",
       "3  [0.8102, 0.1704, 0.018, 0.0013, 0.0001]  \n",
       "4  [0.8102, 0.1704, 0.018, 0.0013, 0.0001]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_labels_features=[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\",\"delta\",\"lambda\",\"overlap_s0_s2_k6a\",\"overlap_s0_s2_k1\",\"overlap_s0_s2_k9a\"]\n",
    "all_data=pd.read_csv(\"G:\\OneDrive - bwedu\\Master\\Forschungspraktikum\\Inga\\pc-forschi\\generated_Data/all_param_4_values_with_overlap.csv\")\n",
    "df_feature_labels=all_data[all_labels_features]\n",
    "\n",
    "max_no_of_peak_list=max(all_data[\"no_of_max\"])\n",
    "print(max_no_of_peak_list)\n",
    "df_feature_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65464, 3)\n",
      "(65464, 15)\n",
      "[[0.5232 0.3386 0.1098 ... 0.     0.     0.    ]\n",
      " [0.9307 0.0669 0.0024 ... 0.     0.     0.    ]\n",
      " [0.9307 0.0669 0.0024 ... 0.     0.     0.    ]\n",
      " ...\n",
      " [0.5232 0.3386 0.1098 ... 0.     0.     0.    ]\n",
      " [0.5232 0.3386 0.1098 ... 0.     0.     0.    ]\n",
      " [0.9307 0.0669 0.0024 ... 0.     0.     0.    ]]\n"
     ]
    }
   ],
   "source": [
    "overlap_s0_s2_k6a_array=np.asarray([  np.asarray([x for x in row.replace(\",\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"  \",\" \",5).replace(\" \",\";\").split(\";\") if x!=\"\"],dtype=np.float64)     for row in df_feature_labels[\"overlap_s0_s2_k6a\"] ])\n",
    "\n",
    "overlap_s0_s2_k1_array=np.asarray([  np.asarray([x for x in row.replace(\",\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"  \",\" \",5).replace(\" \",\";\").split(\";\") if x!=\"\"],dtype=np.float64)     for row in df_feature_labels[\"overlap_s0_s2_k1\"] ])\n",
    "\n",
    "overlap_s0_s2_k9a_array=np.asarray([  np.asarray([x for x in row.replace(\",\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"  \",\" \",5).replace(\" \",\";\").split(\";\") if x!=\"\"],dtype=np.float64)     for row in df_feature_labels[\"overlap_s0_s2_k9a\"] ])\n",
    "\n",
    "\n",
    "concat_feature=np.concatenate((overlap_s0_s2_k6a_array,overlap_s0_s2_k1_array,overlap_s0_s2_k9a_array),axis=1)\n",
    "omega_array=np.zeros((len(concat_feature),3))\n",
    "#w6a\n",
    "omega_array[:][0]=0.0740/27.211\n",
    "#w1\n",
    "omega_array[:][1]=0.1273/27.211\n",
    "#w9a\n",
    "omega_array[:][2]=0.1568/27.211\n",
    "\n",
    "print(omega_array.shape)\n",
    "print(concat_feature.shape)\n",
    "concat_feature=np.concatenate((concat_feature,omega_array),axis=1)\n",
    "\n",
    "\n",
    "concat_label=df_feature_labels[[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"]].abs().to_numpy()#,\"delta\",\"lambda\"\n",
    "\n",
    "\n",
    "x_train_overlap, x_test_overlap,y_train_overlap,y_test_overlap = train_test_split( concat_feature, concat_label  ,test_size=0.20, random_state=42)\n",
    "\n",
    "print(x_train_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0994 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0997 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0997 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - 3s 12ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 2/50\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.1016 - mean_absolute_error: 0.101 - ETA: 2s - loss: 0.1006 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1003 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1003 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - 3s 11ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - ETA: 0s - loss: 0.1010 - mean_absolute_error: 0.101 - ETA: 2s - loss: 0.1004 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1004 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1003 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1003 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - 3s 11ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 4/50\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0987 - mean_absolute_error: 0.098 - ETA: 2s - loss: 0.0997 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0996 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0995 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0997 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - 3s 11ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - ETA: 0s - loss: 0.0987 - mean_absolute_error: 0.098 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - 3s 11ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 6/50\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1003 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.0997 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1003 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1004 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1003 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - 3s 11ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - ETA: 0s - loss: 0.0995 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0997 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0996 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0997 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0996 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0997 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0997 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - 3s 11ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 8/50\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.1013 - mean_absolute_error: 0.101 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0997 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0997 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0997 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - 3s 11ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - ETA: 0s - loss: 0.1009 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.0996 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - 3s 12ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 10/50\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0988 - mean_absolute_error: 0.098 - ETA: 2s - loss: 0.0994 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0997 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0997 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - 3s 11ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - ETA: 0s - loss: 0.1017 - mean_absolute_error: 0.101 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - 3s 11ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 12/50\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.1007 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1003 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1004 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1003 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - 3s 11ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - ETA: 0s - loss: 0.0992 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0995 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0994 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0995 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0994 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0995 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0995 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0995 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0996 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - 3s 12ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 14/50\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.1017 - mean_absolute_error: 0.101 - ETA: 2s - loss: 0.1004 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 2s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - 3s 12ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "momentum=0.7\n",
    "epochs = 50\n",
    "batch_size = 150\n",
    "\n",
    "#specify the classification threshold\n",
    "classification_threshold = 0.15\n",
    "\n",
    "# Establish the metrics the model will measure.\n",
    "metric = [tf.keras.metrics.MeanAbsoluteError()]\n",
    "layers=[16,256,2048,512,64,16]\n",
    "\n",
    "\n",
    "all_label_list=[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"]#,\"delta\",\"lambda\"]\n",
    "\n",
    "my_model= create_model2(learning_rate,momentum,layers,metric,my_act_function=\"relu\")\n",
    "\n",
    "epochs_run, hist = train_model(my_model,x_train_overlap, y_train_overlap, epochs, \n",
    "                          all_label_list, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_metrics_to_plot = ['mean_absolute_error',\"val_mean_absolute_error\"] \n",
    "plot_curve(epochs_run, hist, list_of_metrics_to_plot,\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - ETA: 0s - loss: 0.1007 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1003 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1003 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1001 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1002 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.1002 - mean_absolute_error: 0.100 - 0s 3ms/step - loss: 0.1002 - mean_absolute_error: 0.1002\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(k6a1,)</th>\n",
       "      <th>(k6a2,)</th>\n",
       "      <th>(k11,)</th>\n",
       "      <th>(k12,)</th>\n",
       "      <th>(k9a1,)</th>\n",
       "      <th>(k9a2,)</th>\n",
       "      <th>k6a1_pred</th>\n",
       "      <th>k6a2_pred</th>\n",
       "      <th>k11_pred</th>\n",
       "      <th>k12_pred</th>\n",
       "      <th>k9a1_pred</th>\n",
       "      <th>k9a2_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163530</td>\n",
       "      <td>0.169757</td>\n",
       "      <td>0.170894</td>\n",
       "      <td>0.166699</td>\n",
       "      <td>0.164647</td>\n",
       "      <td>0.164472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163530</td>\n",
       "      <td>0.169757</td>\n",
       "      <td>0.170894</td>\n",
       "      <td>0.166699</td>\n",
       "      <td>0.164647</td>\n",
       "      <td>0.164472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163675</td>\n",
       "      <td>0.169798</td>\n",
       "      <td>0.170573</td>\n",
       "      <td>0.166688</td>\n",
       "      <td>0.164750</td>\n",
       "      <td>0.164515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163675</td>\n",
       "      <td>0.169798</td>\n",
       "      <td>0.170573</td>\n",
       "      <td>0.166688</td>\n",
       "      <td>0.164750</td>\n",
       "      <td>0.164515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163675</td>\n",
       "      <td>0.169798</td>\n",
       "      <td>0.170573</td>\n",
       "      <td>0.166688</td>\n",
       "      <td>0.164750</td>\n",
       "      <td>0.164515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.163988</td>\n",
       "      <td>0.169378</td>\n",
       "      <td>0.170033</td>\n",
       "      <td>0.166921</td>\n",
       "      <td>0.164728</td>\n",
       "      <td>0.164951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163946</td>\n",
       "      <td>0.169583</td>\n",
       "      <td>0.169657</td>\n",
       "      <td>0.166924</td>\n",
       "      <td>0.164949</td>\n",
       "      <td>0.164941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.163988</td>\n",
       "      <td>0.169378</td>\n",
       "      <td>0.170033</td>\n",
       "      <td>0.166921</td>\n",
       "      <td>0.164728</td>\n",
       "      <td>0.164951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.163988</td>\n",
       "      <td>0.169378</td>\n",
       "      <td>0.170033</td>\n",
       "      <td>0.166921</td>\n",
       "      <td>0.164728</td>\n",
       "      <td>0.164951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163530</td>\n",
       "      <td>0.169757</td>\n",
       "      <td>0.170894</td>\n",
       "      <td>0.166699</td>\n",
       "      <td>0.164647</td>\n",
       "      <td>0.164472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.163988</td>\n",
       "      <td>0.169378</td>\n",
       "      <td>0.170033</td>\n",
       "      <td>0.166921</td>\n",
       "      <td>0.164728</td>\n",
       "      <td>0.164951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163675</td>\n",
       "      <td>0.169798</td>\n",
       "      <td>0.170573</td>\n",
       "      <td>0.166688</td>\n",
       "      <td>0.164750</td>\n",
       "      <td>0.164515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.164622</td>\n",
       "      <td>0.168951</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.166780</td>\n",
       "      <td>0.165485</td>\n",
       "      <td>0.165035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.163919</td>\n",
       "      <td>0.169615</td>\n",
       "      <td>0.170079</td>\n",
       "      <td>0.166906</td>\n",
       "      <td>0.164642</td>\n",
       "      <td>0.164839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163946</td>\n",
       "      <td>0.169583</td>\n",
       "      <td>0.169657</td>\n",
       "      <td>0.166924</td>\n",
       "      <td>0.164949</td>\n",
       "      <td>0.164941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.163988</td>\n",
       "      <td>0.169378</td>\n",
       "      <td>0.170033</td>\n",
       "      <td>0.166921</td>\n",
       "      <td>0.164728</td>\n",
       "      <td>0.164951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163530</td>\n",
       "      <td>0.169757</td>\n",
       "      <td>0.170894</td>\n",
       "      <td>0.166699</td>\n",
       "      <td>0.164647</td>\n",
       "      <td>0.164472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163946</td>\n",
       "      <td>0.169583</td>\n",
       "      <td>0.169657</td>\n",
       "      <td>0.166924</td>\n",
       "      <td>0.164949</td>\n",
       "      <td>0.164941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163946</td>\n",
       "      <td>0.169583</td>\n",
       "      <td>0.169657</td>\n",
       "      <td>0.166924</td>\n",
       "      <td>0.164949</td>\n",
       "      <td>0.164941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.163988</td>\n",
       "      <td>0.169378</td>\n",
       "      <td>0.170033</td>\n",
       "      <td>0.166921</td>\n",
       "      <td>0.164728</td>\n",
       "      <td>0.164951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.164398</td>\n",
       "      <td>0.169051</td>\n",
       "      <td>0.168891</td>\n",
       "      <td>0.167102</td>\n",
       "      <td>0.165178</td>\n",
       "      <td>0.165380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163946</td>\n",
       "      <td>0.169583</td>\n",
       "      <td>0.169657</td>\n",
       "      <td>0.166924</td>\n",
       "      <td>0.164949</td>\n",
       "      <td>0.164941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.163988</td>\n",
       "      <td>0.169378</td>\n",
       "      <td>0.170033</td>\n",
       "      <td>0.166921</td>\n",
       "      <td>0.164728</td>\n",
       "      <td>0.164951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.163988</td>\n",
       "      <td>0.169378</td>\n",
       "      <td>0.170033</td>\n",
       "      <td>0.166921</td>\n",
       "      <td>0.164728</td>\n",
       "      <td>0.164951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.164356</td>\n",
       "      <td>0.169330</td>\n",
       "      <td>0.169689</td>\n",
       "      <td>0.166610</td>\n",
       "      <td>0.165483</td>\n",
       "      <td>0.164532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163530</td>\n",
       "      <td>0.169757</td>\n",
       "      <td>0.170894</td>\n",
       "      <td>0.166699</td>\n",
       "      <td>0.164647</td>\n",
       "      <td>0.164472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163675</td>\n",
       "      <td>0.169798</td>\n",
       "      <td>0.170573</td>\n",
       "      <td>0.166688</td>\n",
       "      <td>0.164750</td>\n",
       "      <td>0.164515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.164356</td>\n",
       "      <td>0.169330</td>\n",
       "      <td>0.169689</td>\n",
       "      <td>0.166610</td>\n",
       "      <td>0.165483</td>\n",
       "      <td>0.164532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.164398</td>\n",
       "      <td>0.169051</td>\n",
       "      <td>0.168891</td>\n",
       "      <td>0.167102</td>\n",
       "      <td>0.165178</td>\n",
       "      <td>0.165380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.164398</td>\n",
       "      <td>0.169051</td>\n",
       "      <td>0.168891</td>\n",
       "      <td>0.167102</td>\n",
       "      <td>0.165178</td>\n",
       "      <td>0.165380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163530</td>\n",
       "      <td>0.169757</td>\n",
       "      <td>0.170894</td>\n",
       "      <td>0.166699</td>\n",
       "      <td>0.164647</td>\n",
       "      <td>0.164472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163946</td>\n",
       "      <td>0.169583</td>\n",
       "      <td>0.169657</td>\n",
       "      <td>0.166924</td>\n",
       "      <td>0.164949</td>\n",
       "      <td>0.164941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163675</td>\n",
       "      <td>0.169798</td>\n",
       "      <td>0.170573</td>\n",
       "      <td>0.166688</td>\n",
       "      <td>0.164750</td>\n",
       "      <td>0.164515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163530</td>\n",
       "      <td>0.169757</td>\n",
       "      <td>0.170894</td>\n",
       "      <td>0.166699</td>\n",
       "      <td>0.164647</td>\n",
       "      <td>0.164472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.164356</td>\n",
       "      <td>0.169330</td>\n",
       "      <td>0.169689</td>\n",
       "      <td>0.166610</td>\n",
       "      <td>0.165483</td>\n",
       "      <td>0.164532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.164398</td>\n",
       "      <td>0.169051</td>\n",
       "      <td>0.168891</td>\n",
       "      <td>0.167102</td>\n",
       "      <td>0.165178</td>\n",
       "      <td>0.165380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.163988</td>\n",
       "      <td>0.169378</td>\n",
       "      <td>0.170033</td>\n",
       "      <td>0.166921</td>\n",
       "      <td>0.164728</td>\n",
       "      <td>0.164951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.164356</td>\n",
       "      <td>0.169330</td>\n",
       "      <td>0.169689</td>\n",
       "      <td>0.166610</td>\n",
       "      <td>0.165483</td>\n",
       "      <td>0.164532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163530</td>\n",
       "      <td>0.169757</td>\n",
       "      <td>0.170894</td>\n",
       "      <td>0.166699</td>\n",
       "      <td>0.164647</td>\n",
       "      <td>0.164472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163946</td>\n",
       "      <td>0.169583</td>\n",
       "      <td>0.169657</td>\n",
       "      <td>0.166924</td>\n",
       "      <td>0.164949</td>\n",
       "      <td>0.164941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163530</td>\n",
       "      <td>0.169757</td>\n",
       "      <td>0.170894</td>\n",
       "      <td>0.166699</td>\n",
       "      <td>0.164647</td>\n",
       "      <td>0.164472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.164622</td>\n",
       "      <td>0.168951</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.166780</td>\n",
       "      <td>0.165485</td>\n",
       "      <td>0.165035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163530</td>\n",
       "      <td>0.169757</td>\n",
       "      <td>0.170894</td>\n",
       "      <td>0.166699</td>\n",
       "      <td>0.164647</td>\n",
       "      <td>0.164472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.164622</td>\n",
       "      <td>0.168951</td>\n",
       "      <td>0.169127</td>\n",
       "      <td>0.166780</td>\n",
       "      <td>0.165485</td>\n",
       "      <td>0.165035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.164398</td>\n",
       "      <td>0.169051</td>\n",
       "      <td>0.168891</td>\n",
       "      <td>0.167102</td>\n",
       "      <td>0.165178</td>\n",
       "      <td>0.165380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.164356</td>\n",
       "      <td>0.169330</td>\n",
       "      <td>0.169689</td>\n",
       "      <td>0.166610</td>\n",
       "      <td>0.165483</td>\n",
       "      <td>0.164532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.163919</td>\n",
       "      <td>0.169615</td>\n",
       "      <td>0.170079</td>\n",
       "      <td>0.166906</td>\n",
       "      <td>0.164642</td>\n",
       "      <td>0.164839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.163946</td>\n",
       "      <td>0.169583</td>\n",
       "      <td>0.169657</td>\n",
       "      <td>0.166924</td>\n",
       "      <td>0.164949</td>\n",
       "      <td>0.164941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.163919</td>\n",
       "      <td>0.169615</td>\n",
       "      <td>0.170079</td>\n",
       "      <td>0.166906</td>\n",
       "      <td>0.164642</td>\n",
       "      <td>0.164839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.163988</td>\n",
       "      <td>0.169378</td>\n",
       "      <td>0.170033</td>\n",
       "      <td>0.166921</td>\n",
       "      <td>0.164728</td>\n",
       "      <td>0.164951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    (k6a1,)  (k6a2,)  (k11,)  (k12,)  (k9a1,)  (k9a2,)  k6a1_pred  k6a2_pred  \\\n",
       "0    0.0333   0.0333  0.1000  0.1000   0.0333   0.0333   0.163530   0.169757   \n",
       "1    0.0333   0.0333  0.0333  0.1000   0.1000   0.0333   0.163530   0.169757   \n",
       "2    0.1000   0.0333  0.0333  0.0333   0.1000   0.0333   0.163675   0.169798   \n",
       "3    0.0333   0.0333  0.0333  0.0333   0.1000   0.0333   0.163675   0.169798   \n",
       "4    0.0333   0.0333  0.1000  0.0333   0.0333   0.0333   0.163675   0.169798   \n",
       "5    0.0333   0.0333  0.0333  0.1000   0.1000   0.1000   0.163988   0.169378   \n",
       "6    0.1000   0.1000  0.0333  0.0333   0.0333   0.0333   0.163946   0.169583   \n",
       "7    0.1000   0.0333  0.1000  0.1000   0.0333   0.1000   0.163988   0.169378   \n",
       "8    0.0333   0.0333  0.1000  0.1000   0.1000   0.1000   0.163988   0.169378   \n",
       "9    0.0333   0.0333  0.1000  0.1000   0.1000   0.0333   0.163530   0.169757   \n",
       "10   0.1000   0.0333  0.0333  0.1000   0.1000   0.1000   0.163988   0.169378   \n",
       "11   0.1000   0.0333  0.0333  0.0333   0.1000   0.0333   0.163675   0.169798   \n",
       "12   0.1000   0.1000  0.0333  0.1000   0.1000   0.1000   0.164622   0.168951   \n",
       "13   0.1000   0.0333  0.1000  0.0333   0.0333   0.1000   0.163919   0.169615   \n",
       "14   0.0333   0.1000  0.0333  0.0333   0.0333   0.0333   0.163946   0.169583   \n",
       "15   0.1000   0.0333  0.0333  0.1000   0.0333   0.1000   0.163988   0.169378   \n",
       "16   0.1000   0.0333  0.0333  0.1000   0.1000   0.0333   0.163530   0.169757   \n",
       "17   0.1000   0.1000  0.0333  0.0333   0.1000   0.0333   0.163946   0.169583   \n",
       "18   0.0333   0.1000  0.0333  0.0333   0.0333   0.0333   0.163946   0.169583   \n",
       "19   0.0333   0.0333  0.0333  0.1000   0.1000   0.1000   0.163988   0.169378   \n",
       "20   0.0333   0.1000  0.1000  0.0333   0.0333   0.1000   0.164398   0.169051   \n",
       "21   0.1000   0.1000  0.1000  0.0333   0.1000   0.0333   0.163946   0.169583   \n",
       "22   0.0333   0.0333  0.0333  0.1000   0.1000   0.1000   0.163988   0.169378   \n",
       "23   0.1000   0.0333  0.0333  0.1000   0.1000   0.1000   0.163988   0.169378   \n",
       "24   0.0333   0.1000  0.0333  0.1000   0.1000   0.0333   0.164356   0.169330   \n",
       "25   0.1000   0.0333  0.1000  0.1000   0.1000   0.0333   0.163530   0.169757   \n",
       "26   0.1000   0.0333  0.0333  0.0333   0.1000   0.0333   0.163675   0.169798   \n",
       "27   0.1000   0.1000  0.0333  0.1000   0.1000   0.0333   0.164356   0.169330   \n",
       "28   0.1000   0.1000  0.0333  0.0333   0.0333   0.1000   0.164398   0.169051   \n",
       "29   0.1000   0.1000  0.0333  0.0333   0.1000   0.1000   0.164398   0.169051   \n",
       "30   0.0333   0.0333  0.1000  0.1000   0.0333   0.0333   0.163530   0.169757   \n",
       "31   0.0333   0.1000  0.1000  0.0333   0.0333   0.0333   0.163946   0.169583   \n",
       "32   0.0333   0.0333  0.0333  0.0333   0.0333   0.0333   0.163675   0.169798   \n",
       "33   0.0333   0.0333  0.0333  0.1000   0.0333   0.0333   0.163530   0.169757   \n",
       "34   0.1000   0.1000  0.0333  0.1000   0.0333   0.0333   0.164356   0.169330   \n",
       "35   0.0333   0.1000  0.0333  0.0333   0.0333   0.1000   0.164398   0.169051   \n",
       "36   0.0333   0.0333  0.1000  0.1000   0.0333   0.1000   0.163988   0.169378   \n",
       "37   0.0333   0.1000  0.0333  0.1000   0.0333   0.0333   0.164356   0.169330   \n",
       "38   0.1000   0.0333  0.1000  0.1000   0.1000   0.0333   0.163530   0.169757   \n",
       "39   0.1000   0.1000  0.1000  0.0333   0.0333   0.0333   0.163946   0.169583   \n",
       "40   0.0333   0.0333  0.1000  0.1000   0.1000   0.0333   0.163530   0.169757   \n",
       "41   0.1000   0.1000  0.1000  0.1000   0.1000   0.1000   0.164622   0.168951   \n",
       "42   0.0333   0.0333  0.0333  0.1000   0.0333   0.0333   0.163530   0.169757   \n",
       "43   0.1000   0.1000  0.1000  0.1000   0.1000   0.1000   0.164622   0.168951   \n",
       "44   0.0333   0.1000  0.0333  0.0333   0.0333   0.1000   0.164398   0.169051   \n",
       "45   0.1000   0.1000  0.0333  0.1000   0.0333   0.0333   0.164356   0.169330   \n",
       "46   0.0333   0.0333  0.0333  0.0333   0.1000   0.1000   0.163919   0.169615   \n",
       "47   0.1000   0.1000  0.0333  0.0333   0.0333   0.0333   0.163946   0.169583   \n",
       "48   0.0333   0.0333  0.1000  0.0333   0.0333   0.1000   0.163919   0.169615   \n",
       "49   0.0333   0.0333  0.0333  0.1000   0.0333   0.1000   0.163988   0.169378   \n",
       "\n",
       "    k11_pred  k12_pred  k9a1_pred  k9a2_pred  \n",
       "0   0.170894  0.166699   0.164647   0.164472  \n",
       "1   0.170894  0.166699   0.164647   0.164472  \n",
       "2   0.170573  0.166688   0.164750   0.164515  \n",
       "3   0.170573  0.166688   0.164750   0.164515  \n",
       "4   0.170573  0.166688   0.164750   0.164515  \n",
       "5   0.170033  0.166921   0.164728   0.164951  \n",
       "6   0.169657  0.166924   0.164949   0.164941  \n",
       "7   0.170033  0.166921   0.164728   0.164951  \n",
       "8   0.170033  0.166921   0.164728   0.164951  \n",
       "9   0.170894  0.166699   0.164647   0.164472  \n",
       "10  0.170033  0.166921   0.164728   0.164951  \n",
       "11  0.170573  0.166688   0.164750   0.164515  \n",
       "12  0.169127  0.166780   0.165485   0.165035  \n",
       "13  0.170079  0.166906   0.164642   0.164839  \n",
       "14  0.169657  0.166924   0.164949   0.164941  \n",
       "15  0.170033  0.166921   0.164728   0.164951  \n",
       "16  0.170894  0.166699   0.164647   0.164472  \n",
       "17  0.169657  0.166924   0.164949   0.164941  \n",
       "18  0.169657  0.166924   0.164949   0.164941  \n",
       "19  0.170033  0.166921   0.164728   0.164951  \n",
       "20  0.168891  0.167102   0.165178   0.165380  \n",
       "21  0.169657  0.166924   0.164949   0.164941  \n",
       "22  0.170033  0.166921   0.164728   0.164951  \n",
       "23  0.170033  0.166921   0.164728   0.164951  \n",
       "24  0.169689  0.166610   0.165483   0.164532  \n",
       "25  0.170894  0.166699   0.164647   0.164472  \n",
       "26  0.170573  0.166688   0.164750   0.164515  \n",
       "27  0.169689  0.166610   0.165483   0.164532  \n",
       "28  0.168891  0.167102   0.165178   0.165380  \n",
       "29  0.168891  0.167102   0.165178   0.165380  \n",
       "30  0.170894  0.166699   0.164647   0.164472  \n",
       "31  0.169657  0.166924   0.164949   0.164941  \n",
       "32  0.170573  0.166688   0.164750   0.164515  \n",
       "33  0.170894  0.166699   0.164647   0.164472  \n",
       "34  0.169689  0.166610   0.165483   0.164532  \n",
       "35  0.168891  0.167102   0.165178   0.165380  \n",
       "36  0.170033  0.166921   0.164728   0.164951  \n",
       "37  0.169689  0.166610   0.165483   0.164532  \n",
       "38  0.170894  0.166699   0.164647   0.164472  \n",
       "39  0.169657  0.166924   0.164949   0.164941  \n",
       "40  0.170894  0.166699   0.164647   0.164472  \n",
       "41  0.169127  0.166780   0.165485   0.165035  \n",
       "42  0.170894  0.166699   0.164647   0.164472  \n",
       "43  0.169127  0.166780   0.165485   0.165035  \n",
       "44  0.168891  0.167102   0.165178   0.165380  \n",
       "45  0.169689  0.166610   0.165483   0.164532  \n",
       "46  0.170079  0.166906   0.164642   0.164839  \n",
       "47  0.169657  0.166924   0.164949   0.164941  \n",
       "48  0.170079  0.166906   0.164642   0.164839  \n",
       "49  0.170033  0.166921   0.164728   0.164951  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(y_test_overlap)\n",
    "delta_test_result=test_model(my_model,x_test_overlap,y_test_overlap,all_label_list)\n",
    "\n",
    "\n",
    "\n",
    "delta_test_result.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
