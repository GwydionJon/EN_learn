{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-e65b5c7c3502>:5: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import kerastuner as kt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_curve(epochs, hist, list_of_metrics,name):\n",
    "    \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch \"+name)\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "    for m in list_of_metrics:\n",
    "        x = hist[m]\n",
    "        plt.plot(epochs[1:], x[1:], label=m)\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "# for activation functions check https://keras.io/api/layers/activations/\n",
    "def create_model2(my_learning_rate,momentum,layers,my_metrics,my_act_function = \"softmax\"):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    #model.add(my_feature_layer)\n",
    "\n",
    "    for layer in layers:\n",
    "        model.add(tf.keras.layers.Dense(units = layer, activation = my_act_function))\n",
    "    model.add(tf.keras.layers.Dense(units=6,name='Output', activation = 'softmax'))                             \n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(lr=my_learning_rate,momentum=momentum),                                       \n",
    "                loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "                metrics=my_metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model,x_data, y_data, epochs, label_name,\n",
    "                batch_size=None,shuffle=True):\n",
    "    #features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    history = model.fit(x=x_data, y=y_data, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle,validation_split=0.2,\n",
    "                       #callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)])\n",
    "                       )\n",
    "  \n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    return epochs, hist\n",
    "    \n",
    "    \n",
    "#returns dataframe\n",
    "def test_model(model,x_data, y_data ,label_name):\n",
    "    evaluation=model.evaluate(x = x_data, y = y_data, batch_size=batch_size)\n",
    "    predicted = model.predict(x_data)\n",
    "    df_test=pd.DataFrame(y_data,columns=[label_name])\n",
    "   # print(predicted)\n",
    "    df_predict=pd.DataFrame(predicted,columns=[label+\"_pred\" for label in label_name])\n",
    "    return pd.concat([df_test,df_predict], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k6a1</th>\n",
       "      <th>k6a2</th>\n",
       "      <th>k11</th>\n",
       "      <th>k12</th>\n",
       "      <th>k9a1</th>\n",
       "      <th>k9a2</th>\n",
       "      <th>delta</th>\n",
       "      <th>lambda</th>\n",
       "      <th>overlap_s0_s2_k6a</th>\n",
       "      <th>overlap_s0_s2_k1</th>\n",
       "      <th>overlap_s0_s2_k9a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.5232, 0.3386, 0.1098, 0.0239, 0.004]</td>\n",
       "      <td>[0.7501, 0.2155, 0.031, 0.003, 0.0002]</td>\n",
       "      <td>[0.8102, 0.1704, 0.018, 0.0013, 0.0001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0333</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.5232, 0.3386, 0.1098, 0.0239, 0.004]</td>\n",
       "      <td>[0.7501, 0.2155, 0.031, 0.003, 0.0002]</td>\n",
       "      <td>[0.9769, 0.0228, 0.0003, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.5232, 0.3386, 0.1098, 0.0239, 0.004]</td>\n",
       "      <td>[0.7501, 0.2155, 0.031, 0.003, 0.0002]</td>\n",
       "      <td>[0.8102, 0.1704, 0.018, 0.0013, 0.0001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.5232, 0.3386, 0.1098, 0.0239, 0.004]</td>\n",
       "      <td>[0.7501, 0.2155, 0.031, 0.003, 0.0002]</td>\n",
       "      <td>[0.8102, 0.1704, 0.018, 0.0013, 0.0001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.5232, 0.3386, 0.1098, 0.0239, 0.004]</td>\n",
       "      <td>[0.7501, 0.2155, 0.031, 0.003, 0.0002]</td>\n",
       "      <td>[0.8102, 0.1704, 0.018, 0.0013, 0.0001]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k6a1  k6a2  k11  k12  k9a1    k9a2   delta  lambda  \\\n",
       "0  -0.1  -0.1 -0.1 -0.1  -0.1 -0.1000  0.6000  0.1000   \n",
       "1  -0.1  -0.1 -0.1 -0.1  -0.1 -0.0333  0.6000  0.2333   \n",
       "2  -0.1  -0.1 -0.1 -0.1  -0.1 -0.1000  0.6000  0.2333   \n",
       "3  -0.1  -0.1 -0.1 -0.1  -0.1 -0.1000  0.5333  0.2333   \n",
       "4  -0.1  -0.1 -0.1 -0.1  -0.1 -0.1000  0.5333  0.1000   \n",
       "\n",
       "                         overlap_s0_s2_k6a  \\\n",
       "0  [0.5232, 0.3386, 0.1098, 0.0239, 0.004]   \n",
       "1  [0.5232, 0.3386, 0.1098, 0.0239, 0.004]   \n",
       "2  [0.5232, 0.3386, 0.1098, 0.0239, 0.004]   \n",
       "3  [0.5232, 0.3386, 0.1098, 0.0239, 0.004]   \n",
       "4  [0.5232, 0.3386, 0.1098, 0.0239, 0.004]   \n",
       "\n",
       "                         overlap_s0_s2_k1  \\\n",
       "0  [0.7501, 0.2155, 0.031, 0.003, 0.0002]   \n",
       "1  [0.7501, 0.2155, 0.031, 0.003, 0.0002]   \n",
       "2  [0.7501, 0.2155, 0.031, 0.003, 0.0002]   \n",
       "3  [0.7501, 0.2155, 0.031, 0.003, 0.0002]   \n",
       "4  [0.7501, 0.2155, 0.031, 0.003, 0.0002]   \n",
       "\n",
       "                         overlap_s0_s2_k9a  \n",
       "0  [0.8102, 0.1704, 0.018, 0.0013, 0.0001]  \n",
       "1       [0.9769, 0.0228, 0.0003, 0.0, 0.0]  \n",
       "2  [0.8102, 0.1704, 0.018, 0.0013, 0.0001]  \n",
       "3  [0.8102, 0.1704, 0.018, 0.0013, 0.0001]  \n",
       "4  [0.8102, 0.1704, 0.018, 0.0013, 0.0001]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_labels_features=[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\",\"delta\",\"lambda\",\"overlap_s0_s2_k6a\",\"overlap_s0_s2_k1\",\"overlap_s0_s2_k9a\"]\n",
    "all_data=pd.read_csv(\"../../generated_Data/all_param_4_values_with_overlap.csv\")\n",
    "df_feature_labels=all_data[all_labels_features]\n",
    "\n",
    "max_no_of_peak_list=max(all_data[\"no_of_max\"])\n",
    "print(max_no_of_peak_list)\n",
    "df_feature_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65464, 3)\n",
      "(65464, 15)\n",
      "[[0.5232 0.3386 0.1098 ... 0.     0.     0.    ]\n",
      " [0.9307 0.0669 0.0024 ... 0.     0.     0.    ]\n",
      " [0.9307 0.0669 0.0024 ... 0.     0.     0.    ]\n",
      " ...\n",
      " [0.5232 0.3386 0.1098 ... 0.     0.     0.    ]\n",
      " [0.5232 0.3386 0.1098 ... 0.     0.     0.    ]\n",
      " [0.9307 0.0669 0.0024 ... 0.     0.     0.    ]]\n"
     ]
    }
   ],
   "source": [
    "overlap_s0_s2_k6a_array=np.asarray([  np.asarray([x for x in row.replace(\",\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"  \",\" \",5).replace(\" \",\";\").split(\";\") if x!=\"\"],dtype=np.float64)     for row in df_feature_labels[\"overlap_s0_s2_k6a\"] ])\n",
    "\n",
    "overlap_s0_s2_k1_array=np.asarray([  np.asarray([x for x in row.replace(\",\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"  \",\" \",5).replace(\" \",\";\").split(\";\") if x!=\"\"],dtype=np.float64)     for row in df_feature_labels[\"overlap_s0_s2_k1\"] ])\n",
    "\n",
    "overlap_s0_s2_k9a_array=np.asarray([  np.asarray([x for x in row.replace(\",\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"  \",\" \",5).replace(\" \",\";\").split(\";\") if x!=\"\"],dtype=np.float64)     for row in df_feature_labels[\"overlap_s0_s2_k9a\"] ])\n",
    "\n",
    "\n",
    "concat_feature=np.concatenate((overlap_s0_s2_k6a_array,overlap_s0_s2_k1_array,overlap_s0_s2_k9a_array),axis=1)\n",
    "omega_array=np.zeros((len(concat_feature),3))\n",
    "#w6a\n",
    "omega_array[:][0]=0.0740/27.211\n",
    "#w1\n",
    "omega_array[:][1]=0.1273/27.211\n",
    "#w9a\n",
    "omega_array[:][2]=0.1568/27.211\n",
    "\n",
    "print(omega_array.shape)\n",
    "print(concat_feature.shape)\n",
    "concat_feature=np.concatenate((concat_feature,omega_array),axis=1)\n",
    "\n",
    "\n",
    "concat_label=df_feature_labels[[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"]].abs().to_numpy()#,\"delta\",\"lambda\"\n",
    "\n",
    "\n",
    "x_train_overlap, x_test_overlap,y_train_overlap,y_test_overlap = train_test_split( concat_feature, concat_label  ,test_size=0.20, random_state=42)\n",
    "\n",
    "print(x_train_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "280/280 [==============================] - 6s 19ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 2/50\n",
      "280/280 [==============================] - 4s 16ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 3/50\n",
      "280/280 [==============================] - 4s 14ms/step - loss: 0.1001 - mean_absolute_error: 0.1001 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 4/50\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 5/50\n",
      "280/280 [==============================] - 4s 16ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 6/50\n",
      "280/280 [==============================] - 4s 16ms/step - loss: 0.0998 - mean_absolute_error: 0.0998 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 7/50\n",
      "280/280 [==============================] - 4s 15ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 8/50\n",
      "280/280 [==============================] - 5s 16ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 9/50\n",
      "280/280 [==============================] - 4s 16ms/step - loss: 0.1001 - mean_absolute_error: 0.1001 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 10/50\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 11/50\n",
      "280/280 [==============================] - 5s 16ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 12/50\n",
      "280/280 [==============================] - 5s 16ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 13/50\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 14/50\n",
      "280/280 [==============================] - 5s 18ms/step - loss: 0.0999 - mean_absolute_error: 0.0999 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 15/50\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 16/50\n",
      "280/280 [==============================] - 5s 16ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 17/50\n",
      "280/280 [==============================] - 5s 18ms/step - loss: 0.0999 - mean_absolute_error: 0.0999 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 18/50\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 19/50\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 20/50\n",
      "280/280 [==============================] - 5s 18ms/step - loss: 0.0999 - mean_absolute_error: 0.0999 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 21/50\n",
      "280/280 [==============================] - 5s 18ms/step - loss: 0.0999 - mean_absolute_error: 0.0999 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 22/50\n",
      "280/280 [==============================] - 5s 19ms/step - loss: 0.0999 - mean_absolute_error: 0.0999 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 23/50\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 24/50\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 0.0999 - mean_absolute_error: 0.0999 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 25/50\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 0.1001 - mean_absolute_error: 0.1001 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 26/50\n",
      "280/280 [==============================] - 6s 20ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 27/50\n",
      "280/280 [==============================] - 5s 18ms/step - loss: 0.1002 - mean_absolute_error: 0.1002 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 28/50\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 29/50\n",
      "280/280 [==============================] - 5s 19ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 30/50\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 0.0999 - mean_absolute_error: 0.0999 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 31/50\n",
      "280/280 [==============================] - 5s 18ms/step - loss: 0.0999 - mean_absolute_error: 0.0999 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 32/50\n",
      "280/280 [==============================] - 5s 18ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 33/50\n",
      "280/280 [==============================] - 5s 16ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 34/50\n",
      "280/280 [==============================] - 5s 19ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 35/50\n",
      "280/280 [==============================] - 5s 18ms/step - loss: 0.0999 - mean_absolute_error: 0.0999 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 36/50\n",
      "280/280 [==============================] - 6s 20ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 37/50\n",
      "280/280 [==============================] - 5s 16ms/step - loss: 0.0999 - mean_absolute_error: 0.0999 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 38/50\n",
      "280/280 [==============================] - 5s 16ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 39/50\n",
      "280/280 [==============================] - 6s 21ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 40/50\n",
      "280/280 [==============================] - 5s 16ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 41/50\n",
      "280/280 [==============================] - 5s 19ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 42/50\n",
      "280/280 [==============================] - 5s 19ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 43/50\n",
      "280/280 [==============================] - 5s 18ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 44/50\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 0.0999 - mean_absolute_error: 0.0999 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 45/50\n",
      "280/280 [==============================] - 5s 16ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 46/50\n",
      "280/280 [==============================] - 5s 17ms/step - loss: 0.0999 - mean_absolute_error: 0.0999 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 47/50\n",
      "280/280 [==============================] - 4s 13ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 48/50\n",
      "280/280 [==============================] - 4s 14ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 49/50\n",
      "280/280 [==============================] - 4s 15ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - 4s 15ms/step - loss: 0.1000 - mean_absolute_error: 0.1000 - val_loss: 0.1001 - val_mean_absolute_error: 0.1001\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "momentum=0.3\n",
    "epochs = 50\n",
    "batch_size = 150\n",
    "\n",
    "#specify the classification threshold\n",
    "classification_threshold = 0.15\n",
    "\n",
    "# Establish the metrics the model will measure.\n",
    "metric = [tf.keras.metrics.MeanAbsoluteError()]\n",
    "layers=[16,256,2048,512,64,16]\n",
    "\n",
    "\n",
    "all_label_list=[\"k6a1\",\"k6a2\",\"k11\",\"k12\",\"k9a1\",\"k9a2\"]#,\"delta\",\"lambda\"]\n",
    "\n",
    "my_model= create_model2(learning_rate,momentum,layers,metric,my_act_function=\"relu\")\n",
    "\n",
    "epochs_run, hist = train_model(my_model,x_train_overlap, y_train_overlap, epochs, \n",
    "                          all_label_list, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_metrics_to_plot = ['mean_absolute_error',\"val_mean_absolute_error\"] \n",
    "plot_curve(epochs_run, hist, list_of_metrics_to_plot,\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 1s 6ms/step - loss: 0.1002 - mean_absolute_error: 0.1002\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(k6a1,)</th>\n",
       "      <th>(k6a2,)</th>\n",
       "      <th>(k11,)</th>\n",
       "      <th>(k12,)</th>\n",
       "      <th>(k9a1,)</th>\n",
       "      <th>(k9a2,)</th>\n",
       "      <th>k6a1_pred</th>\n",
       "      <th>k6a2_pred</th>\n",
       "      <th>k11_pred</th>\n",
       "      <th>k12_pred</th>\n",
       "      <th>k9a1_pred</th>\n",
       "      <th>k9a2_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    (k6a1,)  (k6a2,)  (k11,)  (k12,)  (k9a1,)  (k9a2,)  k6a1_pred  k6a2_pred  \\\n",
       "0     0.033    0.033   0.100   0.100    0.033    0.033      0.167      0.168   \n",
       "1     0.033    0.033   0.033   0.100    0.100    0.033      0.167      0.168   \n",
       "2     0.100    0.033   0.033   0.033    0.100    0.033      0.167      0.168   \n",
       "3     0.033    0.033   0.033   0.033    0.100    0.033      0.167      0.168   \n",
       "4     0.033    0.033   0.100   0.033    0.033    0.033      0.167      0.168   \n",
       "5     0.033    0.033   0.033   0.100    0.100    0.100      0.166      0.168   \n",
       "6     0.100    0.100   0.033   0.033    0.033    0.033      0.167      0.168   \n",
       "7     0.100    0.033   0.100   0.100    0.033    0.100      0.166      0.168   \n",
       "8     0.033    0.033   0.100   0.100    0.100    0.100      0.166      0.168   \n",
       "9     0.033    0.033   0.100   0.100    0.100    0.033      0.167      0.168   \n",
       "10    0.100    0.033   0.033   0.100    0.100    0.100      0.166      0.168   \n",
       "11    0.100    0.033   0.033   0.033    0.100    0.033      0.167      0.168   \n",
       "12    0.100    0.100   0.033   0.100    0.100    0.100      0.166      0.168   \n",
       "13    0.100    0.033   0.100   0.033    0.033    0.100      0.166      0.168   \n",
       "14    0.033    0.100   0.033   0.033    0.033    0.033      0.167      0.168   \n",
       "15    0.100    0.033   0.033   0.100    0.033    0.100      0.166      0.168   \n",
       "16    0.100    0.033   0.033   0.100    0.100    0.033      0.167      0.168   \n",
       "17    0.100    0.100   0.033   0.033    0.100    0.033      0.167      0.168   \n",
       "18    0.033    0.100   0.033   0.033    0.033    0.033      0.167      0.168   \n",
       "19    0.033    0.033   0.033   0.100    0.100    0.100      0.166      0.168   \n",
       "20    0.033    0.100   0.100   0.033    0.033    0.100      0.166      0.168   \n",
       "21    0.100    0.100   0.100   0.033    0.100    0.033      0.167      0.168   \n",
       "22    0.033    0.033   0.033   0.100    0.100    0.100      0.166      0.168   \n",
       "23    0.100    0.033   0.033   0.100    0.100    0.100      0.166      0.168   \n",
       "24    0.033    0.100   0.033   0.100    0.100    0.033      0.167      0.168   \n",
       "25    0.100    0.033   0.100   0.100    0.100    0.033      0.167      0.168   \n",
       "26    0.100    0.033   0.033   0.033    0.100    0.033      0.167      0.168   \n",
       "27    0.100    0.100   0.033   0.100    0.100    0.033      0.167      0.168   \n",
       "28    0.100    0.100   0.033   0.033    0.033    0.100      0.166      0.168   \n",
       "29    0.100    0.100   0.033   0.033    0.100    0.100      0.166      0.168   \n",
       "30    0.033    0.033   0.100   0.100    0.033    0.033      0.167      0.168   \n",
       "31    0.033    0.100   0.100   0.033    0.033    0.033      0.167      0.168   \n",
       "32    0.033    0.033   0.033   0.033    0.033    0.033      0.167      0.168   \n",
       "33    0.033    0.033   0.033   0.100    0.033    0.033      0.167      0.168   \n",
       "34    0.100    0.100   0.033   0.100    0.033    0.033      0.167      0.168   \n",
       "35    0.033    0.100   0.033   0.033    0.033    0.100      0.166      0.168   \n",
       "36    0.033    0.033   0.100   0.100    0.033    0.100      0.166      0.168   \n",
       "37    0.033    0.100   0.033   0.100    0.033    0.033      0.167      0.168   \n",
       "38    0.100    0.033   0.100   0.100    0.100    0.033      0.167      0.168   \n",
       "39    0.100    0.100   0.100   0.033    0.033    0.033      0.167      0.168   \n",
       "40    0.033    0.033   0.100   0.100    0.100    0.033      0.167      0.168   \n",
       "41    0.100    0.100   0.100   0.100    0.100    0.100      0.166      0.168   \n",
       "42    0.033    0.033   0.033   0.100    0.033    0.033      0.167      0.168   \n",
       "43    0.100    0.100   0.100   0.100    0.100    0.100      0.166      0.168   \n",
       "44    0.033    0.100   0.033   0.033    0.033    0.100      0.166      0.168   \n",
       "45    0.100    0.100   0.033   0.100    0.033    0.033      0.167      0.168   \n",
       "46    0.033    0.033   0.033   0.033    0.100    0.100      0.166      0.168   \n",
       "47    0.100    0.100   0.033   0.033    0.033    0.033      0.167      0.168   \n",
       "48    0.033    0.033   0.100   0.033    0.033    0.100      0.166      0.168   \n",
       "49    0.033    0.033   0.033   0.100    0.033    0.100      0.166      0.168   \n",
       "\n",
       "    k11_pred  k12_pred  k9a1_pred  k9a2_pred  \n",
       "0      0.166     0.172      0.161      0.167  \n",
       "1      0.166     0.172      0.161      0.167  \n",
       "2      0.165     0.172      0.161      0.166  \n",
       "3      0.165     0.172      0.161      0.166  \n",
       "4      0.165     0.172      0.161      0.166  \n",
       "5      0.165     0.171      0.162      0.167  \n",
       "6      0.165     0.172      0.161      0.167  \n",
       "7      0.165     0.171      0.162      0.167  \n",
       "8      0.165     0.171      0.162      0.167  \n",
       "9      0.166     0.172      0.161      0.167  \n",
       "10     0.165     0.171      0.162      0.167  \n",
       "11     0.165     0.172      0.161      0.166  \n",
       "12     0.165     0.171      0.162      0.168  \n",
       "13     0.165     0.172      0.162      0.167  \n",
       "14     0.165     0.172      0.161      0.167  \n",
       "15     0.165     0.171      0.162      0.167  \n",
       "16     0.166     0.172      0.161      0.167  \n",
       "17     0.165     0.172      0.161      0.167  \n",
       "18     0.165     0.172      0.161      0.167  \n",
       "19     0.165     0.171      0.162      0.167  \n",
       "20     0.165     0.171      0.162      0.167  \n",
       "21     0.165     0.172      0.161      0.167  \n",
       "22     0.165     0.171      0.162      0.167  \n",
       "23     0.165     0.171      0.162      0.167  \n",
       "24     0.165     0.172      0.162      0.167  \n",
       "25     0.166     0.172      0.161      0.167  \n",
       "26     0.165     0.172      0.161      0.166  \n",
       "27     0.165     0.172      0.162      0.167  \n",
       "28     0.165     0.171      0.162      0.167  \n",
       "29     0.165     0.171      0.162      0.167  \n",
       "30     0.166     0.172      0.161      0.167  \n",
       "31     0.165     0.172      0.161      0.167  \n",
       "32     0.165     0.172      0.161      0.166  \n",
       "33     0.166     0.172      0.161      0.167  \n",
       "34     0.165     0.172      0.162      0.167  \n",
       "35     0.165     0.171      0.162      0.167  \n",
       "36     0.165     0.171      0.162      0.167  \n",
       "37     0.165     0.172      0.162      0.167  \n",
       "38     0.166     0.172      0.161      0.167  \n",
       "39     0.165     0.172      0.161      0.167  \n",
       "40     0.166     0.172      0.161      0.167  \n",
       "41     0.165     0.171      0.162      0.168  \n",
       "42     0.166     0.172      0.161      0.167  \n",
       "43     0.165     0.171      0.162      0.168  \n",
       "44     0.165     0.171      0.162      0.167  \n",
       "45     0.165     0.172      0.162      0.167  \n",
       "46     0.165     0.172      0.162      0.167  \n",
       "47     0.165     0.172      0.161      0.167  \n",
       "48     0.165     0.172      0.162      0.167  \n",
       "49     0.165     0.171      0.162      0.167  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(y_test_overlap)\n",
    "delta_test_result=test_model(my_model,x_test_overlap,y_test_overlap,all_label_list)\n",
    "\n",
    "\n",
    "\n",
    "delta_test_result.round(3).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
