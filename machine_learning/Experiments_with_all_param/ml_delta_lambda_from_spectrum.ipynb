{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import kerastuner as kt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_curve(epochs, hist, list_of_metrics,name):\n",
    "    \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch \"+name)\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "    for m in list_of_metrics:\n",
    "        x = hist[m]\n",
    "        plt.plot(epochs[1:], x[1:], label=m)\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "# for activation functions check https://keras.io/api/layers/activations/\n",
    "def create_model2(my_learning_rate,momentum,layers,my_metrics,my_act_function = \"softmax\"):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    #model.add(my_feature_layer)\n",
    "\n",
    "    for layer in layers:\n",
    "        model.add(tf.keras.layers.Dense(units = layer, activation = my_act_function))\n",
    "    model.add(tf.keras.layers.Dense(units=2,name='Output', activation = 'relu'))                             \n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(lr=my_learning_rate,momentum=momentum),                                       \n",
    "                loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "                metrics=my_metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model,x_data, y_data, epochs, label_name,\n",
    "                batch_size=None,shuffle=True):\n",
    "    #features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    history = model.fit(x=x_data, y=y_data, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle,validation_split=0.2)\n",
    "  \n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    return epochs, hist\n",
    "    \n",
    "    \n",
    "#returns dataframe\n",
    "def test_model(model,x_data, y_data ,label_name):\n",
    "    evaluation=model.evaluate(x = x_data, y = y_data, batch_size=batch_size)\n",
    "    predicted = model.predict(x_data)\n",
    "    df_test=pd.DataFrame(y_data,columns=[label_name])\n",
    "   # print(predicted)\n",
    "    df_predict=pd.DataFrame(predicted,columns=[label+\"_pred\" for label in label_name])\n",
    "    return pd.concat([df_test,df_predict], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delta</th>\n",
       "      <th>lambda</th>\n",
       "      <th>all_maxima</th>\n",
       "      <th>Intensity</th>\n",
       "      <th>no_of_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.466 0.54  0.612 0.67  0.742]</td>\n",
       "      <td>[104.475076  104.507572   87.7231385  58.12209...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.51  0.584 0.65  0.712 0.778]</td>\n",
       "      <td>[121.232278  120.055307   85.8745546  56.17655...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.482 0.556 0.628 0.686 0.756]</td>\n",
       "      <td>[101.883081  101.900808   85.5862194  56.79913...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.418 0.492 0.564 0.622 0.692]</td>\n",
       "      <td>[101.197996  101.175315   85.035938   56.43656...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.4   0.474 0.546 0.604 0.676]</td>\n",
       "      <td>[104.345828  104.372914   87.6137903  58.09441...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>[0.38  0.454 0.526 0.584 0.656]</td>\n",
       "      <td>[94.84709   94.8829634 79.6727722 52.8243582 3...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>[0.442 0.514 0.588 0.646 0.718]</td>\n",
       "      <td>[96.1280182 96.1175351 80.8515783 53.6785795 3...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>[0.468 0.542 0.614 0.672 0.744 0.816]</td>\n",
       "      <td>[91.252105  91.2117743 76.572234  50.7450912 3...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>[0.41  0.484 0.556 0.614 0.686]</td>\n",
       "      <td>[89.1194072 88.9942592 74.6345554 49.4436402 3...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.268 0.342 0.414 0.472 0.544]</td>\n",
       "      <td>[103.827555  103.893591   87.1849663  57.82552...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.334 0.408 0.48  0.538 0.61 ]</td>\n",
       "      <td>[104.157311  104.183192   87.4570665  57.97697...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.29  0.364 0.436 0.496 0.566 0.638]</td>\n",
       "      <td>[98.8701401 99.0057555 83.1453902 55.2118129 3...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.354 0.428 0.5   0.558 0.628]</td>\n",
       "      <td>[100.364204  100.365515   84.3101522  55.93780...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>[0.32  0.394 0.466 0.524 0.596 0.672]</td>\n",
       "      <td>[92.7031757 92.6793347 77.7670844 51.599828  3...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>[0.36  0.434 0.506 0.562 0.632]</td>\n",
       "      <td>[81.2879561 82.5636588 68.9138704 46.0688975 2...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.296 0.37  0.438 0.498 0.566]</td>\n",
       "      <td>[123.54248   122.417014   87.4921397  57.19550...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.362 0.436 0.504 0.564 0.632]</td>\n",
       "      <td>[123.965059  122.819058   87.7457244  57.33764...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.318 0.392 0.46  0.52  0.59 ]</td>\n",
       "      <td>[117.485767  116.491049   83.4535853  54.56008...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>[0.348 0.422 0.488 0.55  0.62 ]</td>\n",
       "      <td>[108.929698  107.91198    77.6999907  50.96291...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>[0.384 0.458 0.524 0.586 0.654]</td>\n",
       "      <td>[100.443158   99.1932805  71.2145776  46.19472...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>[0.504 0.578 0.65  0.708 0.78 ]</td>\n",
       "      <td>[97.6018571 97.5769523 81.9871619 54.3620625 3...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>[0.528 0.602 0.674 0.732 0.804 0.878]</td>\n",
       "      <td>[92.8615374 92.8586983 77.955023  51.6680754 3...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.494 0.568 0.636 0.696 0.764]</td>\n",
       "      <td>[124.365248  123.236709   88.031935   57.49532...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.428 0.502 0.568 0.63  0.698]</td>\n",
       "      <td>[124.266736  123.079621   87.9290589  57.45461...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.382 0.456 0.522 0.584 0.652]</td>\n",
       "      <td>[119.323723  118.140383   84.5873386  55.30480...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.446 0.52  0.586 0.648 0.714]</td>\n",
       "      <td>[120.400943  119.16121    85.3406215  55.78332...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>[0.408 0.482 0.548 0.61  0.678]</td>\n",
       "      <td>[112.548961  111.260268   79.911809   52.30489...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>[0.496 0.57  0.636 0.698 0.766]</td>\n",
       "      <td>[108.116505  107.036877   76.7831177  50.18255...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>[0.47  0.542 0.61  0.67  0.74 ]</td>\n",
       "      <td>[114.353858  113.452475   81.351911   53.11568...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>[0.436 0.51  0.578 0.638 0.706]</td>\n",
       "      <td>[103.897038  103.002545   74.545468   48.61250...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.362 0.436 0.504 0.564 0.632]</td>\n",
       "      <td>[124.030258  122.928397   87.7408224  57.30622...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.428 0.502 0.57  0.63  0.698]</td>\n",
       "      <td>[124.337294  123.153609   87.8886973  57.40124...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.446 0.52  0.588 0.648 0.716]</td>\n",
       "      <td>[120.76509   119.658843   85.296774   55.75875...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.384 0.456 0.524 0.584 0.654]</td>\n",
       "      <td>[119.64296   118.54964    84.5374223  55.21646...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>[0.47  0.544 0.612 0.672 0.742]</td>\n",
       "      <td>[115.48208   113.69492    81.1222056  52.84237...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>[0.41  0.484 0.55  0.612 0.68 ]</td>\n",
       "      <td>[113.170417  112.365058   79.4430182  52.18284...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>[0.44  0.512 0.58  0.64  0.708]</td>\n",
       "      <td>[100.619654  100.578202   73.802876   47.77822...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>[0.498 0.572 0.638 0.7   0.768]</td>\n",
       "      <td>[109.094262  108.081017   76.7646309  50.05138...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>[0.478 0.55  0.622 0.68  0.748]</td>\n",
       "      <td>[93.3855706 93.6601436 74.6204464 50.1083262 3...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.298 0.37  0.438 0.498 0.568]</td>\n",
       "      <td>[123.633005  122.447092   87.4735893  57.10564...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.32  0.394 0.46  0.522 0.592]</td>\n",
       "      <td>[117.727125  116.70696    83.2434564  54.36284...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>[0.352 0.426 0.49  0.554 0.62 ]</td>\n",
       "      <td>[107.266492  106.452474   76.7728698  50.12452...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>[0.556 0.63  0.696 0.758 0.826]</td>\n",
       "      <td>[110.184191  109.104995   78.2333902  51.07597...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.27  0.344 0.416 0.474 0.546]</td>\n",
       "      <td>[104.199369  104.426662   86.3768706  57.72268...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.51  0.584 0.652 0.712 0.78 ]</td>\n",
       "      <td>[121.435855  120.365209   85.8860934  56.13438...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>[0.532 0.606 0.672 0.734 0.802]</td>\n",
       "      <td>[116.093523  114.714493   82.3606021  53.69867...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>[0.388 0.462 0.528 0.588 0.656]</td>\n",
       "      <td>[99.9102185 99.3841546 70.4187518 45.8524281 2...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.494 0.568 0.636 0.696 0.764]</td>\n",
       "      <td>[124.453557  123.317149   88.0606377  57.44515...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>[0.532 0.606 0.674 0.734 0.804]</td>\n",
       "      <td>[116.470767  115.532158   82.4410856  53.74647...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.298 0.372 0.442 0.5   0.57 ]</td>\n",
       "      <td>[99.5160332 99.8921476 80.5465096 54.4890789 3...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     delta  lambda                             all_maxima  \\\n",
       "0   0.6000  0.1000        [0.466 0.54  0.612 0.67  0.742]   \n",
       "1   0.6000  0.2333        [0.51  0.584 0.65  0.712 0.778]   \n",
       "2   0.6000  0.2333        [0.482 0.556 0.628 0.686 0.756]   \n",
       "3   0.5333  0.2333        [0.418 0.492 0.564 0.622 0.692]   \n",
       "4   0.5333  0.1000        [0.4   0.474 0.546 0.604 0.676]   \n",
       "5   0.4667  0.3667        [0.38  0.454 0.526 0.584 0.656]   \n",
       "6   0.5333  0.3667        [0.442 0.514 0.588 0.646 0.718]   \n",
       "7   0.5333  0.5000  [0.468 0.542 0.614 0.672 0.744 0.816]   \n",
       "8   0.4667  0.5000        [0.41  0.484 0.556 0.614 0.686]   \n",
       "9   0.4000  0.1000        [0.268 0.342 0.414 0.472 0.544]   \n",
       "10  0.4667  0.1000        [0.334 0.408 0.48  0.538 0.61 ]   \n",
       "11  0.4000  0.2333  [0.29  0.364 0.436 0.496 0.566 0.638]   \n",
       "12  0.4667  0.2333        [0.354 0.428 0.5   0.558 0.628]   \n",
       "13  0.4000  0.3667  [0.32  0.394 0.466 0.524 0.596 0.672]   \n",
       "14  0.4000  0.5000        [0.36  0.434 0.506 0.562 0.632]   \n",
       "15  0.4000  0.1000        [0.296 0.37  0.438 0.498 0.566]   \n",
       "16  0.4667  0.1000        [0.362 0.436 0.504 0.564 0.632]   \n",
       "17  0.4000  0.2333        [0.318 0.392 0.46  0.52  0.59 ]   \n",
       "18  0.4000  0.3667        [0.348 0.422 0.488 0.55  0.62 ]   \n",
       "19  0.4000  0.5000        [0.384 0.458 0.524 0.586 0.654]   \n",
       "20  0.6000  0.3667        [0.504 0.578 0.65  0.708 0.78 ]   \n",
       "21  0.6000  0.5000  [0.528 0.602 0.674 0.732 0.804 0.878]   \n",
       "22  0.6000  0.1000        [0.494 0.568 0.636 0.696 0.764]   \n",
       "23  0.5333  0.1000        [0.428 0.502 0.568 0.63  0.698]   \n",
       "24  0.4667  0.2333        [0.382 0.456 0.522 0.584 0.652]   \n",
       "25  0.5333  0.2333        [0.446 0.52  0.586 0.648 0.714]   \n",
       "26  0.4667  0.3667        [0.408 0.482 0.548 0.61  0.678]   \n",
       "27  0.5333  0.5000        [0.496 0.57  0.636 0.698 0.766]   \n",
       "28  0.5333  0.3667        [0.47  0.542 0.61  0.67  0.74 ]   \n",
       "29  0.4667  0.5000        [0.436 0.51  0.578 0.638 0.706]   \n",
       "30  0.4667  0.1000        [0.362 0.436 0.504 0.564 0.632]   \n",
       "31  0.5333  0.1000        [0.428 0.502 0.57  0.63  0.698]   \n",
       "32  0.5333  0.2333        [0.446 0.52  0.588 0.648 0.716]   \n",
       "33  0.4667  0.2333        [0.384 0.456 0.524 0.584 0.654]   \n",
       "34  0.5333  0.3667        [0.47  0.544 0.612 0.672 0.742]   \n",
       "35  0.4667  0.3667        [0.41  0.484 0.55  0.612 0.68 ]   \n",
       "36  0.4667  0.5000        [0.44  0.512 0.58  0.64  0.708]   \n",
       "37  0.5333  0.5000        [0.498 0.572 0.638 0.7   0.768]   \n",
       "38  0.5333  0.5000        [0.478 0.55  0.622 0.68  0.748]   \n",
       "39  0.4000  0.1000        [0.298 0.37  0.438 0.498 0.568]   \n",
       "40  0.4000  0.2333        [0.32  0.394 0.46  0.522 0.592]   \n",
       "41  0.4000  0.3667        [0.352 0.426 0.49  0.554 0.62 ]   \n",
       "42  0.6000  0.5000        [0.556 0.63  0.696 0.758 0.826]   \n",
       "43  0.4000  0.1000        [0.27  0.344 0.416 0.474 0.546]   \n",
       "44  0.6000  0.2333        [0.51  0.584 0.652 0.712 0.78 ]   \n",
       "45  0.6000  0.3667        [0.532 0.606 0.672 0.734 0.802]   \n",
       "46  0.4000  0.5000        [0.388 0.462 0.528 0.588 0.656]   \n",
       "47  0.6000  0.1000        [0.494 0.568 0.636 0.696 0.764]   \n",
       "48  0.6000  0.3667        [0.532 0.606 0.674 0.734 0.804]   \n",
       "49  0.4000  0.2333        [0.298 0.372 0.442 0.5   0.57 ]   \n",
       "\n",
       "                                            Intensity  no_of_max  \n",
       "0   [104.475076  104.507572   87.7231385  58.12209...          5  \n",
       "1   [121.232278  120.055307   85.8745546  56.17655...          5  \n",
       "2   [101.883081  101.900808   85.5862194  56.79913...          5  \n",
       "3   [101.197996  101.175315   85.035938   56.43656...          5  \n",
       "4   [104.345828  104.372914   87.6137903  58.09441...          5  \n",
       "5   [94.84709   94.8829634 79.6727722 52.8243582 3...          5  \n",
       "6   [96.1280182 96.1175351 80.8515783 53.6785795 3...          5  \n",
       "7   [91.252105  91.2117743 76.572234  50.7450912 3...          6  \n",
       "8   [89.1194072 88.9942592 74.6345554 49.4436402 3...          5  \n",
       "9   [103.827555  103.893591   87.1849663  57.82552...          5  \n",
       "10  [104.157311  104.183192   87.4570665  57.97697...          5  \n",
       "11  [98.8701401 99.0057555 83.1453902 55.2118129 3...          6  \n",
       "12  [100.364204  100.365515   84.3101522  55.93780...          5  \n",
       "13  [92.7031757 92.6793347 77.7670844 51.599828  3...          6  \n",
       "14  [81.2879561 82.5636588 68.9138704 46.0688975 2...          5  \n",
       "15  [123.54248   122.417014   87.4921397  57.19550...          5  \n",
       "16  [123.965059  122.819058   87.7457244  57.33764...          5  \n",
       "17  [117.485767  116.491049   83.4535853  54.56008...          5  \n",
       "18  [108.929698  107.91198    77.6999907  50.96291...          5  \n",
       "19  [100.443158   99.1932805  71.2145776  46.19472...          5  \n",
       "20  [97.6018571 97.5769523 81.9871619 54.3620625 3...          5  \n",
       "21  [92.8615374 92.8586983 77.955023  51.6680754 3...          6  \n",
       "22  [124.365248  123.236709   88.031935   57.49532...          5  \n",
       "23  [124.266736  123.079621   87.9290589  57.45461...          5  \n",
       "24  [119.323723  118.140383   84.5873386  55.30480...          5  \n",
       "25  [120.400943  119.16121    85.3406215  55.78332...          5  \n",
       "26  [112.548961  111.260268   79.911809   52.30489...          5  \n",
       "27  [108.116505  107.036877   76.7831177  50.18255...          5  \n",
       "28  [114.353858  113.452475   81.351911   53.11568...          5  \n",
       "29  [103.897038  103.002545   74.545468   48.61250...          5  \n",
       "30  [124.030258  122.928397   87.7408224  57.30622...          5  \n",
       "31  [124.337294  123.153609   87.8886973  57.40124...          5  \n",
       "32  [120.76509   119.658843   85.296774   55.75875...          5  \n",
       "33  [119.64296   118.54964    84.5374223  55.21646...          5  \n",
       "34  [115.48208   113.69492    81.1222056  52.84237...          5  \n",
       "35  [113.170417  112.365058   79.4430182  52.18284...          5  \n",
       "36  [100.619654  100.578202   73.802876   47.77822...          5  \n",
       "37  [109.094262  108.081017   76.7646309  50.05138...          5  \n",
       "38  [93.3855706 93.6601436 74.6204464 50.1083262 3...          5  \n",
       "39  [123.633005  122.447092   87.4735893  57.10564...          5  \n",
       "40  [117.727125  116.70696    83.2434564  54.36284...          5  \n",
       "41  [107.266492  106.452474   76.7728698  50.12452...          5  \n",
       "42  [110.184191  109.104995   78.2333902  51.07597...          5  \n",
       "43  [104.199369  104.426662   86.3768706  57.72268...          5  \n",
       "44  [121.435855  120.365209   85.8860934  56.13438...          5  \n",
       "45  [116.093523  114.714493   82.3606021  53.69867...          5  \n",
       "46  [99.9102185 99.3841546 70.4187518 45.8524281 2...          5  \n",
       "47  [124.453557  123.317149   88.0606377  57.44515...          5  \n",
       "48  [116.470767  115.532158   82.4410856  53.74647...          5  \n",
       "49  [99.5160332 99.8921476 80.5465096 54.4890789 3...          5  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_labels_features=[\"delta\",\"lambda\",\"all_maxima\",\"Intensity\",\"no_of_max\"]\n",
    "all_data=pd.read_csv(\"G:\\OneDrive - bwedu\\Master\\Forschungspraktikum\\Inga\\pc-forschi\\generated_Data/all_param_4_values_with_overlap.csv\")\n",
    "df_feature_labels=all_data[all_labels_features]\n",
    "\n",
    "max_no_of_peak_list=max(all_data[\"no_of_max\"])\n",
    "print(max_no_of_peak_list)\n",
    "df_feature_labels.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "#convert string lists into numpy arrays in dict\n",
    "\n",
    "all_maxima_array=np.asarray([  np.asarray([x for x in row.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"  \",\" \",5).replace(\" \",\";\").split(\";\") if x!=\"\"],dtype=np.float64)  for row in df_feature_labels[\"all_maxima\"] ])\n",
    "\n",
    "intensity_array=np.asarray([  np.asarray([x for x in row.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"  \",\" \",5).replace(\" \",\";\").split(\";\") if x!=\"\"],dtype=np.float64) for row in df_feature_labels[\"Intensity\"] ])\n",
    "\n",
    "#pad all_maxima_array and intensity_array\n",
    "all_maxima_array_padded=np.zeros((len(all_maxima_array),max_no_of_peak_list))\n",
    "intensity_array_padded=np.zeros((len(intensity_array),max_no_of_peak_list))\n",
    "\n",
    "for i in range(len(all_maxima_array)):\n",
    "    for j in range(len(all_maxima_array[i])):\n",
    "        all_maxima_array_padded[i][j]=all_maxima_array[i][j]\n",
    "        intensity_array_padded[i][j]=intensity_array[i][j]\n",
    "\n",
    "\n",
    "\n",
    "no_of_max_array=df_feature_labels[\"no_of_max\"].to_numpy()\n",
    "delta_array=df_feature_labels[\"delta\"].to_numpy()\n",
    "\n",
    "lambda_array=df_feature_labels[\"lambda\"].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_label=np.zeros((len(delta_array),2))\n",
    "for i in range(len(delta_array)):\n",
    "    concat_label[i][0]=delta_array[i]\n",
    "    concat_label[i][1]=lambda_array[i]\n",
    "\n",
    "\n",
    "#concat_feature=np.concatenate((all_maxima_array_padded,intensity_array_padded),axis=1)\n",
    "concat_feature=no_of_max_array\n",
    "\n",
    "x_train, x_test,y_train,y_test = train_test_split( concat_feature, concat_label  ,test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.3450 - mean_absolute_error: 0.345 - ETA: 1s - loss: 0.1912 - mean_absolute_error: 0.191 - ETA: 1s - loss: 0.1503 - mean_absolute_error: 0.150 - ETA: 1s - loss: 0.1340 - mean_absolute_error: 0.134 - ETA: 1s - loss: 0.1256 - mean_absolute_error: 0.125 - ETA: 1s - loss: 0.1207 - mean_absolute_error: 0.120 - ETA: 1s - loss: 0.1174 - mean_absolute_error: 0.117 - ETA: 1s - loss: 0.1147 - mean_absolute_error: 0.114 - ETA: 1s - loss: 0.1126 - mean_absolute_error: 0.112 - ETA: 1s - loss: 0.1112 - mean_absolute_error: 0.111 - ETA: 1s - loss: 0.1102 - mean_absolute_error: 0.110 - ETA: 1s - loss: 0.1094 - mean_absolute_error: 0.109 - ETA: 0s - loss: 0.1085 - mean_absolute_error: 0.108 - ETA: 0s - loss: 0.1078 - mean_absolute_error: 0.107 - ETA: 0s - loss: 0.1072 - mean_absolute_error: 0.107 - ETA: 0s - loss: 0.1065 - mean_absolute_error: 0.106 - ETA: 0s - loss: 0.1059 - mean_absolute_error: 0.105 - ETA: 0s - loss: 0.1055 - mean_absolute_error: 0.105 - ETA: 0s - loss: 0.1053 - mean_absolute_error: 0.105 - ETA: 0s - loss: 0.1050 - mean_absolute_error: 0.105 - ETA: 0s - loss: 0.1047 - mean_absolute_error: 0.104 - ETA: 0s - loss: 0.1044 - mean_absolute_error: 0.104 - ETA: 0s - loss: 0.1041 - mean_absolute_error: 0.104 - ETA: 0s - loss: 0.1039 - mean_absolute_error: 0.103 - ETA: 0s - loss: 0.1036 - mean_absolute_error: 0.103 - ETA: 0s - loss: 0.1034 - mean_absolute_error: 0.103 - ETA: 0s - loss: 0.1033 - mean_absolute_error: 0.103 - ETA: 0s - loss: 0.1031 - mean_absolute_error: 0.103 - ETA: 0s - loss: 0.1030 - mean_absolute_error: 0.103 - ETA: 0s - loss: 0.1028 - mean_absolute_error: 0.102 - ETA: 0s - loss: 0.1027 - mean_absolute_error: 0.102 - 2s 7ms/step - loss: 0.1027 - mean_absolute_error: 0.1027 - val_loss: 0.0977 - val_mean_absolute_error: 0.0977\n",
      "Epoch 2/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0979 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0984 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0989 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0989 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0991 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0993 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0990 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0988 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0988 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0987 - mean_absolute_error: 0.098 - ETA: 0s - loss: 0.0985 - mean_absolute_error: 0.098 - ETA: 0s - loss: 0.0987 - mean_absolute_error: 0.098 - ETA: 0s - loss: 0.0988 - mean_absolute_error: 0.098 - ETA: 0s - loss: 0.0986 - mean_absolute_error: 0.098 - ETA: 0s - loss: 0.0986 - mean_absolute_error: 0.098 - ETA: 0s - loss: 0.0986 - mean_absolute_error: 0.098 - ETA: 0s - loss: 0.0985 - mean_absolute_error: 0.098 - ETA: 0s - loss: 0.0987 - mean_absolute_error: 0.098 - ETA: 0s - loss: 0.0987 - mean_absolute_error: 0.098 - ETA: 0s - loss: 0.0988 - mean_absolute_error: 0.098 - ETA: 0s - loss: 0.0989 - mean_absolute_error: 0.098 - ETA: 0s - loss: 0.0989 - mean_absolute_error: 0.098 - ETA: 0s - loss: 0.0990 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0992 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0991 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0990 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0991 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0990 - mean_absolute_error: 0.099 - ETA: 0s - loss: 0.0989 - mean_absolute_error: 0.098 - 2s 6ms/step - loss: 0.0989 - mean_absolute_error: 0.0989 - val_loss: 0.0978 - val_mean_absolute_error: 0.0978\n",
      "Epoch 3/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0951 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0975 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0979 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0975 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0975 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0976 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0979 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0979 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0979 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0979 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0979 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0979 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0977 - mean_absolute_error: 0.097 - 2s 6ms/step - loss: 0.0977 - mean_absolute_error: 0.0977 - val_loss: 0.0974 - val_mean_absolute_error: 0.0974\n",
      "Epoch 4/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0989 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0985 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0981 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0976 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0976 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0975 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0976 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0977 - mean_absolute_error: 0.097 - 2s 6ms/step - loss: 0.0977 - mean_absolute_error: 0.0977 - val_loss: 0.0977 - val_mean_absolute_error: 0.0977\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0991 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0982 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0980 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0975 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0980 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0976 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0975 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0975 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0975 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0973 - mean_absolute_error: 0.097 - 2s 7ms/step - loss: 0.0973 - mean_absolute_error: 0.0973 - val_loss: 0.0967 - val_mean_absolute_error: 0.0967\n",
      "Epoch 6/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0961 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0976 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0976 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0976 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0974 - mean_absolute_error: 0.097 - 2s 7ms/step - loss: 0.0974 - mean_absolute_error: 0.0974 - val_loss: 0.0969 - val_mean_absolute_error: 0.0969\n",
      "Epoch 7/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0983 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0980 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0969 - mean_absolute_error: 0.0969 - val_loss: 0.0966 - val_mean_absolute_error: 0.0966\n",
      "Epoch 8/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0983 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0960 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0968 - mean_absolute_error: 0.0968 - val_loss: 0.0969 - val_mean_absolute_error: 0.0969\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - ETA: 0s - loss: 0.0990 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0954 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0962 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0961 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - 2s 7ms/step - loss: 0.0970 - mean_absolute_error: 0.0970 - val_loss: 0.0981 - val_mean_absolute_error: 0.0981\n",
      "Epoch 10/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0995 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0953 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0969 - mean_absolute_error: 0.0969 - val_loss: 0.0969 - val_mean_absolute_error: 0.0969\n",
      "Epoch 11/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0975 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0969 - mean_absolute_error: 0.0969 - val_loss: 0.0972 - val_mean_absolute_error: 0.0972\n",
      "Epoch 12/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0921 - mean_absolute_error: 0.092 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0967 - mean_absolute_error: 0.0967 - val_loss: 0.0963 - val_mean_absolute_error: 0.0963\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - ETA: 0s - loss: 0.0993 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0984 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0958 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0956 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0956 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0958 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0960 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0959 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0959 - mean_absolute_error: 0.095 - ETA: 0s - loss: 0.0959 - mean_absolute_error: 0.095 - ETA: 0s - loss: 0.0960 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0961 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0961 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0962 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0968 - mean_absolute_error: 0.0968 - val_loss: 0.0965 - val_mean_absolute_error: 0.0965\n",
      "Epoch 14/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.1005 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0961 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0966 - mean_absolute_error: 0.0966 - val_loss: 0.0973 - val_mean_absolute_error: 0.0973\n",
      "Epoch 15/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0914 - mean_absolute_error: 0.091 - ETA: 1s - loss: 0.0980 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0960 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0960 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0962 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0967 - mean_absolute_error: 0.0967 - val_loss: 0.0962 - val_mean_absolute_error: 0.0962\n",
      "Epoch 16/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0975 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0967 - mean_absolute_error: 0.0967 - val_loss: 0.0963 - val_mean_absolute_error: 0.0963\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - ETA: 0s - loss: 0.0996 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0990 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0984 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0981 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0976 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0967 - mean_absolute_error: 0.0967 - val_loss: 0.0964 - val_mean_absolute_error: 0.0964\n",
      "Epoch 18/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0954 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0959 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0960 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0960 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0961 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0966 - mean_absolute_error: 0.0966 - val_loss: 0.0965 - val_mean_absolute_error: 0.0965\n",
      "Epoch 19/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0999 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0998 - mean_absolute_error: 0.099 - ETA: 1s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0962 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0965 - mean_absolute_error: 0.0965 - val_loss: 0.0965 - val_mean_absolute_error: 0.0965\n",
      "Epoch 20/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.1033 - mean_absolute_error: 0.103 - ETA: 1s - loss: 0.0982 - mean_absolute_error: 0.098 - ETA: 1s - loss: 0.0977 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0962 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0965 - mean_absolute_error: 0.0965 - val_loss: 0.0963 - val_mean_absolute_error: 0.0963\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0975 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0967 - mean_absolute_error: 0.0967 - val_loss: 0.0964 - val_mean_absolute_error: 0.0964\n",
      "Epoch 22/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0942 - mean_absolute_error: 0.094 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0958 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0957 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0960 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0960 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0962 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0962 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0965 - mean_absolute_error: 0.0965 - val_loss: 0.0966 - val_mean_absolute_error: 0.0966\n",
      "Epoch 23/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0960 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0954 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0956 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0959 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0961 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0961 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0961 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0965 - mean_absolute_error: 0.0965 - val_loss: 0.0963 - val_mean_absolute_error: 0.0963\n",
      "Epoch 24/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0939 - mean_absolute_error: 0.093 - ETA: 1s - loss: 0.0974 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0973 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0969 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - 2s 6ms/step - loss: 0.0966 - mean_absolute_error: 0.0966 - val_loss: 0.0962 - val_mean_absolute_error: 0.0962\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - ETA: 0s - loss: 0.1000 - mean_absolute_error: 0.100 - ETA: 1s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0978 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0965 - mean_absolute_error: 0.0965 - val_loss: 0.0963 - val_mean_absolute_error: 0.0963\n",
      "Epoch 26/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0888 - mean_absolute_error: 0.088 - ETA: 1s - loss: 0.0940 - mean_absolute_error: 0.094 - ETA: 1s - loss: 0.0949 - mean_absolute_error: 0.094 - ETA: 1s - loss: 0.0955 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0960 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0962 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0960 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0962 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0962 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0965 - mean_absolute_error: 0.0965 - val_loss: 0.0963 - val_mean_absolute_error: 0.0963\n",
      "Epoch 27/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0935 - mean_absolute_error: 0.093 - ETA: 1s - loss: 0.0957 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0954 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0954 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0961 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0962 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - 1s 5ms/step - loss: 0.0964 - mean_absolute_error: 0.0964 - val_loss: 0.0966 - val_mean_absolute_error: 0.0966\n",
      "Epoch 28/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.0943 - mean_absolute_error: 0.094 - ETA: 1s - loss: 0.0954 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0962 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - 1s 5ms/step - loss: 0.0964 - mean_absolute_error: 0.0964 - val_loss: 0.0962 - val_mean_absolute_error: 0.0962\n",
      "Epoch 29/30\n",
      "280/280 [==============================] - ETA: 0s - loss: 0.1008 - mean_absolute_error: 0.100 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0961 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - 1s 5ms/step - loss: 0.0964 - mean_absolute_error: 0.0964 - val_loss: 0.0960 - val_mean_absolute_error: 0.0960\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/280 [==============================] - ETA: 0s - loss: 0.0947 - mean_absolute_error: 0.094 - ETA: 1s - loss: 0.0958 - mean_absolute_error: 0.095 - ETA: 1s - loss: 0.0960 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0961 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 1s - loss: 0.0968 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 1s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0963 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0964 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0966 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0965 - mean_absolute_error: 0.096 - 2s 7ms/step - loss: 0.0965 - mean_absolute_error: 0.0965 - val_loss: 0.0959 - val_mean_absolute_error: 0.0959\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "momentum=0.9\n",
    "epochs = 30\n",
    "batch_size = 150\n",
    "\n",
    "#specify the classification threshold\n",
    "classification_threshold = 0.15\n",
    "\n",
    "# Establish the metrics the model will measure.\n",
    "metric = [tf.keras.metrics.MeanAbsoluteError()]\n",
    "layers=[16,256,1024,64,16]\n",
    "\n",
    "\n",
    "all_label_list=[\"delta\",\"lambda\"]\n",
    "\n",
    "my_model= create_model2(learning_rate,momentum,layers,metric,my_act_function=\"relu\")\n",
    "\n",
    "#lambda_train, lambda_test\n",
    "#delta_train, delta_test\n",
    "\n",
    "epochs_run, hist = train_model(my_model,x_train, y_train, epochs, \n",
    "                          all_label_list, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_metrics_to_plot = ['mean_absolute_error',\"val_mean_absolute_error\"] \n",
    "plot_curve(epochs_run, hist, list_of_metrics_to_plot,\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0967 - mean_absolute_error: 0.096 - ETA: 0s - loss: 0.0971 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0972 - mean_absolute_error: 0.097 - ETA: 0s - loss: 0.0970 - mean_absolute_error: 0.097 - 0s 3ms/step - loss: 0.0971 - mean_absolute_error: 0.0971\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(delta,)</th>\n",
       "      <th>(lambda,)</th>\n",
       "      <th>delta_pred</th>\n",
       "      <th>lambda_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.464688</td>\n",
       "      <td>0.353067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.467815</td>\n",
       "      <td>0.291051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.467815</td>\n",
       "      <td>0.291051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.467815</td>\n",
       "      <td>0.291051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.464688</td>\n",
       "      <td>0.353067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>0.530630</td>\n",
       "      <td>0.233357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.464688</td>\n",
       "      <td>0.353067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.504424</td>\n",
       "      <td>0.250312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.464688</td>\n",
       "      <td>0.353067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.504424</td>\n",
       "      <td>0.250312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>0.504424</td>\n",
       "      <td>0.250312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.504424</td>\n",
       "      <td>0.250312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.464688</td>\n",
       "      <td>0.353067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>0.464688</td>\n",
       "      <td>0.353067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.530630</td>\n",
       "      <td>0.233357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.504424</td>\n",
       "      <td>0.250312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.467815</td>\n",
       "      <td>0.291051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.464688</td>\n",
       "      <td>0.353067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.504424</td>\n",
       "      <td>0.250312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.530630</td>\n",
       "      <td>0.233357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.464688</td>\n",
       "      <td>0.353067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.504424</td>\n",
       "      <td>0.250312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.530630</td>\n",
       "      <td>0.233357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.504424</td>\n",
       "      <td>0.250312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>0.530630</td>\n",
       "      <td>0.233357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.4667</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.467815</td>\n",
       "      <td>0.291051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.3667</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.267394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    (delta,)  (lambda,)  delta_pred  lambda_pred\n",
       "0     0.4667     0.5000    0.464688     0.353067\n",
       "1     0.6000     0.5000    0.478049     0.267394\n",
       "2     0.6000     0.5000    0.467815     0.291051\n",
       "3     0.4667     0.3667    0.467815     0.291051\n",
       "4     0.5333     0.3667    0.467815     0.291051\n",
       "5     0.4000     0.1000    0.478049     0.267394\n",
       "6     0.4667     0.5000    0.478049     0.267394\n",
       "7     0.4000     0.3667    0.478049     0.267394\n",
       "8     0.4000     0.2333    0.478049     0.267394\n",
       "9     0.4667     0.5000    0.478049     0.267394\n",
       "10    0.4000     0.2333    0.478049     0.267394\n",
       "11    0.5333     0.5000    0.464688     0.353067\n",
       "12    0.6000     0.2333    0.530630     0.233357\n",
       "13    0.4667     0.3667    0.464688     0.353067\n",
       "14    0.4667     0.1000    0.504424     0.250312\n",
       "15    0.4667     0.1000    0.478049     0.267394\n",
       "16    0.5333     0.2333    0.478049     0.267394\n",
       "17    0.5333     0.2333    0.478049     0.267394\n",
       "18    0.6000     0.5000    0.478049     0.267394\n",
       "19    0.4667     0.5000    0.464688     0.353067\n",
       "20    0.4667     0.5000    0.504424     0.250312\n",
       "21    0.4667     0.2333    0.504424     0.250312\n",
       "22    0.5333     0.2333    0.478049     0.267394\n",
       "23    0.4667     0.1000    0.478049     0.267394\n",
       "24    0.4667     0.3667    0.504424     0.250312\n",
       "25    0.4000     0.5000    0.464688     0.353067\n",
       "26    0.4000     0.2333    0.464688     0.353067\n",
       "27    0.6000     0.3667    0.530630     0.233357\n",
       "28    0.4667     0.5000    0.478049     0.267394\n",
       "29    0.6000     0.3667    0.504424     0.250312\n",
       "30    0.6000     0.1000    0.478049     0.267394\n",
       "31    0.5333     0.3667    0.478049     0.267394\n",
       "32    0.5333     0.3667    0.467815     0.291051\n",
       "33    0.4667     0.3667    0.464688     0.353067\n",
       "34    0.4000     0.1000    0.478049     0.267394\n",
       "35    0.4000     0.5000    0.504424     0.250312\n",
       "36    0.4000     0.1000    0.478049     0.267394\n",
       "37    0.6000     0.1000    0.530630     0.233357\n",
       "38    0.4000     0.3667    0.478049     0.267394\n",
       "39    0.6000     0.3667    0.478049     0.267394\n",
       "40    0.6000     0.5000    0.464688     0.353067\n",
       "41    0.4000     0.3667    0.504424     0.250312\n",
       "42    0.5333     0.3667    0.478049     0.267394\n",
       "43    0.4667     0.1000    0.530630     0.233357\n",
       "44    0.5333     0.3667    0.504424     0.250312\n",
       "45    0.6000     0.2333    0.530630     0.233357\n",
       "46    0.4667     0.5000    0.467815     0.291051\n",
       "47    0.4000     0.2333    0.478049     0.267394\n",
       "48    0.5333     0.1000    0.478049     0.267394\n",
       "49    0.6000     0.3667    0.478049     0.267394"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_test_result=test_model(my_model,x_test,y_test,all_label_list)\n",
    "\n",
    "\n",
    "delta_test_result.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
