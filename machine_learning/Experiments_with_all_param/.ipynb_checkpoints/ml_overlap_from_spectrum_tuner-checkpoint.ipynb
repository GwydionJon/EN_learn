{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "H:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import kerastuner as kt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_curve(epochs, hist, list_of_metrics,name):\n",
    "    \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch \"+name)\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "    for m in list_of_metrics:\n",
    "        x = hist[m]\n",
    "        plt.plot(epochs[1:], x[1:], label=m)\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "def create_model_optimizer(hp):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "\n",
    "    first_layer=hp.Int(str('1_units'+str(i)), min_value = 32, max_value = 64, step = 8)\n",
    "    model.add(tf.keras.layers.Dense(units = first_layer, activation = 'relu'))\n",
    "    \n",
    "    second_layer=hp.Int(str('2_units'+str(i)), min_value = 64, max_value = 256, step = 16)\n",
    "    model.add(tf.keras.layers.Dense(units = second_layer, activation = 'relu'))\n",
    "\n",
    "    third_layer=hp.Int(str('3_units'+str(i)), min_value = 128, max_value = 2048, step = 128)\n",
    "    model.add(tf.keras.layers.Dense(units = third_layer, activation = 'relu'))\n",
    "\n",
    "    fourth_layer=hp.Int(str('4_units'+str(i)), min_value = 512, max_value = 4096, step = 256)\n",
    "    model.add(tf.keras.layers.Dense(units = fourth_layer, activation = 'relu'))\n",
    "\n",
    "    fith_layer=hp.Int(str('5_units'+str(i)), min_value = 128, max_value = 512, step = 32)\n",
    "    model.add(tf.keras.layers.Dense(units = fith_layer, activation = 'relu'))\n",
    "\n",
    "\n",
    "    six_layer=hp.Int(str('6_units'+str(i)), min_value = 32, max_value = 128, step = 16)\n",
    "    model.add(tf.keras.layers.Dense(units = six_layer, activation = 'relu'))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    hp_lr=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    hp_momentum=hp.Choice('momentum', values=[1e-2, 1e-1, 2e-1,5e-1])\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=15,name='Output', activation = 'relu'))                             \n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(lr=hp_lr,momentum=hp_momentum),                                       \n",
    "                loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model,x_data, y_data, epochs, label_name,\n",
    "                batch_size=None,shuffle=True):\n",
    "    #features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    history = model.fit(x=x_data, y=y_data, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle,validation_split=0.2,\n",
    "                       callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)])\n",
    "  \n",
    "    epochs = history.epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    return epochs, hist\n",
    "    \n",
    "    \n",
    "#returns dataframe\n",
    "def test_model(model,x_data, y_data ,label_name):\n",
    "    evaluation=model.evaluate(x = x_data, y = y_data, batch_size=100)\n",
    "    predicted = model.predict(x_data)\n",
    "    df_test=pd.DataFrame(y_data,columns=[label_name])\n",
    "   # print(predicted)\n",
    "    df_predict=pd.DataFrame(predicted,columns=[label+\"_pred\" for label in label_name])\n",
    "    return pd.concat([df_test,df_predict], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delta</th>\n",
       "      <th>lambda</th>\n",
       "      <th>all_maxima</th>\n",
       "      <th>Intensity</th>\n",
       "      <th>overlap_s0_s2_k6a</th>\n",
       "      <th>overlap_s0_s2_k1</th>\n",
       "      <th>overlap_s0_s2_k9a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.466 0.54  0.612 0.67  0.742]</td>\n",
       "      <td>[104.475076  104.507572   87.7231385  58.12209...</td>\n",
       "      <td>[0.5232, 0.3386, 0.1098, 0.0239, 0.004]</td>\n",
       "      <td>[0.7501, 0.2155, 0.031, 0.003, 0.0002]</td>\n",
       "      <td>[0.8102, 0.1704, 0.018, 0.0013, 0.0001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.51  0.584 0.65  0.712 0.778]</td>\n",
       "      <td>[121.232278  120.055307   85.8745546  56.17655...</td>\n",
       "      <td>[0.5232, 0.3386, 0.1098, 0.0239, 0.004]</td>\n",
       "      <td>[0.7501, 0.2155, 0.031, 0.003, 0.0002]</td>\n",
       "      <td>[0.9769, 0.0228, 0.0003, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.482 0.556 0.628 0.686 0.756]</td>\n",
       "      <td>[101.883081  101.900808   85.5862194  56.79913...</td>\n",
       "      <td>[0.5232, 0.3386, 0.1098, 0.0239, 0.004]</td>\n",
       "      <td>[0.7501, 0.2155, 0.031, 0.003, 0.0002]</td>\n",
       "      <td>[0.8102, 0.1704, 0.018, 0.0013, 0.0001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>[0.418 0.492 0.564 0.622 0.692]</td>\n",
       "      <td>[101.197996  101.175315   85.035938   56.43656...</td>\n",
       "      <td>[0.5232, 0.3386, 0.1098, 0.0239, 0.004]</td>\n",
       "      <td>[0.7501, 0.2155, 0.031, 0.003, 0.0002]</td>\n",
       "      <td>[0.8102, 0.1704, 0.018, 0.0013, 0.0001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5333</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>[0.4   0.474 0.546 0.604 0.676]</td>\n",
       "      <td>[104.345828  104.372914   87.6137903  58.09441...</td>\n",
       "      <td>[0.5232, 0.3386, 0.1098, 0.0239, 0.004]</td>\n",
       "      <td>[0.7501, 0.2155, 0.031, 0.003, 0.0002]</td>\n",
       "      <td>[0.8102, 0.1704, 0.018, 0.0013, 0.0001]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    delta  lambda                       all_maxima  \\\n",
       "0  0.6000  0.1000  [0.466 0.54  0.612 0.67  0.742]   \n",
       "1  0.6000  0.2333  [0.51  0.584 0.65  0.712 0.778]   \n",
       "2  0.6000  0.2333  [0.482 0.556 0.628 0.686 0.756]   \n",
       "3  0.5333  0.2333  [0.418 0.492 0.564 0.622 0.692]   \n",
       "4  0.5333  0.1000  [0.4   0.474 0.546 0.604 0.676]   \n",
       "\n",
       "                                           Intensity  \\\n",
       "0  [104.475076  104.507572   87.7231385  58.12209...   \n",
       "1  [121.232278  120.055307   85.8745546  56.17655...   \n",
       "2  [101.883081  101.900808   85.5862194  56.79913...   \n",
       "3  [101.197996  101.175315   85.035938   56.43656...   \n",
       "4  [104.345828  104.372914   87.6137903  58.09441...   \n",
       "\n",
       "                         overlap_s0_s2_k6a  \\\n",
       "0  [0.5232, 0.3386, 0.1098, 0.0239, 0.004]   \n",
       "1  [0.5232, 0.3386, 0.1098, 0.0239, 0.004]   \n",
       "2  [0.5232, 0.3386, 0.1098, 0.0239, 0.004]   \n",
       "3  [0.5232, 0.3386, 0.1098, 0.0239, 0.004]   \n",
       "4  [0.5232, 0.3386, 0.1098, 0.0239, 0.004]   \n",
       "\n",
       "                         overlap_s0_s2_k1  \\\n",
       "0  [0.7501, 0.2155, 0.031, 0.003, 0.0002]   \n",
       "1  [0.7501, 0.2155, 0.031, 0.003, 0.0002]   \n",
       "2  [0.7501, 0.2155, 0.031, 0.003, 0.0002]   \n",
       "3  [0.7501, 0.2155, 0.031, 0.003, 0.0002]   \n",
       "4  [0.7501, 0.2155, 0.031, 0.003, 0.0002]   \n",
       "\n",
       "                         overlap_s0_s2_k9a  \n",
       "0  [0.8102, 0.1704, 0.018, 0.0013, 0.0001]  \n",
       "1       [0.9769, 0.0228, 0.0003, 0.0, 0.0]  \n",
       "2  [0.8102, 0.1704, 0.018, 0.0013, 0.0001]  \n",
       "3  [0.8102, 0.1704, 0.018, 0.0013, 0.0001]  \n",
       "4  [0.8102, 0.1704, 0.018, 0.0013, 0.0001]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_labels_features=[\"delta\",\"lambda\",\"all_maxima\",\"Intensity\",\"overlap_s0_s2_k6a\",\"overlap_s0_s2_k1\",\"overlap_s0_s2_k9a\"]\n",
    "all_data=pd.read_csv(\"G:\\OneDrive - bwedu\\Master\\Forschungspraktikum\\Inga\\pc-forschi\\generated_Data/all_param_4_values_with_overlap.csv\")\n",
    "df_feature_labels=all_data[all_labels_features]\n",
    "\n",
    "max_no_of_peak_list=max(all_data[\"no_of_max\"])\n",
    "print(max_no_of_peak_list)\n",
    "df_feature_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert string lists into numpy arrays in dict\n",
    "\n",
    "all_maxima_array=np.asarray([  np.asarray([x for x in row.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"  \",\" \",5).replace(\" \",\";\").split(\";\") if x!=\"\"],dtype=np.float64)     for row in df_feature_labels[\"all_maxima\"] ])\n",
    "\n",
    "intensity_array=np.asarray([  np.asarray([x for x in row.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"  \",\" \",5).replace(\" \",\";\").split(\";\") if x!=\"\"],dtype=np.float64)     for row in df_feature_labels[\"Intensity\"] ])\n",
    "\n",
    "overlap_s0_s2_k6a_array=np.asarray([  np.asarray([x for x in row.replace(\",\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"  \",\" \",5).replace(\" \",\";\").split(\";\") if x!=\"\"],dtype=np.float64)     for row in df_feature_labels[\"overlap_s0_s2_k6a\"] ])\n",
    "\n",
    "overlap_s0_s2_k1_array=np.asarray([  np.asarray([x for x in row.replace(\",\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"  \",\" \",5).replace(\" \",\";\").split(\";\") if x!=\"\"],dtype=np.float64)     for row in df_feature_labels[\"overlap_s0_s2_k1\"] ])\n",
    "\n",
    "overlap_s0_s2_k9a_array=np.asarray([  np.asarray([x for x in row.replace(\",\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"  \",\" \",5).replace(\" \",\";\").split(\";\") if x!=\"\"],dtype=np.float64)     for row in df_feature_labels[\"overlap_s0_s2_k9a\"] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad all_maxima_array and intensity_array\n",
    "all_maxima_array_padded=np.zeros((len(all_maxima_array),max_no_of_peak_list))\n",
    "intensity_array_padded=np.zeros((len(intensity_array),max_no_of_peak_list))\n",
    "\n",
    "for i in range(len(all_maxima_array)):\n",
    "    for j in range(len(all_maxima_array[i])):\n",
    "        all_maxima_array_padded[i][j]=all_maxima_array[i][j]\n",
    "        intensity_array_padded[i][j]=intensity_array[i][j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_label=np.concatenate((overlap_s0_s2_k6a_array,overlap_s0_s2_k1_array,overlap_s0_s2_k9a_array),axis=1)\n",
    "concat_feature=np.concatenate((all_maxima_array_padded,intensity_array_padded),axis=1)\n",
    "\n",
    "x_train, x_test,y_train,y_test = train_test_split( concat_feature, concat_label  ,test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project franck_condon_test\\third Try\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from franck_condon_test\\third Try\\tuner0.json\n",
      "Epoch 1/10\n",
      " 742/1637 [============>.................] - ETA: 7s - loss: 0.0914 - mean_absolute_error: 0.0914"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-bf09feee709f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mH:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\kerastuner\\engine\\multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcopied_fit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'min'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mH:\\Users\\Gwydion\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_TRIALS = 20\n",
    "\n",
    "EXECUTIONS_PER_TRIAL = 3\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "\n",
    "    create_model_optimizer,\n",
    "\n",
    "    objective='val_mean_absolute_error',\n",
    "\n",
    "    max_trials=MAX_TRIALS,\n",
    "\n",
    "    executions_per_trial=EXECUTIONS_PER_TRIAL,\n",
    "\n",
    "    directory='franck_condon_test',\n",
    "    #overwrite = True,\n",
    "    project_name='third Try',\n",
    "\n",
    "    seed=1\n",
    "\n",
    ")\n",
    "\n",
    "tuner.search(x_train, y_train, epochs = 10, validation_data = (x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tuner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ae7185f22d37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tuner' is not defined"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_units65463': 48,\n",
       " '2_units65463': 96,\n",
       " '3_units65463': 704,\n",
       " '4_units65463': 1920,\n",
       " '5_units65463': 192,\n",
       " '6_units65463': 128,\n",
       " 'learning_rate': 0.001,\n",
       " 'momentum': 0.5}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.oracle.get_best_trials(num_trials=2)[0].hyperparameters.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_units65463': 48,\n",
       " '2_units65463': 176,\n",
       " '3_units65463': 512,\n",
       " '4_units65463': 1280,\n",
       " '5_units65463': 224,\n",
       " '6_units65463': 48,\n",
       " 'learning_rate': 0.01,\n",
       " 'momentum': 0.2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.oracle.get_best_trials(num_trials=2)[1].hyperparameters.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 0s 3ms/step - loss: 0.0445 - mean_absolute_error: 0.0445\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(k9a 0,)</th>\n",
       "      <th>(k9a 1,)</th>\n",
       "      <th>(k9a 2,)</th>\n",
       "      <th>(k9a 3,)</th>\n",
       "      <th>(k9a 4,)</th>\n",
       "      <th>k9a 0_pred</th>\n",
       "      <th>k9a 1_pred</th>\n",
       "      <th>k9a 2_pred</th>\n",
       "      <th>k9a 3_pred</th>\n",
       "      <th>k9a 4_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.793629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.967414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.001015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.013203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.012567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.863990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.978746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.834449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.820393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.838678</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.922924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.956972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.860311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.876602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.035301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.869383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.060711</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.043651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.959764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.783623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.776595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.996668</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.885969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.865972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.854916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.798870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.043344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.975013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.830285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.898740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.057127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.005518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.079389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.016672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.010874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.796270</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.858081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.030506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.957816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.055393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.945377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.861462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.976305</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.851392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.841589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.037975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.783120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.9769</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.058437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.885548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.8102</td>\n",
       "      <td>0.1704</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.856841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    (k9a 0,)  (k9a 1,)  (k9a 2,)  (k9a 3,)  (k9a 4,)  k9a 0_pred  k9a 1_pred  \\\n",
       "0     0.9769    0.0228    0.0003    0.0000    0.0000    0.793629         0.0   \n",
       "1     0.9769    0.0228    0.0003    0.0000    0.0000    0.967414         0.0   \n",
       "2     0.9769    0.0228    0.0003    0.0000    0.0000    1.001015         0.0   \n",
       "3     0.9769    0.0228    0.0003    0.0000    0.0000    1.013203         0.0   \n",
       "4     0.9769    0.0228    0.0003    0.0000    0.0000    1.012567         0.0   \n",
       "5     0.8102    0.1704    0.0180    0.0013    0.0001    0.863990         0.0   \n",
       "6     0.9769    0.0228    0.0003    0.0000    0.0000    0.978746         0.0   \n",
       "7     0.8102    0.1704    0.0180    0.0013    0.0001    0.834449         0.0   \n",
       "8     0.8102    0.1704    0.0180    0.0013    0.0001    0.820393         0.0   \n",
       "9     0.9769    0.0228    0.0003    0.0000    0.0000    0.838678         0.0   \n",
       "10    0.8102    0.1704    0.0180    0.0013    0.0001    0.922924         0.0   \n",
       "11    0.9769    0.0228    0.0003    0.0000    0.0000    0.956972         0.0   \n",
       "12    0.8102    0.1704    0.0180    0.0013    0.0001    0.860311         0.0   \n",
       "13    0.8102    0.1704    0.0180    0.0013    0.0001    0.876602         0.0   \n",
       "14    0.9769    0.0228    0.0003    0.0000    0.0000    1.035301         0.0   \n",
       "15    0.8102    0.1704    0.0180    0.0013    0.0001    0.869383         0.0   \n",
       "16    0.9769    0.0228    0.0003    0.0000    0.0000    1.060711         0.0   \n",
       "17    0.9769    0.0228    0.0003    0.0000    0.0000    1.043651         0.0   \n",
       "18    0.9769    0.0228    0.0003    0.0000    0.0000    0.959764         0.0   \n",
       "19    0.8102    0.1704    0.0180    0.0013    0.0001    0.783623         0.0   \n",
       "20    0.8102    0.1704    0.0180    0.0013    0.0001    0.776595         0.0   \n",
       "21    0.9769    0.0228    0.0003    0.0000    0.0000    0.996668         0.0   \n",
       "22    0.8102    0.1704    0.0180    0.0013    0.0001    0.885969         0.0   \n",
       "23    0.8102    0.1704    0.0180    0.0013    0.0001    0.865972         0.0   \n",
       "24    0.9769    0.0228    0.0003    0.0000    0.0000    0.854916         0.0   \n",
       "25    0.9769    0.0228    0.0003    0.0000    0.0000    0.798870         0.0   \n",
       "26    0.9769    0.0228    0.0003    0.0000    0.0000    1.043344         0.0   \n",
       "27    0.9769    0.0228    0.0003    0.0000    0.0000    0.975013         0.0   \n",
       "28    0.8102    0.1704    0.0180    0.0013    0.0001    0.830285         0.0   \n",
       "29    0.8102    0.1704    0.0180    0.0013    0.0001    0.898740         0.0   \n",
       "30    0.9769    0.0228    0.0003    0.0000    0.0000    1.057127         0.0   \n",
       "31    0.9769    0.0228    0.0003    0.0000    0.0000    1.005518         0.0   \n",
       "32    0.9769    0.0228    0.0003    0.0000    0.0000    1.079389         0.0   \n",
       "33    0.9769    0.0228    0.0003    0.0000    0.0000    1.016672         0.0   \n",
       "34    0.9769    0.0228    0.0003    0.0000    0.0000    1.010874         0.0   \n",
       "35    0.8102    0.1704    0.0180    0.0013    0.0001    0.796270         0.0   \n",
       "36    0.8102    0.1704    0.0180    0.0013    0.0001    0.858081         0.0   \n",
       "37    0.9769    0.0228    0.0003    0.0000    0.0000    1.030506         0.0   \n",
       "38    0.9769    0.0228    0.0003    0.0000    0.0000    0.957816         0.0   \n",
       "39    0.9769    0.0228    0.0003    0.0000    0.0000    1.055393         0.0   \n",
       "40    0.9769    0.0228    0.0003    0.0000    0.0000    0.945377         0.0   \n",
       "41    0.8102    0.1704    0.0180    0.0013    0.0001    0.861462         0.0   \n",
       "42    0.9769    0.0228    0.0003    0.0000    0.0000    0.976305         0.0   \n",
       "43    0.8102    0.1704    0.0180    0.0013    0.0001    0.851392         0.0   \n",
       "44    0.8102    0.1704    0.0180    0.0013    0.0001    0.841589         0.0   \n",
       "45    0.9769    0.0228    0.0003    0.0000    0.0000    1.037975         0.0   \n",
       "46    0.8102    0.1704    0.0180    0.0013    0.0001    0.783120         0.0   \n",
       "47    0.9769    0.0228    0.0003    0.0000    0.0000    1.058437         0.0   \n",
       "48    0.8102    0.1704    0.0180    0.0013    0.0001    0.885548         0.0   \n",
       "49    0.8102    0.1704    0.0180    0.0013    0.0001    0.856841         0.0   \n",
       "\n",
       "    k9a 2_pred  k9a 3_pred  k9a 4_pred  \n",
       "0          0.0         0.0         0.0  \n",
       "1          0.0         0.0         0.0  \n",
       "2          0.0         0.0         0.0  \n",
       "3          0.0         0.0         0.0  \n",
       "4          0.0         0.0         0.0  \n",
       "5          0.0         0.0         0.0  \n",
       "6          0.0         0.0         0.0  \n",
       "7          0.0         0.0         0.0  \n",
       "8          0.0         0.0         0.0  \n",
       "9          0.0         0.0         0.0  \n",
       "10         0.0         0.0         0.0  \n",
       "11         0.0         0.0         0.0  \n",
       "12         0.0         0.0         0.0  \n",
       "13         0.0         0.0         0.0  \n",
       "14         0.0         0.0         0.0  \n",
       "15         0.0         0.0         0.0  \n",
       "16         0.0         0.0         0.0  \n",
       "17         0.0         0.0         0.0  \n",
       "18         0.0         0.0         0.0  \n",
       "19         0.0         0.0         0.0  \n",
       "20         0.0         0.0         0.0  \n",
       "21         0.0         0.0         0.0  \n",
       "22         0.0         0.0         0.0  \n",
       "23         0.0         0.0         0.0  \n",
       "24         0.0         0.0         0.0  \n",
       "25         0.0         0.0         0.0  \n",
       "26         0.0         0.0         0.0  \n",
       "27         0.0         0.0         0.0  \n",
       "28         0.0         0.0         0.0  \n",
       "29         0.0         0.0         0.0  \n",
       "30         0.0         0.0         0.0  \n",
       "31         0.0         0.0         0.0  \n",
       "32         0.0         0.0         0.0  \n",
       "33         0.0         0.0         0.0  \n",
       "34         0.0         0.0         0.0  \n",
       "35         0.0         0.0         0.0  \n",
       "36         0.0         0.0         0.0  \n",
       "37         0.0         0.0         0.0  \n",
       "38         0.0         0.0         0.0  \n",
       "39         0.0         0.0         0.0  \n",
       "40         0.0         0.0         0.0  \n",
       "41         0.0         0.0         0.0  \n",
       "42         0.0         0.0         0.0  \n",
       "43         0.0         0.0         0.0  \n",
       "44         0.0         0.0         0.0  \n",
       "45         0.0         0.0         0.0  \n",
       "46         0.0         0.0         0.0  \n",
       "47         0.0         0.0         0.0  \n",
       "48         0.0         0.0         0.0  \n",
       "49         0.0         0.0         0.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_label_list=['k6a 0',\"k6a 1\",\"k6a 2\",\"k6a 3\",\"k6a 4\",\"k1 0\",\"k1 1\",\"k1 2\",\"k1 3\",\"k1 4\",\"k9a 0\",\"k9a 1\",\"k9a 2\",\"k9a 3\",\"k9a 4\"]\n",
    "\n",
    "delta_test_result=test_model(best_model,x_test,y_test,all_label_list)\n",
    "\n",
    "columns_names=delta_test_result.columns\n",
    "\n",
    "compare_k6a=columns_names[[0,1,2,3,4,15,16,17,18,19]]\n",
    "compare_k1=columns_names[[5,6,7,8,9,20,21,22,23,24]]\n",
    "compare_k9a=columns_names[[10,11,12,13,14,25,26,27,28,29]]\n",
    "#print(compare_k6a)\n",
    "\n",
    "#delta_test_result[compare_k6a].head(50)\n",
    "#delta_test_result[compare_k1].head(50)\n",
    "delta_test_result[compare_k9a].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_test_result[compare_k9a].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Generate a large random dataset\n",
    "rs = np.random.RandomState(33)\n",
    "d = delta_test_result\n",
    "# Compute the correlation matrix\n",
    "corr = d.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_model.save(\"saved_Models/lambda_100_best_model_main_max_err_0_0027\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
